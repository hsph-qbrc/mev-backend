<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="canonical" href="https://webmev.tm4.org/creating_analyses/">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Creating new analyses/operations - WebMeV REST API</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
  <link href="../style.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Creating new analyses/operations";
    var mkdocs_page_input_path = "creating_analyses.md";
    var mkdocs_page_url = "/creating_analyses/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> WebMeV REST API</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../general_architecture/">Architecture</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Installation and Setup</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../install/">Preliminaries</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../mev_cluster_setup/">Installation and setup</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../setup_configuration/">Configuration</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../management_commands/">Management commands</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../data_server/">Mock data server</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">API</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../api/">Intro and core concepts</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="#">Data Structures</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="#">Resources</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../resources/">General info</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../resource_types/">Resource types</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../resource_metadata/">Resource metadata</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../operation_resources/">Operation-associated resources</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../attributes/">Attributes</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../elements/">Observations and Features</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../observation_and_feature_sets/">ObservationSet and FeatureSets</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../workspaces/">Workspaces</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../metadata/">Workspace metadata</a>
                </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../auth/">Authentication</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../operations/">Operation concepts</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Creating new analyses/operations</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#local-docker-based-mode">Local Docker-based mode</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#example">Example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#a-suggested-workflow-for-creating-new-analyses">A suggested workflow for creating new analyses</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#remote-cromwell-based-jobs">Remote, Cromwell-based jobs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#additional-notes">Additional notes:</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../example_workflow/">Workflow and analysis concepts</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../public_data/">Creating and managing public data</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">WebMeV REST API</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Creating new analyses/operations</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/web-mev/mev-backend/edit/master/docs/creating_analyses.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>

          <div role="main">
            <div class="section">
              
                <h2 id="creating-a-new-analysis-operation-for-use-with-webmev"><a class="toclink" href="#creating-a-new-analysis-operation-for-use-with-webmev">Creating a new analysis (<code>Operation</code>) for use with WebMEV</a></h2>
<p>WebMEV analyses (AKA <code>Operation</code>s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. </p>
<p>Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the "run mode" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones.</p>
<hr />
<h3 id="local-docker-based-mode"><a class="toclink" href="#local-docker-based-mode">Local Docker-based mode</a></h3>
<p>Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints.</p>
<p>Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to:</p>
<pre class="codehilite"><code>docker run -d -v &lt;docker volume&gt;:&lt;container workspace&gt; --entrypoint=&lt;CMD&gt; &lt;IMAGE&gt;
</code></pre>

<p>This runs the command specified by <code>&lt;CMD&gt;</code> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. </p>
<p>To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository:</p>
<ul>
<li><code>operation_spec.json</code><ul>
<li>This file dictates the input and output parameters for the analysis. Of type <code>Operation</code>.</li>
</ul>
</li>
<li><code>converters.json</code><ul>
<li>This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/<code>Observation</code>s)
and format it for the commandline invocation (e.g. as a comma-delimited list of strings)</li>
</ul>
</li>
<li><code>entrypoint.txt</code><ul>
<li>This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com)</li>
</ul>
</li>
<li><code>docker/Dockerfile</code><ul>
<li>The <code>docker</code> folder contains (at minimum) a <code>Dockerfile</code> which provides the "recipe" for building the Docker image. Additional files to be included in the Docker build context (such as scripts or static data) can be placed in this folder.</li>
</ul>
</li>
</ul>
<p><strong>Outputs</strong>
While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an <code>outputs.json</code> file in the container's "workspace". This file is accessible to WebMEV via the shared volume provided with the <code>-v</code> argument to the <code>docker run</code> command. More details below in the concrete example.</p>
<p>Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an <code>outputs.json</code> file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the <code>outputs.json</code> and it would be ready for use as an analysis module in WebMEV.</p>
<h4 id="example"><a class="toclink" href="#example">Example</a></h4>
<p>For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/</p>
<p>We first describe the overall structure and then talk specifically about each file.</p>
<p><strong>Overall structure</strong></p>
<p>The repository has:
- <code>operation_spec.json</code> (required)
- <code>converters.json</code> (required)
- <code>entrypoint.txt</code> (required)
- <code>docker/</code>
    - <code>Dockerfile</code> (required)
    - <code>run_pca.py</code>
    - <code>requirements.txt</code></p>
<p>The analysis is designed so that it will execute a single python script (<code>docker/run_pca.py</code>) as follows:</p>
<pre class="codehilite"><code>run_pca.py -i &lt;path to input file&gt; [-s &lt;comma-delimited list of sample names&gt;]
</code></pre>

<p>The first arg (<code>-i</code>) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument (<code>-s</code>) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used.</p>
<p>In addition to running the PCA, this script will also create the <code>outputs.json</code> file. It's not required that you structure the code in any particular manner, but the analysis has to create the <code>outputs.json</code> file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV.</p>
<p><strong><code>docker/</code> folder and Docker context</strong></p>
<p>In the <code>docker/</code> folder we have the required <code>Dockerfile</code>, the script to run (<code>run_pca.py</code>), and a <code>requirements.txt</code> file which provides the packages needed to construct the proper Python installation.</p>
<p>The <code>Dockerfile</code> looks like:</p>
<pre class="codehilite"><code>FROM debian:stretch

RUN apt-get update &amp;&amp; \
    apt-get install -y python3-dev python3-pip

# Install some Python3 libraries:
RUN mkdir /opt/software
ADD requirements.txt /opt/software/
ADD run_pca.py /opt/software/
RUN chmod +x /opt/software/run_pca.py
RUN pip3 install -r /opt/software/requirements.txt

ENTRYPOINT [&quot;/opt/software/run_pca.py&quot;]
</code></pre>

<p><code>requirements.txt</code> looks like: (truncated)</p>
<pre class="codehilite"><code>cryptography==1.7.1
...
scikit-learn==0.22.2.post1
...
scipy==1.4.1
...
</code></pre>

<p><code>run_pca.py</code>:
For brevity, we omit the full <code>run_pca.py</code> script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the <code>Dockerfile</code> places this script in the <code>/opt/software</code> folder. Thus, we have to either append to the <code>PATH</code> in the container, or provide the full path to this script when we invoke it for execution. Below (see <code>entrypoint.txt</code>) we use the latter.</p>
<p>Finally, we note that this script creates an <code>outputs.json</code> file:</p>
<pre class="codehilite"><code>...
outputs = {
    'pca_coordinates': &lt;path to output matrix of principal coordinates&gt;,
    'pc1_explained_variance':pca.explained_variance_ratio_[0],
    'pc2_explained_variance': pca.explained_variance_ratio_[1]
}
json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w'))
</code></pre>

<p>As stated prior, it's not required that <em>this</em> script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to "know about WebMEV". Everything else in the script operates divorced from any notion of WebMEV architecture. </p>
<p><strong><code>operation_spec.json</code></strong></p>
<p>The operation_spec.json file provides a description of the analysis and follows the format of our <code>Operation</code> data structure:</p>
<pre class="codehilite"><code>{
    &quot;name&quot;: &quot;&quot;, 
    &quot;description&quot;: &quot;&quot;, 
    &quot;inputs&quot;: &lt;Mapping of keys to OperationInput objects&gt;, 
    &quot;outputs&quot;: &lt;Mapping of keys to OperationOutput objects&gt;, 
    &quot;mode&quot;: &quot;&quot;
}
</code></pre>

<p>Importantly, the <code>mode</code> key must be set to <code>"local_docker"</code> which lets WebMEV know that this analysis/<code>Operation</code> will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is "ingested" and prepared by WebMEV.</p>
<p>Concretely our PCA analysis:</p>
<pre class="codehilite"><code>{
    &quot;name&quot;: &quot;Principal component analysis (PCA)&quot;, 
    &quot;description&quot;: &quot;Executes a 2-d PCA to examine the structure and variation of a dataset.&quot;, 
    &quot;inputs&quot;: {
        &quot;input_matrix&quot;: {
            &quot;description&quot;: &quot;The input matrix. For example, a gene expression matrix for a cohort of samples.&quot;, 
            &quot;name&quot;: &quot;Input matrix:&quot;, 
            &quot;required&quot;: true, 
            &quot;spec&quot;: {
                &quot;attribute_type&quot;: &quot;DataResource&quot;, 
                &quot;resource_types&quot;: [&quot;MTX&quot;,&quot;I_MTX&quot;, &quot;EXP_MTX&quot;, &quot;RNASEQ_COUNT_MTX&quot;], 
                &quot;many&quot;: false
            }
        }, 
        &quot;samples&quot;: {
            &quot;description&quot;: &quot;The samples to use in the PCA. By default, it will use all samples/observations.&quot;, 
            &quot;name&quot;: &quot;Samples:&quot;, 
            &quot;required&quot;: false, 
            &quot;spec&quot;: {
                &quot;attribute_type&quot;: &quot;ObservationSet&quot;
            }
        }
    }, 
    &quot;outputs&quot;: {
        &quot;pca_coordinates&quot;: {
            &quot;spec&quot;: {
                &quot;attribute_type&quot;: &quot;DataResource&quot;, 
                &quot;resource_type&quot;: &quot;MTX&quot;,
                &quot;many&quot;: false
            }
        },
        &quot;pc1_explained_variance&quot;: {
            &quot;spec&quot;: {
                &quot;attribute_type&quot;: &quot;BoundedFloat&quot;,
                &quot;min&quot;: 0,
                &quot;max&quot;: 1.0
            }
        },
        &quot;pc2_explained_variance&quot;: {
            &quot;spec&quot;: {
                &quot;attribute_type&quot;: &quot;BoundedFloat&quot;,
                &quot;min&quot;: 0,
                &quot;max&quot;: 1.0
            }
        }
    }, 
    &quot;mode&quot;: &quot;local_docker&quot;
}
</code></pre>

<p>In the <code>inputs</code> section, this <code>Operation</code> states that it has one required (<code>input_matrix</code>) and one optional input (<code>samples</code>). For <code>input_matrix</code>, we expect a single input file (a <code>DataResource</code> with <code>many=false</code>) that has an appropriate resource type. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of <code>"MTX"</code>,<code>"I_MTX"</code>, <code>"EXP_MTX"</code>, or <code>"RNASEQ_COUNT_MTX"</code>. The full list of all resource types is available at <code>/api/resource-types/</code></p>
<p>The second, optional input (<code>samples</code> ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type <code>ObservationSet</code>. Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the <code>-s</code> argument. Thus, we will use one of the "converter" classes to convert the <code>ObservationSet</code> instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the <code>ObservationSet</code> instance as an argument to their script and parse that accordingly.</p>
<p>For outputs, we expect a single <code>DataResource</code> with type <code>"MTX"</code> and two bounded floats, which represent the explained variance of the PCA.</p>
<p><strong><code>converters.json</code></strong></p>
<p>The <code>converters.json</code> file tells WebMEV how to take a user-input and convert it to the appropriate format to invoke the script. To accomplish this, we provide a mapping of the input "name" to a class which implements the conversion. For us, this looks like:</p>
<pre class="codehilite"><code>{
  &quot;input_matrix&quot;:&quot;api.converters.data_resource.LocalDockerSingleDataResourceConverter&quot;, 
  &quot;samples&quot;: &quot;api.converters.element_set.ObservationSetCsvConverter&quot;
}
</code></pre>

<p>The class implementations are provided using Python's "dotted" notation. A variety of commonly-used converters are provided with WebMEV, but developers are free to create their own implementations for their own WebMEV instances. The only requirement is that the class implements a common interface method named <code>convert</code>, which takes the user-supplied input and returns an appropriate output.</p>
<p>As an example, we note that the <code>LocalDockerSingleDataResourceConverter</code> above takes the user-supplied input (a UUID which identifies a file/<code>Resource</code> they own) and "converts" it to a path on the server. In this way, the <code>run_pca.py</code> script is not concerned with how WebMEV stores files, etc. WebMEV handles the file moving/copying and analyses can be written without dependencies on WebMEV-related architecture or how files are stored within WebMEV.</p>
<p>The <code>samples</code> input uses the <code>api.converters.element_set.ObservationSetCsvConverter</code> which converts an <code>ObservationSet</code> such as:</p>
<pre class="codehilite"><code>{
    &quot;multiple&quot;: true,
    &quot;elements&quot;: [
        {
            &quot;id&quot;:&quot;sampleA&quot;,
            &quot;attributes&quot;: {}
        },
        {
            &quot;id&quot;:&quot;sampleB&quot;,
            &quot;attributes&quot;: {}
        }    
    ]
}
</code></pre>

<p>into a CSV string: <code>sampleA,sampleB</code>.</p>
<p>Clearly, this requires some knowledge of the available "converter" implementations. We expect that there are not so many that this burden is unreasonable. We decided that the flexibility provided by this inconvenience was more beneficial than restricting the types of inputs and how they can be formatted for invoking jobs.</p>
<p><strong><code>entrypoint.txt</code></strong></p>
<p>The entrypoint file has the command that will be run as the <code>ENTRYPOINT</code> of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax.</p>
<p>In our example, we have:</p>
<pre class="codehilite"><code>/opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %}
</code></pre>

<p>(as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the <code>PATH</code> when building the Docker image)</p>
<p>The variables in this template (between the double braces) must match the keys provided in the <code>inputs</code> section of the <code>operation_spec.json</code> document.</p>
<p>Thus, if the <code>samples</code> input is omitted (which means all samples are used in the PCA calculation), the final command would look like:</p>
<pre class="codehilite"><code>/opt/software/run_pca.py -i &lt;path to matrix&gt;
</code></pre>

<p>If the <code>samples</code> input is provided, WebMEV handles converting the <code>ObservationSet</code> instance into a comma-delimited string to create:</p>
<pre class="codehilite"><code>/opt/software/run_pca.py -i &lt;path to matrix&gt; -s A,B,C
</code></pre>

<p>(e.g. for samples/observations named "A", "B", and "C")</p>
<h4 id="a-suggested-workflow-for-creating-new-analyses"><a class="toclink" href="#a-suggested-workflow-for-creating-new-analyses">A suggested workflow for creating new analyses</a></h4>
<p>First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal <code>Operation</code> into the <code>operation_spec.json</code> file. </p>
<p>Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the <code>outputs.json</code> file at some point in the process. </p>
<p>Take the "prototype" command you would use to execute the script and write it into <code>entrypoint.txt</code> using jinja2 template syntax. The input variable keys should correspond to those in your <code>operation_spec.json</code>. </p>
<p>Create the <code>converters.json</code> file which will reformat the inputs into items that the entrypoint command will understand.</p>
<p>Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV.</p>
<hr />
<h3 id="remote-cromwell-based-jobs"><a class="toclink" href="#remote-cromwell-based-jobs">Remote, Cromwell-based jobs</a></h3>
<p>For jobs that are run remotely with the help of the Cromwell job engine, we have slightly different required files. </p>
<p>Cromwell-based jobs are executed using "Workflow Definition Language" (WDL) syntax files (https://openwdl.org/). When using this job engine, the primary purpose of WebMEV is to validate user inputs and reformat them to be compatible with the inputs required to run the workflow. For those who have not used Broad's Cromwell engine before, the three components of an analysis workflow include:</p>
<ul>
<li>WDL file(s): Specifies the commands that are run. You can think of this as you would a typical shell script.</li>
<li>A JSON-format inputs file: This maps the expected workflow inputs (e.g. strings, numbers, or files) to specific values. For instance, if we expect a file, then the inputs JSON file will map the input variable to a file path. WebMEV is responsible for creating this file at runtime.</li>
<li>One of more Docker containers: Cromwell orchestrates the startup/shutdown of cloud-based virtual machines but all commands are run within Docker runtimes on those machines. Thus, the WDL files will dictate which Docker images are used for each step in the analysis. There can be an arbitrary number of these. </li>
</ul>
<p>Thus, to create a Cromwell-based job that is compatible with WebMEV we require: </p>
<ul>
<li>
<p><code>operation_spec.json</code></p>
<ul>
<li>This file dictates the input and output parameters for the analysis. Of type <code>Operation</code>. This file is the same as with any WebMEV analysis. To specifically create an <code>Operation</code> for the Cromwell job runner, you <em>must</em> specify <code>"mode": "cromwell"</code> in the <code>Operation</code> object.</li>
</ul>
</li>
<li>
<p><code>main.wdl</code></p>
<ul>
<li>In general there can be any number of WDL-format files in the repository. However, the primary or "entry" WDL file <em>must</em> be named as <code>main.wdl</code>.</li>
</ul>
</li>
<li>
<p><code>inputs.json</code></p>
<ul>
<li>This is the JSON-format file which dictates the inputs to the workflow. It is a template that will be appropriately filled at runtime. Thus, the "values" of the mapping do not matter, but the keys must map to input variables in <code>main.wdl</code>. Typically, this file is easily created by Broad's WOMTool. See below for an example.</li>
</ul>
</li>
<li><code>static_inputs.json</code><ul>
<li>An <em>optional</em> file that gives values for variables like genome indexes and other relatively static inputs. Keys in this should be a subset of those in <code>inputs.json</code>. File paths contained here are used to copy files into a WebMEV-associated bucket.</li>
</ul>
</li>
<li><code>converters.json</code><ul>
<li>This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/<code>Observation</code>s)
and format it to be used in <code>inputs.json</code>. As above, this is a mapping of the input name to a "dotted" class implementation.</li>
</ul>
</li>
<li><code>docker/</code><ul>
<li>The <code>docker</code> folder contains one or more Dockerfile-format files and the dependencies to create those Docker images. Each Dockerfile is named according to its target image. For instance, if one of the WDL files specifies that it depends on a Docker image named <code>docker.io/myUser/foo</code>, then the Dockerfile defining that image should be named <code>Dockerfile.foo</code>.</li>
</ul>
</li>
</ul>
<h4 id="additional-notes"><a class="toclink" href="#additional-notes">Additional notes:</a></h4>
<p><strong>Remarks about dependencies between <code>main.wdl</code>, <code>inputs.json</code> and <code>operation_spec.json</code></strong></p>
<p>As much as we try to remove interdependencies between files (for ease of development), there are situations we can't resolve easily. One such case is the interdependencies between <code>main.wdl</code>, <code>inputs.json</code>, and the <code>operation_spec.json</code> files.</p>
<p>As mentioned above, the <code>inputs.json</code> file supplied in the repository is effectively a template which is filled at runtime. The keys of that object correspond to inputs to the main WDL script <code>main.wdl</code>. For example, given a WDL script with the following input definition:</p>
<pre class="codehilite"><code>workflow SomeWorkflow {
    ...
    Array[String] samples
    ....
}
</code></pre>

<p>then the <code>inputs.json</code> would require the key <code>SomeWorkflow.samples</code>. Generally, WDL constructs its inputs in the format of <code>&lt;Workflow name&gt;.&lt;input variable name&gt;</code>. Thus, <code>inputs.json</code> would appear, in part, like</p>
<pre class="codehilite"><code>{
    ...
    &quot;SomeWorkflow.samples&quot;: &quot;Array[String]&quot;,
    ...
}
</code></pre>

<p>As mentioned above, the "value" (here, <code>"Array[String]"</code>) does not matter; Broad's WOMTool will typically fill-in the expected type (as a string) to serve as a cue.</p>
<p>Finally, WebMEV has to know which inputs of the <code>Operation</code> correspond to which inputs of the WDL script. Thus, in our <code>operation_spec.json</code>, the keys in our <code>inputs</code> object must be consistent with <code>main.wdl</code> and <code>inputs.json</code>:</p>
<pre class="codehilite"><code>{
    ...
    &quot;inputs&quot;: {
        ...
        &quot;SomeWorkflow.samples&quot;: &lt;OperationInput&gt;
        ...
    }
}
</code></pre>

<p><strong>Converting inputs</strong></p>
<p>As with all analysis execution modes, we have "converter" classes which translate user inputs into formats that are compatible with the job runner. </p>
<p>For instance, using the example above, one of the inputs for a WDL could be an array of strings (<code>Array[String]</code> in WDL-type syntax). Thus, a converter would be responsible for taking say, an <code>ObservationSet</code>, and turning that into a list of strings to provide the same names. For example, we may wish to convert an <code>ObservationSet</code> given as:</p>
<pre class="codehilite"><code>{
    &quot;multiple&quot;: true,
    &quot;elements&quot;: [
        {
            &quot;id&quot;:&quot;sampleA&quot;,
            &quot;attributes&quot;: {}
        },
        {
            &quot;id&quot;:&quot;sampleB&quot;,
            &quot;attributes&quot;: {}
        }    
    ]
}
</code></pre>

<p>Then, the "inputs" data structure submitted to Cromwell (basically <code>inputs.json</code> after it has been filled-in) would, in part, look like:</p>
<pre class="codehilite"><code>{
    ...
    &quot;SomeWorkflow.samples&quot;: [&quot;sampleA&quot;, &quot;sampleB&quot;],
    ...
}
</code></pre>

<p><strong>Creation of Docker images</strong></p>
<p>As described above, each repository can contain an arbitrary (non-zero) number of WDL files, each of which can depend on one more Docker images for their runtime. There are some custom steps involved during the ingestion of new Cromwell-based workflow, which we explain below.</p>
<p>When Docker images are specified in the <code>runtime</code> section of the WDL files, the line is formatted as:</p>
<pre class="codehilite"><code>runtime {
    ...
    docker: &quot;&lt;repo name&gt;/&lt;username&gt;/&lt;image name&gt;:&lt;tag&gt;&quot;
    ...
}
</code></pre>

<p>e.g.</p>
<pre class="codehilite"><code>runtime {
    ...
    docker: &quot;docker.io/myUser/foo:v1&quot;
    ...
}
</code></pre>

<p>Note that we only use the image name (<code>foo</code>) when we are ingesting and preparing a new workflow for use in WebMEV. The reason is that we wish to keep all Docker images "in house" within our own Dockerhub account; we do not want to depend on external Docker resources which may change without our knowledge or control. Therefore, the repository and username are not used. Further, the tag is also ignored as we ultimately replace it with the git commit hash. Since the git commit hash is not produced until <em>after</em> the commit, we obviously can't append the images with the proper tag prior to the commit. Thus, during the ingestion process we perform the following:</p>
<ul>
<li>Parse all WDL files in the github repo and extract out all the runtime Docker images. This will be a set of strings.</li>
<li>For each Docker "image string" (e.g. <code>someUser/foo:v1</code>):<ul>
<li>Extract the image name, e.g. <code>foo</code></li>
<li>Search for a corresponding Dockerfile in the repo, e.g. <code>docker/Dockerfile.foo</code></li>
<li>Build the image, tagging with the github commit ID, e.g. <code>abc123</code></li>
<li>Push the image to the WebMEV Dockerhub, e.g. <code>docker.io/web-mev</code></li>
<li>Edit and save the WDL file with the newly-built Docker image, e.g. <code>docker.io/web-mev/foo:abc123</code>.</li>
</ul>
</li>
</ul>
<p>Thus, regardless of whoever creates the original image, the repository should have all the files necessary to build a fresh image which we "claim as our own" by assigning our username/tag and pushing it to the WebMEV Dockerhub account. </p>
<p>We note that this technically modifies the workflow relative to the github repository, so the WebMEV-internal version is not <em>exactly</em> the same. However, this difference is limited to the name of the Docker image. All other aspects of the analysis are able to be exactly recreated based on the repository.</p>
<p>Note that repositories based on many Docker containers may take a significant time to ingest, as each image must be built and pushed to Dockerhub.</p>
<p><strong>Copying of static resources</strong></p>
<p>If the <code>static_inputs.json</code> file is present, we expect that this file will be used for static items that are not dependent on user input. We could also put such items as <code>default</code> in the <code>operation_spec.json</code> file, but we instead choose to extract them out to this file.</p>
<p>Upon ingestion, the files will be copied to an <code>Operation</code>-specific bucket/folder. For instance, given the following <code>static_inputs.json</code>:</p>
<pre class="codehilite"><code>{
    &quot;Workflow.genome_idx&quot;:&quot;gs://some-bucket/grch38.idx&quot;
}
</code></pre>

<p>During ingestion, this <code>Operation</code> will be assigned a UUID. Then, we copy this index to a new location identified by that UUID. The updated/edited <code>static_inputs.json</code> file will be:</p>
<pre class="codehilite"><code>{
    &quot;Workflow.genome_idx&quot;:&quot;gs://mev-bucket/&lt;UUID&gt;/grch38.idx&quot;
}
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../example_workflow/" class="btn btn-neutral float-right" title="Workflow and analysis concepts">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../operations/" class="btn btn-neutral" title="Operation concepts"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/web-mev/mev-backend/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../operations/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../example_workflow/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
