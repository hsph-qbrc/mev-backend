{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the WebMEV documentation page. This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Home"},{"location":"#welcome-to-the-webmev-documentation-page","text":"This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Welcome to the WebMEV documentation page."},{"location":"api/","text":"Documentation on the API There are two aspects of the WebMEV backend. First, we have the public-facing RESTful API endpoints which are used to drive an analysis, upload files, and perform other actions. Documentation of the API is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation . The second aspect of WebMEV is the data structures, models, and concepts that we use to architect the system. You will find information about these entities and their relationships in this section. Understanding these data structures and associated nomenclature will be important when describing how to work with WebMEV and create new workflows. Core concepts The goal of WebMEV is to provide a suite of analysis tools and visualizations to guide users through self-directed analyses, most commonly with genomic (particularly transcriptomic) data. At a high level, users will start from either files that they provide (e.g. a previously generated expression matrix) or (if available) can import data from public repositories that are available through WebMEV. Users will then create one or more Workspace s which facilitates logical organization of analyses and separation of distinct projects or experiments. Within the context of the Workspace , users can perform a custom analysis as a series of atomic steps which we call Operation s. More specific details about each of these steps, including how we handle metadata are available in the other sections of this documentation.","title":"Intro and core concepts"},{"location":"api/#documentation-on-the-api","text":"There are two aspects of the WebMEV backend. First, we have the public-facing RESTful API endpoints which are used to drive an analysis, upload files, and perform other actions. Documentation of the API is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation . The second aspect of WebMEV is the data structures, models, and concepts that we use to architect the system. You will find information about these entities and their relationships in this section. Understanding these data structures and associated nomenclature will be important when describing how to work with WebMEV and create new workflows.","title":"Documentation on the API"},{"location":"api/#core-concepts","text":"The goal of WebMEV is to provide a suite of analysis tools and visualizations to guide users through self-directed analyses, most commonly with genomic (particularly transcriptomic) data. At a high level, users will start from either files that they provide (e.g. a previously generated expression matrix) or (if available) can import data from public repositories that are available through WebMEV. Users will then create one or more Workspace s which facilitates logical organization of analyses and separation of distinct projects or experiments. Within the context of the Workspace , users can perform a custom analysis as a series of atomic steps which we call Operation s. More specific details about each of these steps, including how we handle metadata are available in the other sections of this documentation.","title":"Core concepts"},{"location":"attributes/","text":"Attributes Attribute s serve as \"parameters\" and are a way of providing validation and type-checking for values that are passed around within WebMEV. The different types represent different simple entities within WebMEV. For example, we have simple wrappers around primitives like integers which enforce constraints on the underlying primitive type (e.g. for a probability, we can use a BoundedFloatAttribute set with bounds of [0,1]). Other types can represent and validate files (\"data resources\") Attribute s are used to provide metadata (e.g. a phenotype of a sample given as a StringAttribute ) or are used as parameters to analyses (e.g. a BoundedFloatAttribute for filtering p-values less than a particular value) class data_structures.base_attribute. BaseAttributeType ( val , **kwargs ) This is a base class for all the types of attributes we might implement. It contains common operations/logic. class data_structures.attribute_types. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttributeType and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class data_structures.attribute_types. IntegerAttribute ( val , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class data_structures.attribute_types. PositiveIntegerAttribute ( val , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class data_structures.attribute_types. NonnegativeIntegerAttribute ( val , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class data_structures.attribute_types. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class data_structures.attribute_types. FloatAttribute ( val , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } Note that positive/negative infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER constants.NEGATIVE_INF_MARKER class data_structures.attribute_types. PositiveFloatAttribute ( val , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } Note that positive infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER class data_structures.attribute_types. NonnegativeFloatAttribute ( val , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } Note that positive infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER class data_structures.attribute_types. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class data_structures.attribute_types. StringAttribute ( val , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } class data_structures.attribute_types. OptionStringAttribute ( value , **kwargs ) A String type that only admits one from a set of preset options (e.g. like a dropdown) { \"attribute_type\": \"OptionString\", \"value\": <str>, \"options\": [<str>, <str>,...,<str>] } class data_structures.attribute_types. BooleanAttribute ( val , **kwargs ) Basic boolean { \"attribute_type\": \"Boolean\", \"value\": <bool> } class data_structures.data_resource_attributes. DataResourceAttribute ( value , **kwargs ) This class holds info that represents one or more resources of a fixed type. If the input can be one of multiple types, use one of the other classes. The serialized representation looks like: { \"attribute_type\": \"DataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, \"resource_type\": <str> } resource_type dictates the allowable type of resource (e.g. integer matrix only). It's value is matched against a controlled set of strings. class data_structures.data_resource_attributes. OperationDataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances which are user-independent, such as database-like resources which are used for analyses. Structure { \"attribute_type\": \"OperationDataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, \"resource_type\": <str>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key.","title":"Attributes"},{"location":"attributes/#attributes","text":"Attribute s serve as \"parameters\" and are a way of providing validation and type-checking for values that are passed around within WebMEV. The different types represent different simple entities within WebMEV. For example, we have simple wrappers around primitives like integers which enforce constraints on the underlying primitive type (e.g. for a probability, we can use a BoundedFloatAttribute set with bounds of [0,1]). Other types can represent and validate files (\"data resources\") Attribute s are used to provide metadata (e.g. a phenotype of a sample given as a StringAttribute ) or are used as parameters to analyses (e.g. a BoundedFloatAttribute for filtering p-values less than a particular value) class data_structures.base_attribute. BaseAttributeType ( val , **kwargs ) This is a base class for all the types of attributes we might implement. It contains common operations/logic. class data_structures.attribute_types. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttributeType and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class data_structures.attribute_types. IntegerAttribute ( val , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class data_structures.attribute_types. PositiveIntegerAttribute ( val , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class data_structures.attribute_types. NonnegativeIntegerAttribute ( val , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class data_structures.attribute_types. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class data_structures.attribute_types. FloatAttribute ( val , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } Note that positive/negative infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER constants.NEGATIVE_INF_MARKER class data_structures.attribute_types. PositiveFloatAttribute ( val , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } Note that positive infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER class data_structures.attribute_types. NonnegativeFloatAttribute ( val , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } Note that positive infinite values are acceptable provided they are specified using constants.POSITIVE_INF_MARKER class data_structures.attribute_types. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class data_structures.attribute_types. StringAttribute ( val , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } class data_structures.attribute_types. OptionStringAttribute ( value , **kwargs ) A String type that only admits one from a set of preset options (e.g. like a dropdown) { \"attribute_type\": \"OptionString\", \"value\": <str>, \"options\": [<str>, <str>,...,<str>] } class data_structures.attribute_types. BooleanAttribute ( val , **kwargs ) Basic boolean { \"attribute_type\": \"Boolean\", \"value\": <bool> } class data_structures.data_resource_attributes. DataResourceAttribute ( value , **kwargs ) This class holds info that represents one or more resources of a fixed type. If the input can be one of multiple types, use one of the other classes. The serialized representation looks like: { \"attribute_type\": \"DataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, \"resource_type\": <str> } resource_type dictates the allowable type of resource (e.g. integer matrix only). It's value is matched against a controlled set of strings. class data_structures.data_resource_attributes. OperationDataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances which are user-independent, such as database-like resources which are used for analyses. Structure { \"attribute_type\": \"OperationDataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, \"resource_type\": <str>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key.","title":"Attributes"},{"location":"auth/","text":"Authentication with MEV Once a user is registered (with an email + password or via social authentication such as Google), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication"},{"location":"auth/#authentication-with-mev","text":"Once a user is registered (with an email + password or via social authentication such as Google), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication with MEV"},{"location":"creating_analyses/","text":"Creating a new analysis ( Operation ) for use with WebMEV WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones. Local Docker-based mode Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts or static data) can be placed in this folder. About container registries and Docker images for local jobs You can distribute your Docker images in a couple of ways, but they ultimately must be located in a public Docker repository where WebMEV (and others!) can find and pull them. At the current time, you must choose one method for all your local analysis tools. That is, you can't have one tool pull from Dockerhub while another uses Github. There are two options here, currently: (preferred and current default) Configure Github Actions to build your Docker image automatically upon each commit. This requires the use of a Github Actions yaml file which can be added to the repository; see the other WebMEV tool repositories and/or look at Github actions docs. As configured for the currently available tools, the script builds the image with the following name format: <github-org>/<repository-name>:<commit hash> . Then, the image will be available at ghcr.io/<github-org>/<repository-name>:<commit hash> . The advantage of this method is that everything is contained in a single github repository and there is a lesser chance of the built image drifting from that specified in docker/Dockerfile . Manually build and push your Docker images (based off docker/Dockerfile ) to Dockerhub in a public account. If you do this, then you must configure WebMEV to use Dockerhub (change the container_registry setting to \"dockerhub\" in deployment-aws/puppet/mevapi/manifests/init.pp ). This requires a manual build, however, which requires an extra step and can result in differences between the committed docker/Dockerfile and the one used to build the image. Note that depending on the container_registry setting, the workflow ingestion scripts will expect a certain container repository. For example, with the default setting of \"github\" , the image URI will be prefixed by \"ghcr.io\" . Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV. Example For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleVariableDataResourceConverter\", \"spec\": { \"attribute_type\": \"VariableDataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"required\": true, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"required\": true, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\", \"workspace_operation\": true } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a VariableDataResource with many=false ); the VariableDataResource permits multiple resource types. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use the api.converters.element_set.ObservationSetCsvConverter \"converter\" class to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. As a concrete example of the converter, consider the following ObservationSet : { \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } The api.converters.element_set.ObservationSetCsvConverter class will take that data structure and return a comma-delimited string. In this case, \"sampleA,sampleB\" . For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\") A suggested workflow for creating new analyses First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV. Remote, Cromwell-based jobs For jobs that are run remotely with the help of the Cromwell job engine, we have slightly different required files. Cromwell-based jobs are executed using \"Workflow Definition Language\" (WDL) syntax files (https://openwdl.org/). When using this job engine, the primary purpose of WebMEV is to validate user inputs and reformat them to be compatible with the inputs required to run the workflow. For those who have not used Broad's Cromwell engine before, the three components of an analysis workflow include: WDL file(s): Specifies the commands that are run. You can think of this as you would a typical shell script. A JSON-format inputs file: This maps the expected workflow inputs (e.g. strings, numbers, or files) to specific values. For instance, if we expect a file, then the inputs JSON file will map the input variable to a file path. WebMEV is responsible for creating this file at runtime. One of more Docker containers: Cromwell orchestrates the startup/shutdown of cloud-based virtual machines but all commands are run within Docker runtimes on those machines. Thus, the WDL files will dictate which Docker images are used for each step in the analysis. There can be an arbitrary number of these. Furthermore, unlike the local Docker-based jobs which require a single image, these jobs can use multiple images which can be placed in any combination of public Docker repositories (Dockerhub, Github CR, quay.io). Thus, to create a Cromwell-based job that is compatible with WebMEV we require: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . This file is the same as with any WebMEV analysis. To specifically create an Operation for the Cromwell job runner, you must specify \"mode\": \"cromwell\" in the Operation object. main.wdl In general there can be any number of WDL-format files in the repository. However, the primary or \"entry\" WDL file must be named as main.wdl . inputs.json This is the JSON-format file which dictates the inputs to the workflow. It is a template that will be appropriately filled at runtime. Thus, the \"values\" of the mapping do not matter, but the keys must map to input variables in main.wdl . Typically, this file is easily created by Broad's WOMTool. See below for an example. docker/ The docker folder contains one or more Dockerfile-format files and the dependencies to create those Docker images. Additional notes: Remarks about dependencies between main.wdl , inputs.json and operation_spec.json As much as we try to remove interdependencies between files (for ease of development), there are situations we can't resolve easily. One such case is the interdependencies between main.wdl , inputs.json , and the operation_spec.json files. As mentioned above, the inputs.json file supplied in the repository is effectively a template which is filled at runtime. The keys of that object correspond to inputs to the main WDL script main.wdl . For example, given a WDL script with the following input definition: workflow SomeWorkflow { ... Array[String] samples .... } then the inputs.json would require the key SomeWorkflow.samples . Generally, WDL constructs its inputs in the format of <Workflow name>.<input variable name> . Thus, inputs.json would appear, in part, like { ... \"SomeWorkflow.samples\": \"Array[String]\", ... } As mentioned above, the \"value\" (here, \"Array[String]\" ) does not matter; Broad's WOMTool will typically fill-in the expected type (as a string) to serve as a cue. Finally, WebMEV has to know which inputs of the Operation correspond to which inputs of the WDL script. Thus, in our operation_spec.json , the keys in our inputs object must be consistent with main.wdl and inputs.json : { ... \"inputs\": { ... \"SomeWorkflow.samples\": <OperationInput> ... } } Converting inputs As with all analysis execution modes, we have \"converter\" classes (specified in operation_spec.json ) which translate user inputs into formats that are compatible with the job runner. For instance, using the example above, one of the inputs for a WDL could be an array of strings ( Array[String] in WDL-type syntax). Thus, a converter would be responsible for taking say, an ObservationSet , and turning that into a list of strings to provide the same names. For example, we may wish to convert an ObservationSet given as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } Then, the \"inputs\" data structure submitted to Cromwell (basically inputs.json after it has been filled-in) would, in part, look like: { ... \"SomeWorkflow.samples\": [\"sampleA\", \"sampleB\"], ... } Creation of Docker images As described above, each repository can contain an arbitrary (non-zero) number of WDL files, each of which can depend on one more Docker images for their runtime. In contrast to local jobs which can only use a single image, WDL/Cromwell-based jobs can use any number of images. However, there are some custom steps involved during the ingestion of new Cromwell-based workflow, which we explain below. When Docker images are specified in the runtime section of the WDL files, the line is formatted as: runtime { ... docker: \"<repo name>/<username>/<image name>:<tag>\" ... } e.g. runtime { ... docker: \"docker.io/myUser/foo:v1\" ... } When using pre-built Docker images: Recall that if there is no explicit image \"tag\", then Docker defaults to using the image with the \"latest\" tag, which is implicitly the last build. Thus, if you are using an external Docker image (e.g. biocontainers/samtools) for one of your WDL tasks, then be sure to pick a specific tagged version so that it is not ambiguous, since latest can change over time. When creating our own Docker images: As mentioned in the local tools section, by default we use Github Actions to automatically build Docker images off the current repository state. The images are tagged with the github commit hash. If you have configured WebMEV to use the Dockerhub repo by default, then just be sure to build/tag/push your images carefully. That is, if you use docker: \"docker.io/myUser/foo:v1\" in your WDL, you need to name+tag your image, e.g. docker build -t myUser/foo:v1 . When using a custom Docker image built directly by Github Actions, we have a bit of a \"chicken or egg\" problem when it comes to providing tagged Docker images in our WDL files. As configured, Github Actions tags the Docker image with the commit hash. However, we obviously can't know that hash up front and use it in our WDL file(s). Hence, we reference untagged images in the docker attribute as follows: runtime { ... docker: \"ghcr.io/web-mev/mev-hcl\" ... } As part of the process of ingesting analysis tools into WebMEV, we modify that docker WDL attribute to attach a tag corresponding to the git commit. The process is roughly: Parse all WDL files in the github repo and extract out all the runtime Docker images. This will be a set of strings. For each Docker untagged \"image string\" (e.g. ghcr.io/someUser/foo ): Get the current git commit (say, abc123 ) and append that (e.g. ghcr.io/someUser/foo:abc123 ) Search for that image. If it does not exist, fail the ingestion process. This effectively prevents untagged images from being used. For instance, if an untagged docker.io/biocontainers/samtools Docker image was specifed, the full, tagged image would be docker.io/bioco ntainers/samtools:abc123 . There is a vanishingly small chance that our git repository's commit hash has a corresponding tag in the biocontainers/samtools` library. For each tagged \"image string\" (e.g. docker.io/biocontainers/samtools:v1.9-4-deb_cv1 ), simply search for it. We note that this technically modifies the workflow relative to the github repository, so the WebMEV-internal version is not exactly the same. However, this difference is limited to the name of the Docker image. All other aspects of the analysis are able to be exactly recreated based on the repository. Copying of static resources If the static_inputs.json file is present, we expect that this file will be used for static items that are not dependent on user input. We could also put such items as default in the operation_spec.json file, but we instead choose to extract them out to this file. At current, no tools use this feature, but this will be updated if necessary.","title":"Creating new analyses/operations"},{"location":"creating_analyses/#creating-a-new-analysis-operation-for-use-with-webmev","text":"WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones.","title":"Creating a new analysis (Operation) for use with WebMEV"},{"location":"creating_analyses/#local-docker-based-mode","text":"Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts or static data) can be placed in this folder. About container registries and Docker images for local jobs You can distribute your Docker images in a couple of ways, but they ultimately must be located in a public Docker repository where WebMEV (and others!) can find and pull them. At the current time, you must choose one method for all your local analysis tools. That is, you can't have one tool pull from Dockerhub while another uses Github. There are two options here, currently: (preferred and current default) Configure Github Actions to build your Docker image automatically upon each commit. This requires the use of a Github Actions yaml file which can be added to the repository; see the other WebMEV tool repositories and/or look at Github actions docs. As configured for the currently available tools, the script builds the image with the following name format: <github-org>/<repository-name>:<commit hash> . Then, the image will be available at ghcr.io/<github-org>/<repository-name>:<commit hash> . The advantage of this method is that everything is contained in a single github repository and there is a lesser chance of the built image drifting from that specified in docker/Dockerfile . Manually build and push your Docker images (based off docker/Dockerfile ) to Dockerhub in a public account. If you do this, then you must configure WebMEV to use Dockerhub (change the container_registry setting to \"dockerhub\" in deployment-aws/puppet/mevapi/manifests/init.pp ). This requires a manual build, however, which requires an extra step and can result in differences between the committed docker/Dockerfile and the one used to build the image. Note that depending on the container_registry setting, the workflow ingestion scripts will expect a certain container repository. For example, with the default setting of \"github\" , the image URI will be prefixed by \"ghcr.io\" . Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV.","title":"Local Docker-based mode"},{"location":"creating_analyses/#example","text":"For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleVariableDataResourceConverter\", \"spec\": { \"attribute_type\": \"VariableDataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"required\": true, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"required\": true, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\", \"workspace_operation\": true } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a VariableDataResource with many=false ); the VariableDataResource permits multiple resource types. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use the api.converters.element_set.ObservationSetCsvConverter \"converter\" class to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. As a concrete example of the converter, consider the following ObservationSet : { \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } The api.converters.element_set.ObservationSetCsvConverter class will take that data structure and return a comma-delimited string. In this case, \"sampleA,sampleB\" . For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\")","title":"Example"},{"location":"creating_analyses/#a-suggested-workflow-for-creating-new-analyses","text":"First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV.","title":"A suggested workflow for creating new analyses"},{"location":"creating_analyses/#remote-cromwell-based-jobs","text":"For jobs that are run remotely with the help of the Cromwell job engine, we have slightly different required files. Cromwell-based jobs are executed using \"Workflow Definition Language\" (WDL) syntax files (https://openwdl.org/). When using this job engine, the primary purpose of WebMEV is to validate user inputs and reformat them to be compatible with the inputs required to run the workflow. For those who have not used Broad's Cromwell engine before, the three components of an analysis workflow include: WDL file(s): Specifies the commands that are run. You can think of this as you would a typical shell script. A JSON-format inputs file: This maps the expected workflow inputs (e.g. strings, numbers, or files) to specific values. For instance, if we expect a file, then the inputs JSON file will map the input variable to a file path. WebMEV is responsible for creating this file at runtime. One of more Docker containers: Cromwell orchestrates the startup/shutdown of cloud-based virtual machines but all commands are run within Docker runtimes on those machines. Thus, the WDL files will dictate which Docker images are used for each step in the analysis. There can be an arbitrary number of these. Furthermore, unlike the local Docker-based jobs which require a single image, these jobs can use multiple images which can be placed in any combination of public Docker repositories (Dockerhub, Github CR, quay.io). Thus, to create a Cromwell-based job that is compatible with WebMEV we require: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . This file is the same as with any WebMEV analysis. To specifically create an Operation for the Cromwell job runner, you must specify \"mode\": \"cromwell\" in the Operation object. main.wdl In general there can be any number of WDL-format files in the repository. However, the primary or \"entry\" WDL file must be named as main.wdl . inputs.json This is the JSON-format file which dictates the inputs to the workflow. It is a template that will be appropriately filled at runtime. Thus, the \"values\" of the mapping do not matter, but the keys must map to input variables in main.wdl . Typically, this file is easily created by Broad's WOMTool. See below for an example. docker/ The docker folder contains one or more Dockerfile-format files and the dependencies to create those Docker images.","title":"Remote, Cromwell-based jobs"},{"location":"creating_analyses/#additional-notes","text":"Remarks about dependencies between main.wdl , inputs.json and operation_spec.json As much as we try to remove interdependencies between files (for ease of development), there are situations we can't resolve easily. One such case is the interdependencies between main.wdl , inputs.json , and the operation_spec.json files. As mentioned above, the inputs.json file supplied in the repository is effectively a template which is filled at runtime. The keys of that object correspond to inputs to the main WDL script main.wdl . For example, given a WDL script with the following input definition: workflow SomeWorkflow { ... Array[String] samples .... } then the inputs.json would require the key SomeWorkflow.samples . Generally, WDL constructs its inputs in the format of <Workflow name>.<input variable name> . Thus, inputs.json would appear, in part, like { ... \"SomeWorkflow.samples\": \"Array[String]\", ... } As mentioned above, the \"value\" (here, \"Array[String]\" ) does not matter; Broad's WOMTool will typically fill-in the expected type (as a string) to serve as a cue. Finally, WebMEV has to know which inputs of the Operation correspond to which inputs of the WDL script. Thus, in our operation_spec.json , the keys in our inputs object must be consistent with main.wdl and inputs.json : { ... \"inputs\": { ... \"SomeWorkflow.samples\": <OperationInput> ... } } Converting inputs As with all analysis execution modes, we have \"converter\" classes (specified in operation_spec.json ) which translate user inputs into formats that are compatible with the job runner. For instance, using the example above, one of the inputs for a WDL could be an array of strings ( Array[String] in WDL-type syntax). Thus, a converter would be responsible for taking say, an ObservationSet , and turning that into a list of strings to provide the same names. For example, we may wish to convert an ObservationSet given as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } Then, the \"inputs\" data structure submitted to Cromwell (basically inputs.json after it has been filled-in) would, in part, look like: { ... \"SomeWorkflow.samples\": [\"sampleA\", \"sampleB\"], ... } Creation of Docker images As described above, each repository can contain an arbitrary (non-zero) number of WDL files, each of which can depend on one more Docker images for their runtime. In contrast to local jobs which can only use a single image, WDL/Cromwell-based jobs can use any number of images. However, there are some custom steps involved during the ingestion of new Cromwell-based workflow, which we explain below. When Docker images are specified in the runtime section of the WDL files, the line is formatted as: runtime { ... docker: \"<repo name>/<username>/<image name>:<tag>\" ... } e.g. runtime { ... docker: \"docker.io/myUser/foo:v1\" ... } When using pre-built Docker images: Recall that if there is no explicit image \"tag\", then Docker defaults to using the image with the \"latest\" tag, which is implicitly the last build. Thus, if you are using an external Docker image (e.g. biocontainers/samtools) for one of your WDL tasks, then be sure to pick a specific tagged version so that it is not ambiguous, since latest can change over time. When creating our own Docker images: As mentioned in the local tools section, by default we use Github Actions to automatically build Docker images off the current repository state. The images are tagged with the github commit hash. If you have configured WebMEV to use the Dockerhub repo by default, then just be sure to build/tag/push your images carefully. That is, if you use docker: \"docker.io/myUser/foo:v1\" in your WDL, you need to name+tag your image, e.g. docker build -t myUser/foo:v1 . When using a custom Docker image built directly by Github Actions, we have a bit of a \"chicken or egg\" problem when it comes to providing tagged Docker images in our WDL files. As configured, Github Actions tags the Docker image with the commit hash. However, we obviously can't know that hash up front and use it in our WDL file(s). Hence, we reference untagged images in the docker attribute as follows: runtime { ... docker: \"ghcr.io/web-mev/mev-hcl\" ... } As part of the process of ingesting analysis tools into WebMEV, we modify that docker WDL attribute to attach a tag corresponding to the git commit. The process is roughly: Parse all WDL files in the github repo and extract out all the runtime Docker images. This will be a set of strings. For each Docker untagged \"image string\" (e.g. ghcr.io/someUser/foo ): Get the current git commit (say, abc123 ) and append that (e.g. ghcr.io/someUser/foo:abc123 ) Search for that image. If it does not exist, fail the ingestion process. This effectively prevents untagged images from being used. For instance, if an untagged docker.io/biocontainers/samtools Docker image was specifed, the full, tagged image would be docker.io/bioco ntainers/samtools:abc123 . There is a vanishingly small chance that our git repository's commit hash has a corresponding tag in the biocontainers/samtools` library. For each tagged \"image string\" (e.g. docker.io/biocontainers/samtools:v1.9-4-deb_cv1 ), simply search for it. We note that this technically modifies the workflow relative to the github repository, so the WebMEV-internal version is not exactly the same. However, this difference is limited to the name of the Docker image. All other aspects of the analysis are able to be exactly recreated based on the repository. Copying of static resources If the static_inputs.json file is present, we expect that this file will be used for static items that are not dependent on user input. We could also put such items as default in the operation_spec.json file, but we instead choose to extract them out to this file. At current, no tools use this feature, but this will be updated if necessary.","title":"Additional notes:"},{"location":"elements/","text":"Elements, Observations, and Features We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from the BaseElement class, which captures their common structure and behavior. Specialization for each can be overridden in the child classes. In the context of a biological experimental, Observation s are synonymous with samples. Further, each Observation can have Feature s associated with it (e.g. gene expressions for 30,000 genes). One can think of Observation s and Feature s as comprising the columns and rows of a two-dimensional matrix. Note that in our convention, due to the typical format of expression matrices, we take each column to represent an Observation and each row to represent a Feature . We use Observation s and Feature s to hold metadata (as key-value pairs) about data that we manipulating in WebMEV. For instance, given a typical gene expression matrix we have information about only the names of the Observation s/samples and Feature s/genes. We can then specify attributes to annotate the Observation s and Feature s, allowing users to define experimental groups, or specify other information useful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class data_structures.element. BaseElement ( val , **kwargs ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity and potential customization, we keep those entities separate. As a type of attribute, an Element (using an Observation below) would look like: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } We require that all Element instances be created with an identifier. Equality (e.g. in set operations) is checked using this identifier member The nested attributes are objects that dictate a simple attribute For instance: { \"id\": <string identifier>, \"attributes\": { \"stage\": { \"attribute_type\": \"String\", \"value\": \"IV\" }, \"age\": { \"attribute_type\": \"PositiveInteger\", \"value\": 5 } } } The nested dict attributes CAN be empty. In situations like annotation tables where certain rows may not have values (but others do), we want to be able to permit null attributes if the constructor is explicitly passed the permit_null_attributes kwarg class data_structures.observation. Observation ( val , **kwargs ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class data_structures.feature. Feature ( val , **kwargs ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Observations and Features"},{"location":"elements/#elements-observations-and-features","text":"We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from the BaseElement class, which captures their common structure and behavior. Specialization for each can be overridden in the child classes. In the context of a biological experimental, Observation s are synonymous with samples. Further, each Observation can have Feature s associated with it (e.g. gene expressions for 30,000 genes). One can think of Observation s and Feature s as comprising the columns and rows of a two-dimensional matrix. Note that in our convention, due to the typical format of expression matrices, we take each column to represent an Observation and each row to represent a Feature . We use Observation s and Feature s to hold metadata (as key-value pairs) about data that we manipulating in WebMEV. For instance, given a typical gene expression matrix we have information about only the names of the Observation s/samples and Feature s/genes. We can then specify attributes to annotate the Observation s and Feature s, allowing users to define experimental groups, or specify other information useful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class data_structures.element. BaseElement ( val , **kwargs ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity and potential customization, we keep those entities separate. As a type of attribute, an Element (using an Observation below) would look like: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } We require that all Element instances be created with an identifier. Equality (e.g. in set operations) is checked using this identifier member The nested attributes are objects that dictate a simple attribute For instance: { \"id\": <string identifier>, \"attributes\": { \"stage\": { \"attribute_type\": \"String\", \"value\": \"IV\" }, \"age\": { \"attribute_type\": \"PositiveInteger\", \"value\": 5 } } } The nested dict attributes CAN be empty. In situations like annotation tables where certain rows may not have values (but others do), we want to be able to permit null attributes if the constructor is explicitly passed the permit_null_attributes kwarg class data_structures.observation. Observation ( val , **kwargs ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class data_structures.feature. Feature ( val , **kwargs ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Elements, Observations, and Features"},{"location":"example_workflow/","text":"Example workflow To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Workflow and analysis concepts"},{"location":"example_workflow/#example-workflow","text":"To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Example workflow"},{"location":"general_architecture/","text":"General architecture of WebMEV WebMEV is a Django-based RESTful API web application with the following components: Components located within the dotted outline are always located on the same server. Their roles are: The nginx server accepts requests on port 80. If the request is for static content (resources such as CSS files located at /static/ ) then nginx will directly respond to the request. Note that there are minimal CSS and JS static files associated with Django Rest Framework's browsable API. It is expected that the API will be accessed with an appropriate frontend and thus there are minimal static files related to rendering user interfaces. The gunicorn application server handles non-static requests and is the interface between nginx and Django. These connect through a unix socket. For the database - We use a postgres database to store information about users, their files and workspaces, and metadata about the analysis operations and their executions. Depending on the deployment environment, the postgres server is implemented either on the host machine or using a cloud-based service . - For local, Vagrant/virtualbox-based deployments, postgres is directly installed on the host VM. - For cloud-based deployments, we connect to a cloud-based postgres instance. For GCP, the connection is established by use of Google's cloud SQL proxy (https://cloud.google.com/sql/docs/postgres/sql-proxy). This software allows the cloud-based VM to securely connect to the database instance and creates a socket on the VM. Django can then communicate with the database via this socket as if the database server were also located on the VM. The Cromwell server To run larger, more computationally demanding jobs we connect WebMEV to a remote Cromwell job scheduler which provides on-demand compute resources. When implemented as part of a cloud-based deployment, Cromwell resides on an independent VM. Job requests are sent from the WebMEV server to Cromwell's RESTful API. Upon receiving the necessary components of a job request (WDL scripts, JSON inputs), Cromwell orchestrates the provisioning of hardware to execute the job(s). After completion, Cromwell handles the destruction of these ephemeral resources and places any output files into storage buckets. WebMEV handles the querying of job status, relocation of result files, and any other WebMEV-specific business logic.","title":"Architecture"},{"location":"general_architecture/#general-architecture-of-webmev","text":"WebMEV is a Django-based RESTful API web application with the following components: Components located within the dotted outline are always located on the same server. Their roles are: The nginx server accepts requests on port 80. If the request is for static content (resources such as CSS files located at /static/ ) then nginx will directly respond to the request. Note that there are minimal CSS and JS static files associated with Django Rest Framework's browsable API. It is expected that the API will be accessed with an appropriate frontend and thus there are minimal static files related to rendering user interfaces. The gunicorn application server handles non-static requests and is the interface between nginx and Django. These connect through a unix socket. For the database - We use a postgres database to store information about users, their files and workspaces, and metadata about the analysis operations and their executions. Depending on the deployment environment, the postgres server is implemented either on the host machine or using a cloud-based service . - For local, Vagrant/virtualbox-based deployments, postgres is directly installed on the host VM. - For cloud-based deployments, we connect to a cloud-based postgres instance. For GCP, the connection is established by use of Google's cloud SQL proxy (https://cloud.google.com/sql/docs/postgres/sql-proxy). This software allows the cloud-based VM to securely connect to the database instance and creates a socket on the VM. Django can then communicate with the database via this socket as if the database server were also located on the VM. The Cromwell server To run larger, more computationally demanding jobs we connect WebMEV to a remote Cromwell job scheduler which provides on-demand compute resources. When implemented as part of a cloud-based deployment, Cromwell resides on an independent VM. Job requests are sent from the WebMEV server to Cromwell's RESTful API. Upon receiving the necessary components of a job request (WDL scripts, JSON inputs), Cromwell orchestrates the provisioning of hardware to execute the job(s). After completion, Cromwell handles the destruction of these ephemeral resources and places any output files into storage buckets. WebMEV handles the querying of job status, relocation of result files, and any other WebMEV-specific business logic.","title":"General architecture of WebMEV"},{"location":"install/","text":"Installation instructions See the instructions in the main repository page for local, Vagrant-based builds and for cloud deployments.","title":"Install"},{"location":"install/#installation-instructions","text":"See the instructions in the main repository page for local, Vagrant-based builds and for cloud deployments.","title":"Installation instructions"},{"location":"management_commands/","text":"Django management commands in WebMeV There are several custom Django management commands available. For the most part, these are used by processes such as provisioning scripts. However, there are several that are actively used by WebMeV admins. We list these below and note what they do and how they are used. All of these are initiated using the standard Django python3 manage.py <command> calls. Help for each can be found by appending the -h flag. Commands for working public datasets WebMeV provides these commands for managing public datasets (e.g. TCGA) within the application. Rather than relying on cron jobs or other periodic mechanisms for data import/indexing, we provide these commands so admins can actively manage these datasets. pull_public_data This command calls the underlying implementation for a given public dataset. Provide the unique string identifier to start the data download/processing: usage: manage.py pull_public_data -d DATASET_ID This command only performs download and data munging- it does not modify any database tables or expose any new datasets. It only handles the preparation of the data that will ultimately be indexed in another step. Note that each public dataset has, in general, different formats and requirements. We expect that the implementation of this command for each public dataset will appropriately inform the admin of its actions and any output files. For instance, the TCGA dataset icreates two files- a metadata file and a count matrix. index_data This command will index one or more flat files into the specified dataset. This command modifies the database so that it's available for querying and usage. usage: manage.py index_data -d DATASET_ID <key=value> [<key=value>] ... where the >=1 key-value pairs are specific to the dataset we are indexing. Those parameters tell the indexing process the identity of the files. Recall that each dataset will have different requirements and files, in general. As an example, consider the TCGA RNA-seq dataset. Here, the pull_public_data command prepares a HDF5-format file of gene expression counts and a CSV-format file of sample annotations. To index RNA-seq data from TCGA (and most RNA-seq datasets), we would run: manage.py index_data -d tcga-rnaseq annotations=<path to annotation CSV file> counts=<path to HDF5 file> For the TCGA dataset, our process knows to look for these two key-value pairs. It knows to use the value paired with the annotations key as the file for indexing by Solr. The database then tracks both of these files so that we can query and prepare datasets for users. Note that this assumes a \"core\" (in solr parlance) already exists for the dataset; typically these are created during machine provisioning. Since each dataset requires some amount of manual curation, we still have to do the work of understanding each dataset and preparing the necessary Solr core files. Other commands add_static_operations This is used to add \"static\" operations like file transfers (e.g. the Drobox+Cromwell flow) to the application. This is different than the \"dynamic\" analysis operations that are added after the WebMeV application is running. This is typically called by the provisioning script and does not need to be run manually. In principle, one could choose to amend this script to automatically ingest a set of analysis applications. build_docs This command creates this documentation. If the --push flag is added, then the documentation will be pushed to github to the gh-pages branch. By default, it is not pushed so you can inspect the pages locally. dump_db_for_test This is a thin wrapper around the Django dumpdata command, which will dump the contents of the database models under api to a JSON file. That data can then be used for the test suite. Note that the repository includes a api/tests/test_db.json file, so this command would typically be run by a developer to add new data to the test database. populate_db This command will add some dummy data to the database. The data added are a minimum set of database records that will allow for unit testing. As mentioned, the repository includes a test database file at api/tests/test_db.json . However, one could instead use this command to populate the database with new data, then subsequently use the dumb_db_for_test command above to create a new api/tests/test_db.json file.","title":"Management commands"},{"location":"management_commands/#django-management-commands-in-webmev","text":"There are several custom Django management commands available. For the most part, these are used by processes such as provisioning scripts. However, there are several that are actively used by WebMeV admins. We list these below and note what they do and how they are used. All of these are initiated using the standard Django python3 manage.py <command> calls. Help for each can be found by appending the -h flag.","title":"Django management commands in WebMeV"},{"location":"management_commands/#commands-for-working-public-datasets","text":"WebMeV provides these commands for managing public datasets (e.g. TCGA) within the application. Rather than relying on cron jobs or other periodic mechanisms for data import/indexing, we provide these commands so admins can actively manage these datasets.","title":"Commands for working public datasets"},{"location":"management_commands/#pull_public_data","text":"This command calls the underlying implementation for a given public dataset. Provide the unique string identifier to start the data download/processing: usage: manage.py pull_public_data -d DATASET_ID This command only performs download and data munging- it does not modify any database tables or expose any new datasets. It only handles the preparation of the data that will ultimately be indexed in another step. Note that each public dataset has, in general, different formats and requirements. We expect that the implementation of this command for each public dataset will appropriately inform the admin of its actions and any output files. For instance, the TCGA dataset icreates two files- a metadata file and a count matrix.","title":"pull_public_data"},{"location":"management_commands/#index_data","text":"This command will index one or more flat files into the specified dataset. This command modifies the database so that it's available for querying and usage. usage: manage.py index_data -d DATASET_ID <key=value> [<key=value>] ... where the >=1 key-value pairs are specific to the dataset we are indexing. Those parameters tell the indexing process the identity of the files. Recall that each dataset will have different requirements and files, in general. As an example, consider the TCGA RNA-seq dataset. Here, the pull_public_data command prepares a HDF5-format file of gene expression counts and a CSV-format file of sample annotations. To index RNA-seq data from TCGA (and most RNA-seq datasets), we would run: manage.py index_data -d tcga-rnaseq annotations=<path to annotation CSV file> counts=<path to HDF5 file> For the TCGA dataset, our process knows to look for these two key-value pairs. It knows to use the value paired with the annotations key as the file for indexing by Solr. The database then tracks both of these files so that we can query and prepare datasets for users. Note that this assumes a \"core\" (in solr parlance) already exists for the dataset; typically these are created during machine provisioning. Since each dataset requires some amount of manual curation, we still have to do the work of understanding each dataset and preparing the necessary Solr core files.","title":"index_data"},{"location":"management_commands/#other-commands","text":"","title":"Other commands"},{"location":"management_commands/#add_static_operations","text":"This is used to add \"static\" operations like file transfers (e.g. the Drobox+Cromwell flow) to the application. This is different than the \"dynamic\" analysis operations that are added after the WebMeV application is running. This is typically called by the provisioning script and does not need to be run manually. In principle, one could choose to amend this script to automatically ingest a set of analysis applications.","title":"add_static_operations"},{"location":"management_commands/#build_docs","text":"This command creates this documentation. If the --push flag is added, then the documentation will be pushed to github to the gh-pages branch. By default, it is not pushed so you can inspect the pages locally.","title":"build_docs"},{"location":"management_commands/#dump_db_for_test","text":"This is a thin wrapper around the Django dumpdata command, which will dump the contents of the database models under api to a JSON file. That data can then be used for the test suite. Note that the repository includes a api/tests/test_db.json file, so this command would typically be run by a developer to add new data to the test database.","title":"dump_db_for_test"},{"location":"management_commands/#populate_db","text":"This command will add some dummy data to the database. The data added are a minimum set of database records that will allow for unit testing. As mentioned, the repository includes a test database file at api/tests/test_db.json . However, one could instead use this command to populate the database with new data, then subsequently use the dumb_db_for_test command above to create a new api/tests/test_db.json file.","title":"populate_db"},{"location":"metadata/","text":"About Workspace metadata As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"WT\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side; the API will not keep the additional information about groupings that the user has defined. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. The creation and visualization of custom ObservationSet s is merely a convenience provided by the front-end (if available). After all, in direct requests to the API for analyses that require OperationSet s, the requester can create those at will. Using the metadata for analyses After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } } Additional user-supplied metadata Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex age S1 M 43 S2 F 44 S3 F 54 S4 F 33 S5 M 65 S6 F 58 Users can then create custom groups of samples This would then incorporate into the existing Observation instances so they now would look like: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 43 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"M\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 58 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"F\" } } } ] } Backend endpoints To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/observations/ which will track the union of all the Resource metadata (specifically Observation s) in the Workspace . Note that there is currently no endpoint for querying Feature s since there are often too many to make this feasible. The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 43 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"M\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 58 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"F\" } } }, { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] }","title":"Workspace metadata"},{"location":"metadata/#about-workspace-metadata","text":"As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"WT\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": { \"attribute_type\": \"String\", \"value\": \"mutant\" } } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side; the API will not keep the additional information about groupings that the user has defined. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. The creation and visualization of custom ObservationSet s is merely a convenience provided by the front-end (if available). After all, in direct requests to the API for analyses that require OperationSet s, the requester can create those at will.","title":"About Workspace metadata"},{"location":"metadata/#using-the-metadata-for-analyses","text":"After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } }","title":"Using the metadata for analyses"},{"location":"metadata/#additional-user-supplied-metadata","text":"Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex age S1 M 43 S2 F 44 S3 F 54 S4 F 33 S5 M 65 S6 F 58 Users can then create custom groups of samples This would then incorporate into the existing Observation instances so they now would look like: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 43 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"M\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 58 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"F\" } } } ] }","title":"Additional user-supplied metadata"},{"location":"metadata/#backend-endpoints","text":"To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/observations/ which will track the union of all the Resource metadata (specifically Observation s) in the Workspace . Note that there is currently no endpoint for querying Feature s since there are often too many to make this feasible. The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 43 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"M\" } } }, ... { \"id\": \"S6\", \"attributes\": { \"age\": { \"attribute_type\": \"Integer\", \"value\": 58 }, \"sex\": { \"attribute_type\": \"UnrestrictedString\", \"value\": \"F\" } } }, { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] }","title":"Backend endpoints"},{"location":"observation_and_feature_sets/","text":"ObservationSet and FeatureSet We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class data_structures.element_set. BaseElementSet ( val , **kwargs ) A BaseElementSet is a collection of unique BaseElement instances (more likely the derived children classes) and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet (a child of BaseElementSet) is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable BaseElement instances (and children). This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate BaseElement s, in contrast to native sets which silently ignore duplicate elements. A serialized representation (where the concrete BaseElement type is Observation ) would look like: { \"elements\": [ <Observation>, <Observation>, ... ] } class data_structures.observation_set. ObservationSet ( val , **kwargs ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"elements\": [ <Observation>, <Observation>, ... ] } class data_structures.feature_set. FeatureSet ( val , **kwargs ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSets"},{"location":"observation_and_feature_sets/#observationset-and-featureset","text":"We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class data_structures.element_set. BaseElementSet ( val , **kwargs ) A BaseElementSet is a collection of unique BaseElement instances (more likely the derived children classes) and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet (a child of BaseElementSet) is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable BaseElement instances (and children). This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate BaseElement s, in contrast to native sets which silently ignore duplicate elements. A serialized representation (where the concrete BaseElement type is Observation ) would look like: { \"elements\": [ <Observation>, <Observation>, ... ] } class data_structures.observation_set. ObservationSet ( val , **kwargs ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"elements\": [ <Observation>, <Observation>, ... ] } class data_structures.feature_set. FeatureSet ( val , **kwargs ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSet"},{"location":"operation_resources/","text":"Operation resources (Note that this feature is not fully implemented yet, and may change. Update as necessary. The ideas below are sketches) For certain Operation s, we require data such as gene lists, gene-alias lookups, genome indices, and similar files which are not provided or owned by WebMEV users. Instead, these special Resource s are associated with the Operation they are used with. Suggestively, we call these OperationResource instances. Depending on the run-mode of the Operation , these OperationResource s are handled in different ways. Regardless of the run-mode of the Operation (i.e. local, remote), we can handle these operation-specific resources in two ways. One method relies on the image container to hold the file. The other method requires WebMEV to \"register\" the OperationResource just as it does with user-associated Resource s . The first method to distribute these user-independent files is to simply build them directly into the container. For small files, this solution is straightforward and does not require any special handling by WebMEV itself. For reproducing analyses, the versioned container image can always be pulled and run, ensuring the same files are used. Note that this assumes the files are \"static\" within that container and not dynamically queried/generated when the container is executed for an analysis operation. For instance, if the gene annotations, etc. are queried from BioMart on-the-fly during an analysis execution, then we can't necessarily guarantee 100% fidelity as the underlying data may change. On the other hand, if the files are created upon building/pushing the image, then those files can be safely distributed with the image and will remain unchanged within that image. Instead of building the files directly into the container image, one may also choose to create the files up front and then have WebMEV \"register\" them when the Operation is ingested. We do this by including an addition file ( operation_resources.json ) with the repository. This file gives the \"name\" of the file, its resource type (e.g. integer matrix, BED file, etc.), and the path to the file. For example, if we have an alignment process that depends on a pre-computed index, our operation_spec.json definition file would have an OperationResource input as follows: { \"name\": \"BWA alignment\", ... \"inputs\": { ... \"genome_index\": { \"description\": \"The genome to align to.\", \"name\": \"Genome choice:\", \"required\": true, \"spec\": { \"attribute_type\": \"OperationResource\", \"resource_type\": \"*\" } } ... } ... } (the resource_type wildcard means \"any\" file type, which is fine for our purposes here). Then, in the same repository, we would have an operation_resources.json that looks like: { \"genome_index\": [ { \"name\": \"Human\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/human_index.tar\" }, { \"name\": \"Mouse\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/mouse_index.tar\" } ] } Upon ingestion of this Operation , WebMEV will check for the presence of those files and, if present, will create database objects for these files. When the user wishes to run this Operation , those resources will be presented as the available options for that genome_index input. In accordance with our goal of reproducible research, the ingestion process copies the files to an operation-specific directory. Thus, given the Operation 's UUID, we will copy the file(s) to a storage location identified by that UUID. This way, we can prevent that original path from being overwritten or deleted over time. Note that the path provided may either be \"local\" (e.g. \"path\": \"some_file.tsv\" ) or remote (e.g. \"path\": \"gs://my-bucket/human_index.tar ). In the former case (where the path is relative or does not specify a \"special\" prefix like \"gs://\") WebMEV expects the file to be among the cloned repository files. Otherwise, we confirm the existence of the file using the appropriate libraries capable of interfacing with the remote storage systems. If the files are not found, then the ingestion of the Operation fails. The \"repository-tracked\" files are limited to relatively small files, however. Larger files, such as genome indexes which can be many GB, must be done using the remote storage option, which does require a bit more care to ensure everything is in-sync prior to ingestion. After ingestion, everything is properly versioned by reference to the Operation s UUID. As a final note, in the spirit of reproducible analyses, we advise that all scripts, etc. used to create the resource files are included in the repository or are otherwise adequately described. This is not something that can be enforced by WebMEV, but we aim to adhere with this guideline.","title":"Operation-associated resources"},{"location":"operation_resources/#operation-resources","text":"(Note that this feature is not fully implemented yet, and may change. Update as necessary. The ideas below are sketches) For certain Operation s, we require data such as gene lists, gene-alias lookups, genome indices, and similar files which are not provided or owned by WebMEV users. Instead, these special Resource s are associated with the Operation they are used with. Suggestively, we call these OperationResource instances. Depending on the run-mode of the Operation , these OperationResource s are handled in different ways. Regardless of the run-mode of the Operation (i.e. local, remote), we can handle these operation-specific resources in two ways. One method relies on the image container to hold the file. The other method requires WebMEV to \"register\" the OperationResource just as it does with user-associated Resource s . The first method to distribute these user-independent files is to simply build them directly into the container. For small files, this solution is straightforward and does not require any special handling by WebMEV itself. For reproducing analyses, the versioned container image can always be pulled and run, ensuring the same files are used. Note that this assumes the files are \"static\" within that container and not dynamically queried/generated when the container is executed for an analysis operation. For instance, if the gene annotations, etc. are queried from BioMart on-the-fly during an analysis execution, then we can't necessarily guarantee 100% fidelity as the underlying data may change. On the other hand, if the files are created upon building/pushing the image, then those files can be safely distributed with the image and will remain unchanged within that image. Instead of building the files directly into the container image, one may also choose to create the files up front and then have WebMEV \"register\" them when the Operation is ingested. We do this by including an addition file ( operation_resources.json ) with the repository. This file gives the \"name\" of the file, its resource type (e.g. integer matrix, BED file, etc.), and the path to the file. For example, if we have an alignment process that depends on a pre-computed index, our operation_spec.json definition file would have an OperationResource input as follows: { \"name\": \"BWA alignment\", ... \"inputs\": { ... \"genome_index\": { \"description\": \"The genome to align to.\", \"name\": \"Genome choice:\", \"required\": true, \"spec\": { \"attribute_type\": \"OperationResource\", \"resource_type\": \"*\" } } ... } ... } (the resource_type wildcard means \"any\" file type, which is fine for our purposes here). Then, in the same repository, we would have an operation_resources.json that looks like: { \"genome_index\": [ { \"name\": \"Human\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/human_index.tar\" }, { \"name\": \"Mouse\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/mouse_index.tar\" } ] } Upon ingestion of this Operation , WebMEV will check for the presence of those files and, if present, will create database objects for these files. When the user wishes to run this Operation , those resources will be presented as the available options for that genome_index input. In accordance with our goal of reproducible research, the ingestion process copies the files to an operation-specific directory. Thus, given the Operation 's UUID, we will copy the file(s) to a storage location identified by that UUID. This way, we can prevent that original path from being overwritten or deleted over time. Note that the path provided may either be \"local\" (e.g. \"path\": \"some_file.tsv\" ) or remote (e.g. \"path\": \"gs://my-bucket/human_index.tar ). In the former case (where the path is relative or does not specify a \"special\" prefix like \"gs://\") WebMEV expects the file to be among the cloned repository files. Otherwise, we confirm the existence of the file using the appropriate libraries capable of interfacing with the remote storage systems. If the files are not found, then the ingestion of the Operation fails. The \"repository-tracked\" files are limited to relatively small files, however. Larger files, such as genome indexes which can be many GB, must be done using the remote storage option, which does require a bit more care to ensure everything is in-sync prior to ingestion. After ingestion, everything is properly versioned by reference to the Operation s UUID. As a final note, in the spirit of reproducible analyses, we advise that all scripts, etc. used to create the resource files are included in the repository or are otherwise adequately described. This is not something that can be enforced by WebMEV, but we aim to adhere with this guideline.","title":"Operation resources"},{"location":"operations/","text":"Operations and ExecutedOperations An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository_url\": <string url>, \"repo_name\": <string>. \"git_hash\": <string>, \"workspace_operation\": <bool> } where: mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) workspace_operation : This bool tells us whether the operation is run within the context of a user's workspace. Some operations, such as file uploads, can be run outside of a workspace and hence don't require some of the additional fields that a workspace-associated operation would use. repository_url , repo_name : identifies the github repo (and name) used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"converter\": <string>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). The spec allows us to validate a user's input for compliance with the expected type; for instance, when an analysis is requested, the spec ensures that we get sensible inputs for that field. The converter key addresses a \"dot-style\" Python class which takes the input value, e.g. 0.05 and casts to a float. In this case, the converter is trivial, but more complex transformations can be created. ExecutedOperation s Once a user makes a request with the proper, validated inputs, we create an ExecutedOperation which tracks the execution of the job. An ExecutedOperation tracks the following: The Workspace , assuming the underlying Operation has workspace_operation=True . This gives access to the user who initiated the execution. a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings A concrete example For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\": \"DESeq2\", \"description\": \"Execute a simple differential expression analysis comparing two groups of samples.\", \"inputs\": { \"raw_counts\": { \"description\": \"The input raw count matrix. Must be an integer-based table.\", \"name\": \"Count matrix:\", \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleVariableDataResourceConverter\", \"spec\": { \"attribute_type\": \"VariableDataResource\", \"resource_types\": [\"I_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"base_condition_samples\": { \"description\": \"The set of samples that are in the \\\"base\\\" or \\\"control\\\" condition.\", \"name\": \"Control/base group samples:\", \"required\": true, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } }, \"experimental_condition_samples\": { \"description\": \"The set of samples that are in the \\\"treated\\\" or \\\"experimental\\\" condition.\", \"name\": \"Treated/experimental samples:\", \"required\": true, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } }, \"base_condition_name\": { \"description\": \"The condition that should be considered as the \\\"control\\\" or \\\"baseline\\\".\", \"name\": \"Base condition:\", \"required\": false, \"converter\": \"api.converters.basic_attributes.StringConverter\", \"spec\": { \"attribute_type\": \"String\", \"default\": \"Control\" } }, \"experimental_condition_name\": { \"description\": \"The condition that should be considered as the \\\"non-control\\\" or \\\"experimental\\\".\", \"name\": \"Experimental/treated condition\", \"required\": false, \"converter\": \"api.converters.basic_attributes.StringConverter\", \"spec\": { \"attribute_type\": \"String\", \"default\": \"Experimental\" } } }, \"outputs\": { \"dge_results\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\", \"many\": false } }, \"normalized_counts\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"EXP_MTX\", \"many\": false } } }, \"mode\": \"local_docker\", \"workspace_operation\": true } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields. Executing an Operation The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0]. Operation modes Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be built and pushed to Dockerhub during ingestion. The image will be tagged with the github commit hash so that the file(s) used to build the image can be uniquely identified and re-traced over time.","title":"Operation concepts"},{"location":"operations/#operations-and-executedoperations","text":"An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository_url\": <string url>, \"repo_name\": <string>. \"git_hash\": <string>, \"workspace_operation\": <bool> } where: mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) workspace_operation : This bool tells us whether the operation is run within the context of a user's workspace. Some operations, such as file uploads, can be run outside of a workspace and hence don't require some of the additional fields that a workspace-associated operation would use. repository_url , repo_name : identifies the github repo (and name) used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"converter\": <string>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"converter\": \"api.converters.basic_attributes.FloatConverter\", \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). The spec allows us to validate a user's input for compliance with the expected type; for instance, when an analysis is requested, the spec ensures that we get sensible inputs for that field. The converter key addresses a \"dot-style\" Python class which takes the input value, e.g. 0.05 and casts to a float. In this case, the converter is trivial, but more complex transformations can be created.","title":"Operations and ExecutedOperations"},{"location":"operations/#executedoperations","text":"Once a user makes a request with the proper, validated inputs, we create an ExecutedOperation which tracks the execution of the job. An ExecutedOperation tracks the following: The Workspace , assuming the underlying Operation has workspace_operation=True . This gives access to the user who initiated the execution. a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"ExecutedOperations"},{"location":"operations/#a-concrete-example","text":"For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\": \"DESeq2\", \"description\": \"Execute a simple differential expression analysis comparing two groups of samples.\", \"inputs\": { \"raw_counts\": { \"description\": \"The input raw count matrix. Must be an integer-based table.\", \"name\": \"Count matrix:\", \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleVariableDataResourceConverter\", \"spec\": { \"attribute_type\": \"VariableDataResource\", \"resource_types\": [\"I_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"base_condition_samples\": { \"description\": \"The set of samples that are in the \\\"base\\\" or \\\"control\\\" condition.\", \"name\": \"Control/base group samples:\", \"required\": true, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } }, \"experimental_condition_samples\": { \"description\": \"The set of samples that are in the \\\"treated\\\" or \\\"experimental\\\" condition.\", \"name\": \"Treated/experimental samples:\", \"required\": true, \"converter\": \"api.converters.element_set.ObservationSetCsvConverter\", \"spec\": { \"attribute_type\": \"ObservationSet\" } }, \"base_condition_name\": { \"description\": \"The condition that should be considered as the \\\"control\\\" or \\\"baseline\\\".\", \"name\": \"Base condition:\", \"required\": false, \"converter\": \"api.converters.basic_attributes.StringConverter\", \"spec\": { \"attribute_type\": \"String\", \"default\": \"Control\" } }, \"experimental_condition_name\": { \"description\": \"The condition that should be considered as the \\\"non-control\\\" or \\\"experimental\\\".\", \"name\": \"Experimental/treated condition\", \"required\": false, \"converter\": \"api.converters.basic_attributes.StringConverter\", \"spec\": { \"attribute_type\": \"String\", \"default\": \"Experimental\" } } }, \"outputs\": { \"dge_results\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\", \"many\": false } }, \"normalized_counts\": { \"required\": true, \"converter\": \"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"EXP_MTX\", \"many\": false } } }, \"mode\": \"local_docker\", \"workspace_operation\": true } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields.","title":"A concrete example"},{"location":"operations/#executing-an-operation","text":"The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0].","title":"Executing an Operation"},{"location":"operations/#operation-modes","text":"Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be built and pushed to Dockerhub during ingestion. The image will be tagged with the github commit hash so that the file(s) used to build the image can be uniquely identified and re-traced over time.","title":"Operation modes"},{"location":"public_data/","text":"Working with public datasets WebMeV provides the ability to easily create custom datasets derived from publicly available databases such as the Cancer Genome Atlas (TCGA). Query functionality of these datasets is exposed through Solr, which provides us a standard indexing and query syntax. Given that each data repository has different formats and content, this functionality requires some custom code to create and manage the data on a case-by-case basis. In this guide, we describe the necessary elements to create/index a new data source. In this guide, we will often refer to the TCGA RNA-seq as a prototypical example. This dataset contains count-based RNA-seq data downloaded from the NCI's genomic data commons (GDC) repository under the TCGA project. The quick way: For convenience, there is a management command that handles most of the steps. Be sure to pay attention to the log messages, as they will direct you to change various things. If that script fails for whatever reason, we provide all the atomic details below. Run the following in your Vagrant build: python3 manage.py create_new_public_dataset \\ -d <core name> -f <path to an example file you want to index> The core name ( -d ) should be relatively short/simple (e.g. ccle-data ) and the the file ( -f ) should be an example of the data you wish to index. It can be a row subset (not ALL the data), but should have all the potential fields. Note that the script will still require you to fill out the stubbed-out python module for the new dataset AND also require you to add some content to the Puppet manifests to fully integrate this new core. The script will direct you to the stubs and remind you what needs to be done. You will also need to commit the solr files ( solr/<core>/schema.xml and solr/<core>/solrconfig.xml ) when everything is fully ready. The longer way (or the way if the script above does not work!) Step 1: Defining and creating the data To create a new public dataset, we need to define how the data is collected and prepared. This section describes how to write the proper hooks for data ingestion and preparation. 1.1 Create a new directory for your dataset To keep everything organized, we expect that each dataset will be kept in separate subdirectories of the mev/api/public_data/sources directory. However, note that there is no enforcement of this convention or any expected hierarchy of those subdirectories. Example: Note that mev/api/public_data/sources contains a gdc subdirectory and was created with the intent of holding all GDC projects; beyond TCGA, there are many other datasets exposed via the GDC repository. We expect that more of these GDC datasets will be included over time, so this was a logical way to structure the folder. You may choose to structure new datasets in an alternate manner. The gdc directory contains several python modules which define how the GDC-derived datasets are to be downloaded and prepared. We define a gdc.py module which contains code expected to be common to all GDC projects. Code specific to preparation of TCGA data is contained in the tcga.py module. 1.2 Create an implementing class To provide a common means of ingesting/preparing all datasets, we expect that each dataset will be mapped 1:1 with a Python class that derives from api.public_data.sources.base.PublicDataset . This class requires the following: Class attributes: TAG : This is a unique string which acts as an identifier for the dataset. If the name is not unique, then registering the dataset in the database will not be permitted, and hence the indexed dataset will not be usable. Additionally, the unit test suite will check that all implementing classes have unique identifiers. This string is limited to 30 characters by a database constraint. PUBLIC_NAME : This string should be a relatively short \"name\" for the dataset, such as \"TCGA RNA-seq\" DESCRIPTION : This is another string which provides more context and a thorough description of the data is contains. DATASET_FILES : This is a list which enumerates the files necessary for this dataset. For example, in the TCGA RNA-seq dataset, we have the metadata/annotations file, and we have a separate count file. This list of used to ensure that the proper files are present when indexing and creating datasets. Methods: prepare(self) : a method that takes no arguments (other than the class instance) and prepares the data. A return value is not expected. verify_files(self, file_dict) : a method that takes a dictionary with keys that should match those in DATASET_FILES and the values are file paths so we can verify that the files exist. get_indexable_files(self, file_dict) : a method that iterates through the passed dictionary and returns a list of file paths for files that are to be indexed. get_additional_metadata(self) : a method that returns a dictionary containing additional information that is relevant to the dataset, but not part of the traditional data or metadata. An example from the TCGA project is a mapping of the TCGA IDs (e.g. \"TCGA-LUAD\") to full names (e.g. \"Lung adenocarcinoma\"). The results of this method are placed into the additional_metadata JSON field in the database. There is no defined structure so that each dataset will be different in general. create_from_query(self, query_params) : a method that takes a dict which specifies how to create the dataset. Think of the dict as a generic payload for filtering, etc. Returns a tuple of a filepath(string) and a resource type string (one of our \"special\" types like \"EXP_MTX\" ) A template for this new class: from api.public_data.sources.base import PublicDataSource class MyDataset(PublicDataSource): TAG = '' PUBLIC_NAME = '' DESCRIPTION = '' DATASET_FILES = [] def prepare(self): pass def verify_files(self, file_dict): pass def get_indexable_files(self, file_dict): pass def create_from_query(self, query_params): pass Note that you are not obligated to derive your implementation directly from PublicDataset ; for the TCGA RNA-seq example, we created a hierarchy of PublicDataset -> GDCDataSource -> TCGADataSource -> TCGARnaSeqDataSource which reflects the fact that certain functionality is general to all GDC data sources and hence can be re-used if we incorporate other datasets from the GDC. Also note that we make no requirements for the prepare method. In its simplest implementation, the dataset can be manually prepared and the prepare method can be left as an empty pass-through. This is the easiest option for datasets that are not actively updated or are difficult to automate. However, you still need to define this implementing class, even if the implementation is trivial . More generally, the prepare method is the entry function for potentially many steps of data download and preparation. Again, while not enforced, we expect that the prepared data will ultimately be located under the /data/public_data directory so that it will be consistent with other data sources and can be persisted on redeployments. 1.3 \"Register\" the implementing class To allow WebMeV to \"see\" this new implementation, we have to add it to the IMPLEMENTING_CLASSES list in api/public_data/__init__.py . If this is not done, you will receive errors that will warn you of an unknown dataset. Step 2: Define your solr core The process of adding new cores for additional datasets is handled during provisioning and requires changes to the Puppet scripts. However, prior to that point, we need to create the proper files that will be used to create this new core. This section covers details of how to create these files. To do this, below we will perform the following: Create a new (but ephemeral) Solr core Index an example file from your dataset using Solr's auto-detection functionality Query the auto-generated schema and edit it to ensure the inferred field types are correct. After these steps, we will have the necessary files which we can add to the WebMeV repository. The provisioning process will then use those files to add the new core. 2.1 Creating the core This step assumes you have a Vagrant-based system up and have SSH'd in. By default, you are the vagrant user. Important: the name of the Solr core you create MUST match the TAG attribute of your implementing class . This is how the implementing class knows which Solr core to query. Run: sudo -u solr /opt/solr/bin/solr create_core -c <core name> Solr should report that the new core was created. Step 3: Obtaining and modifying the data schema 3.1 Index an example file Above, Solr will create a default/schemaless core. For our purposes in WebMeV, the data is typically more structured and we have some notion of data structures and types in a given dataset (e.g. age as an integer type). Furthermore, we don't want to necessarily rely on Solr to properly guess the types of various fields. Hence, we will first use Solr to auto-index an example file. Following this, we will request a \"dump\" of the current schema which we will subsequently edit to fit our needs. Ideally, the example file you use below is very close to the final data you are hoping to index (or is the data you want to expose). Mismatches between the schema and the data files will cause failures. To index the example file: /opt/solr/bin/post -c <core name> <file path> 3.2 Query for the current schema and edit Now, we will query for the current/inferred schema that was just created. Here, we put this in the /vagrant/solr directory, but it does not really matter where it goes, as long as you remember. curl http://localhost:8983/solr/<core name>/schema?wt=schema.xml >/vagrant/solr/current_schema.xml This curl request will provide the structured XML schema that is currently supporting the core. At the top of the current_schema.xml file you will see many field type ( <fieldType> ) entries, which can be left as-is. You may, however, wish to remove those that are (likely) unnecessary. Examples include various tokenizer classes and filters corresponding to free-text analyzers and considerations for foreign language support. Those fields are not likely to be too relevant for most biomedical data we are analyzing within WebMeV. However, it is also fine to leave those all as-is. You will , however, want to review and potentially make edits to the <field> entities located towards the tail of the schema. You should see field names corresponding to the columns/fields of the example file you indexed before. Depending on your data, you may choose to edit the type attributes. For instance, the default may be something like: <field name=\"year_of_birth\" type=\"pdoubles\"/> but we may wish to change that to integers: <field name=\"year_of_birth\" type=\"pint\"/> Similarly, many string-based fields default to a type of text_general which causes solr to initiate various NLP methods on these fields upon query. In most cases of biomedical data, these fields can better be indexed using a string type, which avoids unnecessary text processing. Values in string types are treated like enumerables (i.e. a finite set of strings) instead of a free text field that requires analysis. For example, in the TCGA dataset, we have a finite number of defined cancer types (e.g. TCGA-BRCA) that appear in the project_id field, thus, we can edit: <field name=\"project_id\" type=\"text_general\"/> to <field name=\"project_id\" type=\"string\"/> Important: Do NOT remove this field, as this causes a failure to build the core on provisioning: <field name=\"_version_\" type=\"plong\" indexed=\"false\" stored=\"false\"/> There will likely be many <dynamicField> and <copyField> entries which can be removed. These are added to enable further text processing that (usually) is not necessary and only increases the size of the index. In the end, you should have a simple, human-interpretable list of fields that correspond to data types you recognize in the dataset. You could have created this all yourself, but Solr typically does a good job of guessing for most things. Step 4: Copy the core files to the repository and commit Recall that to create your core, we had to do a bit of a workaround above. The files defining your solr core are located in /var/solr/data/<core name> . We want to copy these files and our edited schema (AND remove the managed schema) to the WebMeV repository so that all the required items will be there for the new dataset. Hence: cd /vagrant/solr mkdir <core name> cp /vagrant/solr/edited_schema.xml <core name>/schema.xml cp /vagrant/solr/basic_solrconfig.xml <core name>/solrconfig.xml At this point, the core files should be ready. You can commit these files to the repository. Step 5: Add the core to your Puppet manifest By adding the following snippet to your Puppet manifest at deploy/puppet/mevapi/manifests/init.pp , the provisioning step will create the necessary elements so that your new core will be ready-to-go on the next round of provisioning: Replace <CORE> below: solr::core { '<CORE>': schema_src_file => \"${project_root}/solr/<CORE>/schema.xml\", solrconfig_src_file => \"${project_root}/solr/<CORE>/solrconfig.xml\", } Step 6: Verify To be sure that everything works, it's best to start a fresh local build. Since we manually created a new core and modified some of Solr's configuration files, we need to ensure our new dataset files work out of the box. Destroy and re-created the VM however you wish. After, you can attempt to index a file into your new collection using the management commands we provide. If you want to test the data download/preparation process, SSH into your VM and run python3 manage.py pull_public_data -d <core name> Then, to index a file into this core: python3 manage.py index_data -d <core name> <path> [<path> ...] Finally, note that if you core expected core does not exist, you can always check by visiting the Solr admin interface at http://localhost:8983/solr/ on the host machine. The \"core admin\" tab will typically report error messages that occurred.","title":"Creating and managing public data"},{"location":"public_data/#working-with-public-datasets","text":"WebMeV provides the ability to easily create custom datasets derived from publicly available databases such as the Cancer Genome Atlas (TCGA). Query functionality of these datasets is exposed through Solr, which provides us a standard indexing and query syntax. Given that each data repository has different formats and content, this functionality requires some custom code to create and manage the data on a case-by-case basis. In this guide, we describe the necessary elements to create/index a new data source. In this guide, we will often refer to the TCGA RNA-seq as a prototypical example. This dataset contains count-based RNA-seq data downloaded from the NCI's genomic data commons (GDC) repository under the TCGA project.","title":"Working with public datasets"},{"location":"public_data/#the-quick-way","text":"For convenience, there is a management command that handles most of the steps. Be sure to pay attention to the log messages, as they will direct you to change various things. If that script fails for whatever reason, we provide all the atomic details below. Run the following in your Vagrant build: python3 manage.py create_new_public_dataset \\ -d <core name> -f <path to an example file you want to index> The core name ( -d ) should be relatively short/simple (e.g. ccle-data ) and the the file ( -f ) should be an example of the data you wish to index. It can be a row subset (not ALL the data), but should have all the potential fields. Note that the script will still require you to fill out the stubbed-out python module for the new dataset AND also require you to add some content to the Puppet manifests to fully integrate this new core. The script will direct you to the stubs and remind you what needs to be done. You will also need to commit the solr files ( solr/<core>/schema.xml and solr/<core>/solrconfig.xml ) when everything is fully ready.","title":"The quick way:"},{"location":"public_data/#the-longer-way-or-the-way-if-the-script-above-does-not-work","text":"","title":"The longer way (or the way if the script above does not work!)"},{"location":"public_data/#step-1-defining-and-creating-the-data","text":"To create a new public dataset, we need to define how the data is collected and prepared. This section describes how to write the proper hooks for data ingestion and preparation. 1.1 Create a new directory for your dataset To keep everything organized, we expect that each dataset will be kept in separate subdirectories of the mev/api/public_data/sources directory. However, note that there is no enforcement of this convention or any expected hierarchy of those subdirectories. Example: Note that mev/api/public_data/sources contains a gdc subdirectory and was created with the intent of holding all GDC projects; beyond TCGA, there are many other datasets exposed via the GDC repository. We expect that more of these GDC datasets will be included over time, so this was a logical way to structure the folder. You may choose to structure new datasets in an alternate manner. The gdc directory contains several python modules which define how the GDC-derived datasets are to be downloaded and prepared. We define a gdc.py module which contains code expected to be common to all GDC projects. Code specific to preparation of TCGA data is contained in the tcga.py module. 1.2 Create an implementing class To provide a common means of ingesting/preparing all datasets, we expect that each dataset will be mapped 1:1 with a Python class that derives from api.public_data.sources.base.PublicDataset . This class requires the following: Class attributes: TAG : This is a unique string which acts as an identifier for the dataset. If the name is not unique, then registering the dataset in the database will not be permitted, and hence the indexed dataset will not be usable. Additionally, the unit test suite will check that all implementing classes have unique identifiers. This string is limited to 30 characters by a database constraint. PUBLIC_NAME : This string should be a relatively short \"name\" for the dataset, such as \"TCGA RNA-seq\" DESCRIPTION : This is another string which provides more context and a thorough description of the data is contains. DATASET_FILES : This is a list which enumerates the files necessary for this dataset. For example, in the TCGA RNA-seq dataset, we have the metadata/annotations file, and we have a separate count file. This list of used to ensure that the proper files are present when indexing and creating datasets. Methods: prepare(self) : a method that takes no arguments (other than the class instance) and prepares the data. A return value is not expected. verify_files(self, file_dict) : a method that takes a dictionary with keys that should match those in DATASET_FILES and the values are file paths so we can verify that the files exist. get_indexable_files(self, file_dict) : a method that iterates through the passed dictionary and returns a list of file paths for files that are to be indexed. get_additional_metadata(self) : a method that returns a dictionary containing additional information that is relevant to the dataset, but not part of the traditional data or metadata. An example from the TCGA project is a mapping of the TCGA IDs (e.g. \"TCGA-LUAD\") to full names (e.g. \"Lung adenocarcinoma\"). The results of this method are placed into the additional_metadata JSON field in the database. There is no defined structure so that each dataset will be different in general. create_from_query(self, query_params) : a method that takes a dict which specifies how to create the dataset. Think of the dict as a generic payload for filtering, etc. Returns a tuple of a filepath(string) and a resource type string (one of our \"special\" types like \"EXP_MTX\" ) A template for this new class: from api.public_data.sources.base import PublicDataSource class MyDataset(PublicDataSource): TAG = '' PUBLIC_NAME = '' DESCRIPTION = '' DATASET_FILES = [] def prepare(self): pass def verify_files(self, file_dict): pass def get_indexable_files(self, file_dict): pass def create_from_query(self, query_params): pass Note that you are not obligated to derive your implementation directly from PublicDataset ; for the TCGA RNA-seq example, we created a hierarchy of PublicDataset -> GDCDataSource -> TCGADataSource -> TCGARnaSeqDataSource which reflects the fact that certain functionality is general to all GDC data sources and hence can be re-used if we incorporate other datasets from the GDC. Also note that we make no requirements for the prepare method. In its simplest implementation, the dataset can be manually prepared and the prepare method can be left as an empty pass-through. This is the easiest option for datasets that are not actively updated or are difficult to automate. However, you still need to define this implementing class, even if the implementation is trivial . More generally, the prepare method is the entry function for potentially many steps of data download and preparation. Again, while not enforced, we expect that the prepared data will ultimately be located under the /data/public_data directory so that it will be consistent with other data sources and can be persisted on redeployments. 1.3 \"Register\" the implementing class To allow WebMeV to \"see\" this new implementation, we have to add it to the IMPLEMENTING_CLASSES list in api/public_data/__init__.py . If this is not done, you will receive errors that will warn you of an unknown dataset.","title":"Step 1: Defining and creating the data"},{"location":"public_data/#step-2-define-your-solr-core","text":"The process of adding new cores for additional datasets is handled during provisioning and requires changes to the Puppet scripts. However, prior to that point, we need to create the proper files that will be used to create this new core. This section covers details of how to create these files. To do this, below we will perform the following: Create a new (but ephemeral) Solr core Index an example file from your dataset using Solr's auto-detection functionality Query the auto-generated schema and edit it to ensure the inferred field types are correct. After these steps, we will have the necessary files which we can add to the WebMeV repository. The provisioning process will then use those files to add the new core. 2.1 Creating the core This step assumes you have a Vagrant-based system up and have SSH'd in. By default, you are the vagrant user. Important: the name of the Solr core you create MUST match the TAG attribute of your implementing class . This is how the implementing class knows which Solr core to query. Run: sudo -u solr /opt/solr/bin/solr create_core -c <core name> Solr should report that the new core was created.","title":"Step 2: Define your solr core"},{"location":"public_data/#step-3-obtaining-and-modifying-the-data-schema","text":"3.1 Index an example file Above, Solr will create a default/schemaless core. For our purposes in WebMeV, the data is typically more structured and we have some notion of data structures and types in a given dataset (e.g. age as an integer type). Furthermore, we don't want to necessarily rely on Solr to properly guess the types of various fields. Hence, we will first use Solr to auto-index an example file. Following this, we will request a \"dump\" of the current schema which we will subsequently edit to fit our needs. Ideally, the example file you use below is very close to the final data you are hoping to index (or is the data you want to expose). Mismatches between the schema and the data files will cause failures. To index the example file: /opt/solr/bin/post -c <core name> <file path> 3.2 Query for the current schema and edit Now, we will query for the current/inferred schema that was just created. Here, we put this in the /vagrant/solr directory, but it does not really matter where it goes, as long as you remember. curl http://localhost:8983/solr/<core name>/schema?wt=schema.xml >/vagrant/solr/current_schema.xml This curl request will provide the structured XML schema that is currently supporting the core. At the top of the current_schema.xml file you will see many field type ( <fieldType> ) entries, which can be left as-is. You may, however, wish to remove those that are (likely) unnecessary. Examples include various tokenizer classes and filters corresponding to free-text analyzers and considerations for foreign language support. Those fields are not likely to be too relevant for most biomedical data we are analyzing within WebMeV. However, it is also fine to leave those all as-is. You will , however, want to review and potentially make edits to the <field> entities located towards the tail of the schema. You should see field names corresponding to the columns/fields of the example file you indexed before. Depending on your data, you may choose to edit the type attributes. For instance, the default may be something like: <field name=\"year_of_birth\" type=\"pdoubles\"/> but we may wish to change that to integers: <field name=\"year_of_birth\" type=\"pint\"/> Similarly, many string-based fields default to a type of text_general which causes solr to initiate various NLP methods on these fields upon query. In most cases of biomedical data, these fields can better be indexed using a string type, which avoids unnecessary text processing. Values in string types are treated like enumerables (i.e. a finite set of strings) instead of a free text field that requires analysis. For example, in the TCGA dataset, we have a finite number of defined cancer types (e.g. TCGA-BRCA) that appear in the project_id field, thus, we can edit: <field name=\"project_id\" type=\"text_general\"/> to <field name=\"project_id\" type=\"string\"/> Important: Do NOT remove this field, as this causes a failure to build the core on provisioning: <field name=\"_version_\" type=\"plong\" indexed=\"false\" stored=\"false\"/> There will likely be many <dynamicField> and <copyField> entries which can be removed. These are added to enable further text processing that (usually) is not necessary and only increases the size of the index. In the end, you should have a simple, human-interpretable list of fields that correspond to data types you recognize in the dataset. You could have created this all yourself, but Solr typically does a good job of guessing for most things.","title":"Step 3: Obtaining and modifying the data schema"},{"location":"public_data/#step-4-copy-the-core-files-to-the-repository-and-commit","text":"Recall that to create your core, we had to do a bit of a workaround above. The files defining your solr core are located in /var/solr/data/<core name> . We want to copy these files and our edited schema (AND remove the managed schema) to the WebMeV repository so that all the required items will be there for the new dataset. Hence: cd /vagrant/solr mkdir <core name> cp /vagrant/solr/edited_schema.xml <core name>/schema.xml cp /vagrant/solr/basic_solrconfig.xml <core name>/solrconfig.xml At this point, the core files should be ready. You can commit these files to the repository.","title":"Step 4: Copy the core files to the repository and commit"},{"location":"public_data/#step-5-add-the-core-to-your-puppet-manifest","text":"By adding the following snippet to your Puppet manifest at deploy/puppet/mevapi/manifests/init.pp , the provisioning step will create the necessary elements so that your new core will be ready-to-go on the next round of provisioning: Replace <CORE> below: solr::core { '<CORE>': schema_src_file => \"${project_root}/solr/<CORE>/schema.xml\", solrconfig_src_file => \"${project_root}/solr/<CORE>/solrconfig.xml\", }","title":"Step 5: Add the core to your Puppet manifest"},{"location":"public_data/#step-6-verify","text":"To be sure that everything works, it's best to start a fresh local build. Since we manually created a new core and modified some of Solr's configuration files, we need to ensure our new dataset files work out of the box. Destroy and re-created the VM however you wish. After, you can attempt to index a file into your new collection using the management commands we provide. If you want to test the data download/preparation process, SSH into your VM and run python3 manage.py pull_public_data -d <core name> Then, to index a file into this core: python3 manage.py index_data -d <core name> <path> [<path> ...] Finally, note that if you core expected core does not exist, you can always check by visiting the Solr admin interface at http://localhost:8983/solr/ on the host machine. The \"core admin\" tab will typically report error messages that occurred.","title":"Step 6: Verify"},{"location":"resource_metadata/","text":"Resource metadata Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). Since it represents a database table, it does not do perform validation, etc. on the actual files that have the data. However, the DataResource is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Thus, instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent.","title":"Resource metadata"},{"location":"resource_metadata/#resource-metadata","text":"Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). Since it represents a database table, it does not do perform validation, etc. on the actual files that have the data. However, the DataResource is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Thus, instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent.","title":"Resource metadata"},{"location":"resource_types/","text":"Resource types A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. Resource types allow us to specify the format of input/output files of Operation s. Therefore, we can predictably present options for those inputs and allow Resource s to flow from one analysis to another. The string identifiers map to concrete classes that implement validation methods for the Resource . For example, the string I_MTX indicates that the Resource is an integer matrix. When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into several broad categories: Table-based formats Sequence-based formats JSON General. Not a true type, but rather denotes that a better, more specific type cannot be specified. Table-based formats Table-based formats are any matrix-like format, such as a typical CSV file. In addition to being a common file format for expression matrices and similar experimental data, this covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. Specific types are shown below in the auto-generated documentation, but we touch on some of the more general descriptions immediately below. During validation, the primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. We enforce that specific sub-types of this general table-based format adhere to our expectations. For instance, an expression matrix requires a first row which contains samples/observation names. Furthermore, the first column should correspond to gene identifiers ( Feature s more generally). While we cannot exhaustively validate every file, we make certain reasonable assumptions. For example, if the first row is all numbers, we assume that a header is missing. Certainly one could name their samples with numeric identifiers, but we enforce that they need to be strings. Failure to conform to these expectations will result in the file failing to validate. Users should be informed of the failure with a helpful message for resolution. Also note that while the user may submit files in a format such as CSV, we internally convert to a common format (e.g. TSV) so that downstream tools can avoid having to include multiple file-parsing schemes. Since table-based formats naturally lend themselves to arrays of atomic items (i.e. each row as a \"record\"), the contents of table-based formats can be requested in a paginated manner via the API. Sequence-based formats Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. These types of files cannot reasonably be validated up front, so any Operation s which use these should plan on gracefully handling problems with their format. JSON For data that is not easily represented in a table-based format, we retain JSON as a general format. We use Python's internal json library to enforce the format of these files. Any failure to parse the file results in a validation error. Note that the contents of array-based JSON files can be paginated, but general JSON objected-based resources cannot. An example of the former is: [ { \"keyA\": 1, \"some_value\": \"abc\" }, ... { \"keyA\": 8, \"some_value\": \"xyz\" } ] These can be paginated so that each internal \"object\" (e.g. {\"keyA\": 1,\"some_value\":\"abc\"} ) is a record. General Generally this format should be avoided as it allows un-validated/unrestricted data formats to be passed around. However, for certain types (such as a tarball of many files), we sometimes have no other reasonable option. Table-based resource types class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. RnaSeqCountMatrix ( ) A very-explicit class (mainly for making things user-friendly) where we provide specialized behavior/messages specific to count matrices generated from RNA-seq data. The same as an integer matrix, but named to be suggestive for WebMEV users. class resource_types.table_types. ElementTable ( ) An ElementTable captures common behavior of tables which annotate Observation s (AnnotationTable) or Feature s (FeatureTable) It's effectively an abstract class. See the derived classes which implement the specific behavior for Observation s or Feature s. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to add Attribute s to the corresponding Observation s. Note that the backend doesn't manage this. Instead, the front-end will be responsible for taking the AnnotationTable and creating/modifying Observation s. class resource_types.table_types. FeatureTable ( ) A FeatureTable is a type of table that has aggregate information about the features, but does not have any \"observations\" in the columns. An example would be the results of a differential expression analysis. Each row corresponds to a gene (feature) and the columns are information about that gene (such as p-value). Another example could be a table of metadata about genes (e.g. pathways or perhaps a mapping to a different gene identifier). The first column will give the feature/gene identifiers and the remaining columns will have information about that gene class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here. Sequence-based formats class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for compressed Fasta files class resource_types.sequence_types. FastQResource ( ) This resource type is for gzip-compressed Fastq files class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files.","title":"Resource types"},{"location":"resource_types/#resource-types","text":"A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. Resource types allow us to specify the format of input/output files of Operation s. Therefore, we can predictably present options for those inputs and allow Resource s to flow from one analysis to another. The string identifiers map to concrete classes that implement validation methods for the Resource . For example, the string I_MTX indicates that the Resource is an integer matrix. When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into several broad categories: Table-based formats Sequence-based formats JSON General. Not a true type, but rather denotes that a better, more specific type cannot be specified. Table-based formats Table-based formats are any matrix-like format, such as a typical CSV file. In addition to being a common file format for expression matrices and similar experimental data, this covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. Specific types are shown below in the auto-generated documentation, but we touch on some of the more general descriptions immediately below. During validation, the primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. We enforce that specific sub-types of this general table-based format adhere to our expectations. For instance, an expression matrix requires a first row which contains samples/observation names. Furthermore, the first column should correspond to gene identifiers ( Feature s more generally). While we cannot exhaustively validate every file, we make certain reasonable assumptions. For example, if the first row is all numbers, we assume that a header is missing. Certainly one could name their samples with numeric identifiers, but we enforce that they need to be strings. Failure to conform to these expectations will result in the file failing to validate. Users should be informed of the failure with a helpful message for resolution. Also note that while the user may submit files in a format such as CSV, we internally convert to a common format (e.g. TSV) so that downstream tools can avoid having to include multiple file-parsing schemes. Since table-based formats naturally lend themselves to arrays of atomic items (i.e. each row as a \"record\"), the contents of table-based formats can be requested in a paginated manner via the API. Sequence-based formats Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. These types of files cannot reasonably be validated up front, so any Operation s which use these should plan on gracefully handling problems with their format. JSON For data that is not easily represented in a table-based format, we retain JSON as a general format. We use Python's internal json library to enforce the format of these files. Any failure to parse the file results in a validation error. Note that the contents of array-based JSON files can be paginated, but general JSON objected-based resources cannot. An example of the former is: [ { \"keyA\": 1, \"some_value\": \"abc\" }, ... { \"keyA\": 8, \"some_value\": \"xyz\" } ] These can be paginated so that each internal \"object\" (e.g. {\"keyA\": 1,\"some_value\":\"abc\"} ) is a record. General Generally this format should be avoided as it allows un-validated/unrestricted data formats to be passed around. However, for certain types (such as a tarball of many files), we sometimes have no other reasonable option.","title":"Resource types"},{"location":"resource_types/#table-based-resource-types","text":"class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. RnaSeqCountMatrix ( ) A very-explicit class (mainly for making things user-friendly) where we provide specialized behavior/messages specific to count matrices generated from RNA-seq data. The same as an integer matrix, but named to be suggestive for WebMEV users. class resource_types.table_types. ElementTable ( ) An ElementTable captures common behavior of tables which annotate Observation s (AnnotationTable) or Feature s (FeatureTable) It's effectively an abstract class. See the derived classes which implement the specific behavior for Observation s or Feature s. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to add Attribute s to the corresponding Observation s. Note that the backend doesn't manage this. Instead, the front-end will be responsible for taking the AnnotationTable and creating/modifying Observation s. class resource_types.table_types. FeatureTable ( ) A FeatureTable is a type of table that has aggregate information about the features, but does not have any \"observations\" in the columns. An example would be the results of a differential expression analysis. Each row corresponds to a gene (feature) and the columns are information about that gene (such as p-value). Another example could be a table of metadata about genes (e.g. pathways or perhaps a mapping to a different gene identifier). The first column will give the feature/gene identifiers and the remaining columns will have information about that gene class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here.","title":"Table-based resource types"},{"location":"resource_types/#sequence-based-formats","text":"class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for compressed Fasta files class resource_types.sequence_types. FastQResource ( ) This resource type is for gzip-compressed Fastq files class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files.","title":"Sequence-based formats"},{"location":"resources/","text":"Resources Resource s represent data in some file-based format. They come in two types-- those owned by specific users ( Resource ) and those that are user-independent and associated with analysis operations ( OperationResource ). Examples of the latter include files for reference genomes, aligner indexes, or other analysis-specific files that a user does not need to maintain or directly interact with. Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders (e.g. Dropbox). They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. OperationResource s do not have owners, but instead maintain a foreign-key relationship with their associated Operation . Resource \"type\" and \"format\" To properly parse and validate a file, a Resource is required to have: a \"type\" (e.g. an integer matrix) which we call a resource_type . This describes what the file represents (e.g. BED file, expression matrix, etc.). Upon creation, resource_type is set to None which indicates that the Resource has not been validated. a \"format\" which tells us how the data is stored (e.g. TSV, CSV, Excel) We need both the type and format to proceed with validation. The type and format of the Resource can be specified immediately following the file upload or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. Note that we can only validate certain types of files, like expression matrices. Validation of sequence-based files such as FastQ and BAM is not feasible and thus we skip validation. If the validation fails, we revert back to the previous successfully validated type and format. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Succesfully validated files are changed to a convenient internal representation. For instance, we accept expression matrices in multiple formats (e.g. CSV, TSV, XLSX). However, to avoid each analysis Operation from having to parse many potential file formats, we internally convert it to a consistent format, such as TSV. Thus, all the downstream tools expect that the validated resource passed as an input is saved in a TSV/tab-delimited format. Resources and metadata Depending on the type of Resource , we are able to infer and extract metadata from the file based on the format. For example, given a validated Resource that represents an RNA-seq count matrix, we assume that the column headers represent samples ( Observation s) and the rows represent genes ( Feature s). These metadata allow us to create subsets of the Observation s and Feature s for creating experimental contrasts and other typical analysis tasks. More on Observation s and Feature s is described elsewhere. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. When a user chooses to \"add\" a Resource to a Workspace , we append the Workspace to the set of Workspace instances associated with that Resource . That is, each Resource tracks which Workspace s it is associated with. This is accomplished via a many-to-many mapping in the database. Users can remove a Resource from a Workspace , but only if it has NOT been used for any portions of the analysis . We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Note that removing a Resource from a Workspace does not delete a file- it only modifies the workspaces field on the Resource database instance. Deletion of Resources Resource s can only be deleted from the file manager on the \"home\" screen (i.e. not in the Workspace view) in the UI. If a Resource is associated/attached to one or more Workspace s, then you cannot delete the Resource . A Resource can only be deleted if: It is associated with zero Workspace s It is not used in any Operation Technically, we only need the first case. If a Resource has been used in an Operation , we don't allow the user to remove it from the Workspace . Thus, a file being associated with zero Workspace s means that it has not been used in any Operation s Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process, such as validation. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.abstract_resource. AbstractResource ( *args , **kwargs ) This is the base class which holds common fields for both the user-owned Resource model and the user-independent OperationResource model. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). class api.models.operation_resource. OperationResource ( *args , **kwargs ) An OperationResource is a specialization of a Resource which is not owned by anyone specific, but is rather associated with a single Operation . Used for things like genome indexes, etc. where the user is not responsible for supplying or maintaining the resource. Note that it maintains a reference to the Operation input field it corresponds to. This front-end components to easily map the OperationResource to the proper input field for user selection. The is_active and is_public fields default to True .","title":"General info"},{"location":"resources/#resources","text":"Resource s represent data in some file-based format. They come in two types-- those owned by specific users ( Resource ) and those that are user-independent and associated with analysis operations ( OperationResource ). Examples of the latter include files for reference genomes, aligner indexes, or other analysis-specific files that a user does not need to maintain or directly interact with. Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders (e.g. Dropbox). They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. OperationResource s do not have owners, but instead maintain a foreign-key relationship with their associated Operation . Resource \"type\" and \"format\" To properly parse and validate a file, a Resource is required to have: a \"type\" (e.g. an integer matrix) which we call a resource_type . This describes what the file represents (e.g. BED file, expression matrix, etc.). Upon creation, resource_type is set to None which indicates that the Resource has not been validated. a \"format\" which tells us how the data is stored (e.g. TSV, CSV, Excel) We need both the type and format to proceed with validation. The type and format of the Resource can be specified immediately following the file upload or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. Note that we can only validate certain types of files, like expression matrices. Validation of sequence-based files such as FastQ and BAM is not feasible and thus we skip validation. If the validation fails, we revert back to the previous successfully validated type and format. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Succesfully validated files are changed to a convenient internal representation. For instance, we accept expression matrices in multiple formats (e.g. CSV, TSV, XLSX). However, to avoid each analysis Operation from having to parse many potential file formats, we internally convert it to a consistent format, such as TSV. Thus, all the downstream tools expect that the validated resource passed as an input is saved in a TSV/tab-delimited format. Resources and metadata Depending on the type of Resource , we are able to infer and extract metadata from the file based on the format. For example, given a validated Resource that represents an RNA-seq count matrix, we assume that the column headers represent samples ( Observation s) and the rows represent genes ( Feature s). These metadata allow us to create subsets of the Observation s and Feature s for creating experimental contrasts and other typical analysis tasks. More on Observation s and Feature s is described elsewhere. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. When a user chooses to \"add\" a Resource to a Workspace , we append the Workspace to the set of Workspace instances associated with that Resource . That is, each Resource tracks which Workspace s it is associated with. This is accomplished via a many-to-many mapping in the database. Users can remove a Resource from a Workspace , but only if it has NOT been used for any portions of the analysis . We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Note that removing a Resource from a Workspace does not delete a file- it only modifies the workspaces field on the Resource database instance. Deletion of Resources Resource s can only be deleted from the file manager on the \"home\" screen (i.e. not in the Workspace view) in the UI. If a Resource is associated/attached to one or more Workspace s, then you cannot delete the Resource . A Resource can only be deleted if: It is associated with zero Workspace s It is not used in any Operation Technically, we only need the first case. If a Resource has been used in an Operation , we don't allow the user to remove it from the Workspace . Thus, a file being associated with zero Workspace s means that it has not been used in any Operation s Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process, such as validation. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.abstract_resource. AbstractResource ( *args , **kwargs ) This is the base class which holds common fields for both the user-owned Resource model and the user-independent OperationResource model. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). class api.models.operation_resource. OperationResource ( *args , **kwargs ) An OperationResource is a specialization of a Resource which is not owned by anyone specific, but is rather associated with a single Operation . Used for things like genome indexes, etc. where the user is not responsible for supplying or maintaining the resource. Note that it maintains a reference to the Operation input field it corresponds to. This front-end components to easily map the OperationResource to the proper input field for user selection. The is_active and is_public fields default to True .","title":"Resources"},{"location":"setup_configuration/","text":"Configuration options and parameters As described in the installation section, the WebMeV API depends on environment variables to configure the application. For local development with Vagrant, this may require setting some environment variables before running vagrant up . See Vagrantfile for details. In most cases, you do not need to modify anything or set environment variables. For a cloud-based deployment with terraform, you will need to create a terraform.tfvars file and enter the config parameters. See deployment-aws/terraform/README.md for details. Comments about configuration parameters: While most variables are sane default, note the following: admin_email_csv : This a comma-delimited string of the emails for administrators. Note that this does not create Django superusers, but rather provides a set of email addresses who will receive notifications (about errors, feedback, etc.) enable_remote_job_runners : The value \"yes\" enables the remote job runners like Cromwell. If any other value, then we will not allow remote jobs to be executed, which limits the types of analyses that can be run. frontend_domain : This is not strictly necessary, but will allow a frontend application located on another domain to interact with the API. Otherwise, Django will reject the request due to same-origin CORS policies. Do NOT include protocol. additional_cors_origins : This will allow additional frontend applications to connect. For instance, if you would like your local development frontend (accessible at localhost:4200) to connect to this deployment, you can add \"http://localhost:4200\". This is a comma-delimited string, so you can have multiple values. Be sure to include the http protocol, as shown. storage_location : This is either \"remote\" or \"local\". Using \"remote\" makes use of remote bucket storage. The \"local\" setting stores files on the server, which can be inappropriate for working with larger files since it requires larger hard disk space. For cloud deployments, just set this to \"remote\". from_email : We uses AWS simple email service (SES) to send emails. This sets to \"from\" field in the emails. It's helpful to set this to something like \"WebMEV <noreply@mail.webmev.tm4.org>\" so that users will know not to reply directly to the email. sentry_url : Only necessary to use if you are using a Sentry issue-tracker. If not using Sentry, then you can just leave this as an empty string. Email backends The \"dev\" settings ( mev/settings_dev.py ) use the default \"console\" email backend. This means that for development purposes, no emails will be sent. If you are deploying into a cloud environment, but want that enabled, alter that file OR just simply declare your dev environment as production. There's nothing particularly special about the \"production\" designation. It's effectively only production if it's properly associated with our \"live\" URL. The \"production\" email uses AWS SES. Terraform will create an IAM user with appropriate permissions and the user/host/etc will automatically be linked up with the Django application during the provisioning. About storage backends Storage of user files can be either local (on the WebMEV server) or in some remote filesystem (e.g. in an AWS S3 storage bucket). To abstract this, we use the django-storages library for a common interface. For cloud deployments, we always use the \"remote\" setting, which corresponds to S3 storage. This setting is also required if you are planning to use the Cromwell (or other) remote job runner. Since Cromwell needs access to the files, we must be using bucket storage (barring some other, more complex setup).","title":"Configuration"},{"location":"setup_configuration/#configuration-options-and-parameters","text":"As described in the installation section, the WebMeV API depends on environment variables to configure the application. For local development with Vagrant, this may require setting some environment variables before running vagrant up . See Vagrantfile for details. In most cases, you do not need to modify anything or set environment variables. For a cloud-based deployment with terraform, you will need to create a terraform.tfvars file and enter the config parameters. See deployment-aws/terraform/README.md for details. Comments about configuration parameters: While most variables are sane default, note the following: admin_email_csv : This a comma-delimited string of the emails for administrators. Note that this does not create Django superusers, but rather provides a set of email addresses who will receive notifications (about errors, feedback, etc.) enable_remote_job_runners : The value \"yes\" enables the remote job runners like Cromwell. If any other value, then we will not allow remote jobs to be executed, which limits the types of analyses that can be run. frontend_domain : This is not strictly necessary, but will allow a frontend application located on another domain to interact with the API. Otherwise, Django will reject the request due to same-origin CORS policies. Do NOT include protocol. additional_cors_origins : This will allow additional frontend applications to connect. For instance, if you would like your local development frontend (accessible at localhost:4200) to connect to this deployment, you can add \"http://localhost:4200\". This is a comma-delimited string, so you can have multiple values. Be sure to include the http protocol, as shown. storage_location : This is either \"remote\" or \"local\". Using \"remote\" makes use of remote bucket storage. The \"local\" setting stores files on the server, which can be inappropriate for working with larger files since it requires larger hard disk space. For cloud deployments, just set this to \"remote\". from_email : We uses AWS simple email service (SES) to send emails. This sets to \"from\" field in the emails. It's helpful to set this to something like \"WebMEV <noreply@mail.webmev.tm4.org>\" so that users will know not to reply directly to the email. sentry_url : Only necessary to use if you are using a Sentry issue-tracker. If not using Sentry, then you can just leave this as an empty string.","title":"Configuration options and parameters"},{"location":"setup_configuration/#email-backends","text":"The \"dev\" settings ( mev/settings_dev.py ) use the default \"console\" email backend. This means that for development purposes, no emails will be sent. If you are deploying into a cloud environment, but want that enabled, alter that file OR just simply declare your dev environment as production. There's nothing particularly special about the \"production\" designation. It's effectively only production if it's properly associated with our \"live\" URL. The \"production\" email uses AWS SES. Terraform will create an IAM user with appropriate permissions and the user/host/etc will automatically be linked up with the Django application during the provisioning.","title":"Email backends"},{"location":"setup_configuration/#about-storage-backends","text":"Storage of user files can be either local (on the WebMEV server) or in some remote filesystem (e.g. in an AWS S3 storage bucket). To abstract this, we use the django-storages library for a common interface. For cloud deployments, we always use the \"remote\" setting, which corresponds to S3 storage. This setting is also required if you are planning to use the Cromwell (or other) remote job runner. Since Cromwell needs access to the files, we must be using bucket storage (barring some other, more complex setup).","title":"About storage backends"},{"location":"workspaces/","text":"Workspaces class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"},{"location":"workspaces/#workspaces","text":"class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"}]}