{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the WebMEV documentation page. This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Home"},{"location":"#welcome-to-the-webmev-documentation-page","text":"This will serve as the main source of information and documentation on the architecture and structure of MEV and its RESTful API. For documentation on the endpoints, checkout the API specification","title":"Welcome to the WebMEV documentation page."},{"location":"api/","text":"Documentation on the API There are two aspects of the WebMEV backend. First, we have the public-facing RESTful API endpoints which are used to drive an analysis, upload files, and perform other actions. Documentation of the API is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation . The second aspect of WebMEV is the data structures, models, and concepts that we use to architect the system. You will find information about these entities and their relationships in this section. Understanding these data structures and associated nomenclature will be important when describing how to work with WebMEV and create new workflows. Core concepts The goal of WebMEV is to provide a suite of analysis tools and visualizations to guide users through self-directed analyses, most commonly with genomic (particularly transcriptomic) data. At a high level, users will start from either files that they provide (e.g. a previously generated expression matrix) or (if available) can import data from public repositories that are available through WebMEV. Users will then create one or more Workspace s which facilitates logical organization of analyses and separation of distinct projects or experiments. Within the context of the Workspace , users can perform a custom analysis as a series of atomic steps which we call Operation s. More specific details about each of these steps, including how we handle metadata are available in the other sections of this documentation.","title":"Intro and core concepts"},{"location":"api/#documentation-on-the-api","text":"There are two aspects of the WebMEV backend. First, we have the public-facing RESTful API endpoints which are used to drive an analysis, upload files, and perform other actions. Documentation of the API is provided by auto-generated documentation conforming to the OpenAPI spec here: API documentation . The second aspect of WebMEV is the data structures, models, and concepts that we use to architect the system. You will find information about these entities and their relationships in this section. Understanding these data structures and associated nomenclature will be important when describing how to work with WebMEV and create new workflows.","title":"Documentation on the API"},{"location":"api/#core-concepts","text":"The goal of WebMEV is to provide a suite of analysis tools and visualizations to guide users through self-directed analyses, most commonly with genomic (particularly transcriptomic) data. At a high level, users will start from either files that they provide (e.g. a previously generated expression matrix) or (if available) can import data from public repositories that are available through WebMEV. Users will then create one or more Workspace s which facilitates logical organization of analyses and separation of distinct projects or experiments. Within the context of the Workspace , users can perform a custom analysis as a series of atomic steps which we call Operation s. More specific details about each of these steps, including how we handle metadata are available in the other sections of this documentation.","title":"Core concepts"},{"location":"attributes/","text":"Attributes Attribute s serve as \"parameters\" and are a way of providing validation and type-checking for values that are passed around within WebMEV. The different types represent different simple entities within WebMEV. For example, we have simple wrappers around primitives like integers which enforce constraints on the underlying primitive type (e.g. for a probability, we can use a BoundedFloatAttribute set with bounds of [0,1]). Other types can represent and validate files (\"data resources\") Attribute s are used to provide metadata (e.g. a phenotype of a sample given as a StringAttribute ) or are used as parameters to analyses (e.g. a BoundedFloatAttribute for filtering p-values less than a particular value) class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } class api.data_structures.attributes. OptionStringAttribute ( value , **kwargs ) A String type that only admits one from a set of preset options (e.g. like a dropdown) { \"attribute_type\": \"OptionString\", \"value\": <str>, \"options\": [<str>, <str>,...,<str>] } class api.data_structures.attributes. BooleanAttribute ( value , **kwargs ) Basic boolean { \"attribute_type\": \"Boolean\", \"value\": <bool> } class api.data_structures.attributes. DataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances. { \"attribute_type\": \"DataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key. class api.data_structures.attributes. OperationDataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances which are user-independent, such as database-like resources which are used for analyses. { \"attribute_type\": \"OperationDataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key. api.data_structures. create_attribute ( attr_key , attribute_dict , allow_null=False ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"attributes/#attributes","text":"Attribute s serve as \"parameters\" and are a way of providing validation and type-checking for values that are passed around within WebMEV. The different types represent different simple entities within WebMEV. For example, we have simple wrappers around primitives like integers which enforce constraints on the underlying primitive type (e.g. for a probability, we can use a BoundedFloatAttribute set with bounds of [0,1]). Other types can represent and validate files (\"data resources\") Attribute s are used to provide metadata (e.g. a phenotype of a sample given as a StringAttribute ) or are used as parameters to analyses (e.g. a BoundedFloatAttribute for filtering p-values less than a particular value) class api.data_structures.attributes. BaseAttribute ( value , **kwargs ) Base object which defines some common methods and members for Attribute types Classes that derive from BaseAttribute have strings which identify their type ( typename ) and a value , which is specific to the child class implementation. See child classes for examples. class api.data_structures.attributes. BoundedBaseAttribute ( value , **kwargs ) This class derives from BaseAttribute and adds logic for numeric attributes that are bounded between specified values. In addition to the typename and value members, these require a min and a max to set the bounds. Classes deriving from this can be used for things like bounding a p-value from a hypothesis test (which is 0<=p<=1) class api.data_structures.attributes. IntegerAttribute ( value , **kwargs ) General, unbounded integers. Represented by { \"attribute_type\": \"Integer\", \"value\": <integer> } class api.data_structures.attributes. PositiveIntegerAttribute ( value , **kwargs ) Integers > 0 { \"attribute_type\": \"PositiveInteger\", \"value\": <integer> } class api.data_structures.attributes. NonnegativeIntegerAttribute ( value , **kwargs ) Integers >=0 { \"attribute_type\": \"NonNegativeInteger\", \"value\": <integer> } class api.data_structures.attributes. BoundedIntegerAttribute ( value , **kwargs ) Integers that are bounded between a min and max value. { \"attribute_type\": \"BoundedInteger\", \"value\": <integer>, \"min\": <integer lower bound>, \"max\": <integer upper bound> } class api.data_structures.attributes. FloatAttribute ( value , **kwargs ) General, unbounded float type { \"attribute_type\": \"Float\", \"value\": <float> } class api.data_structures.attributes. PositiveFloatAttribute ( value , **kwargs ) Positive (>0) float type { \"attribute_type\": \"PositiveFloat\", \"value\": <float> } class api.data_structures.attributes. NonnegativeFloatAttribute ( value , **kwargs ) Non-negative (>=0) float type { \"attribute_type\": \"NonNegativeFloat\", \"value\": <float> } class api.data_structures.attributes. BoundedFloatAttribute ( value , **kwargs ) Floats that are bounded between a min and max value. { \"attribute_type\": \"BoundedFloat\", \"value\": <float>, \"min\": <integer/float lower bound>, \"max\": <integer/float upper bound> } class api.data_structures.attributes. StringAttribute ( value , **kwargs ) String type that has basic guards against non-typical characters. { \"attribute_type\": \"String\", \"value\": <str> } class api.data_structures.attributes. OptionStringAttribute ( value , **kwargs ) A String type that only admits one from a set of preset options (e.g. like a dropdown) { \"attribute_type\": \"OptionString\", \"value\": <str>, \"options\": [<str>, <str>,...,<str>] } class api.data_structures.attributes. BooleanAttribute ( value , **kwargs ) Basic boolean { \"attribute_type\": \"Boolean\", \"value\": <bool> } class api.data_structures.attributes. DataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances. { \"attribute_type\": \"DataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key. class api.data_structures.attributes. OperationDataResourceAttribute ( value , **kwargs ) Used to specify a reference to one or more Resource instances which are user-independent, such as database-like resources which are used for analyses. { \"attribute_type\": \"OperationDataResource\", \"value\": <one or more Resource UUIDs>, \"many\": <bool>, } Note that \"many\" controls whether >1 are allowed. It's not an indicator for whether there are multiple Resources specified in the \"value\" key. api.data_structures. create_attribute ( attr_key , attribute_dict , allow_null=False ) Utility function used by the serializers to create/return BaseAttribute-derived instances. Accepts an attribute_dict which is a Python dictionary object containing the keys appropriate to create a particular attribute. For example, to create a BoundedIntegerAttribute , this dict would be formatted as, attr_dict = { 'attribute_type': 'BoundedInteger', 'value': 3, 'min': 0, 'max': 10 }","title":"Attributes"},{"location":"auth/","text":"Authentication with MEV Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication"},{"location":"auth/#authentication-with-mev","text":"Once a user is registered (with an email and password), requests to the API are controlled with a JWT contained in the request header. Below is an example using Python's Requests library. This example assumes you have created a user. First, exchange the username/password to get the API token: import requests token_url = 'http://127.0.0.1:8000/api/token/' payload = {'email': '<EMAIL>', 'password': '<PASSWD>'} token_response = requests.post(token_url, data=payload) token_json = token_response.json() Then, looking at token_json : {'refresh': '<REFRESH TOKEN>', 'access': '<ACCESS_TOKEN>'} We can then use that token in requests to the API: access_token = token_json['access'] resource_list_url = 'http://127.0.0.1:8000/api/resources/' headers = {'Authorization': 'Bearer %s' % access_token} resource_response = requests.get(resource_list_url, headers=headers) resource_json = resource_response.json() If the token expires (a 401 response), you need to request a new token or refresh: refresh_url = 'http://127.0.0.1:8000/api/token/refresh/' payload = {'refresh': refresh_token} refresh_response = requests.post(refresh_url, data=payload) access_token = refresh_response.json()['access']","title":"Authentication with MEV"},{"location":"creating_analyses/","text":"Creating a new analysis ( Operation ) for use with WebMEV WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones. Local Docker-based mode Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it for the commandline invocation (e.g. as a comma-delimited list of strings) entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts or static data) can be placed in this folder. Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV. Example For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - converters.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\" } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a DataResource with many=false ) that has an appropriate resource type. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use one of the \"converter\" classes to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. converters.json The converters.json file tells WebMEV how to take a user-input and convert it to the appropriate format to invoke the script. To accomplish this, we provide a mapping of the input \"name\" to a class which implements the conversion. For us, this looks like: { \"input_matrix\":\"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"samples\": \"api.converters.element_set.ObservationSetCsvConverter\" } The class implementations are provided using Python's \"dotted\" notation. A variety of commonly-used converters are provided with WebMEV, but developers are free to create their own implementations for their own WebMEV instances. The only requirement is that the class implements a common interface method named convert , which takes the user-supplied input and returns an appropriate output. As an example, we note that the LocalDockerSingleDataResourceConverter above takes the user-supplied input (a UUID which identifies a file/ Resource they own) and \"converts\" it to a path on the server. In this way, the run_pca.py script is not concerned with how WebMEV stores files, etc. WebMEV handles the file moving/copying and analyses can be written without dependencies on WebMEV-related architecture or how files are stored within WebMEV. The samples input uses the api.converters.element_set.ObservationSetCsvConverter which converts an ObservationSet such as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } into a CSV string: sampleA,sampleB . Clearly, this requires some knowledge of the available \"converter\" implementations. We expect that there are not so many that this burden is unreasonable. We decided that the flexibility provided by this inconvenience was more beneficial than restricting the types of inputs and how they can be formatted for invoking jobs. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\") A suggested workflow for creating new analyses First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Create the converters.json file which will reformat the inputs into items that the entrypoint command will understand. Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV. Remote, Cromwell-based jobs For jobs that are run remotely with the help of the Cromwell job engine, we have slightly different required files. Cromwell-based jobs are executed using \"Workflow Definition Language\" (WDL) syntax files (https://openwdl.org/). When using this job engine, the primary purpose of WebMEV is to validate user inputs and reformat them to be compatible with the inputs required to run the workflow. For those who have not used Broad's Cromwell engine before, the three components of an analysis workflow include: WDL file(s): Specifies the commands that are run. You can think of this as you would a typical shell script. A JSON-format inputs file: This maps the expected workflow inputs (e.g. strings, numbers, or files) to specific values. For instance, if we expect a file, then the inputs JSON file will map the input variable to a file path. WebMEV is responsible for creating this file at runtime. One of more Docker containers: Cromwell orchestrates the startup/shutdown of cloud-based virtual machines but all commands are run within Docker runtimes on those machines. Thus, the WDL files will dictate which Docker images are used for each step in the analysis. There can be an arbitrary number of these. Thus, to create a Cromwell-based job that is compatible with WebMEV we require: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . This file is the same as with any WebMEV analysis. To specifically create an Operation for the Cromwell job runner, you must specify \"mode\": \"cromwell\" in the Operation object. main.wdl In general there can be any number of WDL-format files in the repository. However, the primary or \"entry\" WDL file must be named as main.wdl . inputs.json This is the JSON-format file which dictates the inputs to the workflow. It is a template that will be appropriately filled at runtime. Thus, the \"values\" of the mapping do not matter, but the keys must map to input variables in main.wdl . Typically, this file is easily created by Broad's WOMTool. See below for an example. static_inputs.json An optional file that gives values for variables like genome indexes and other relatively static inputs. Keys in this should be a subset of those in inputs.json . File paths contained here are used to copy files into a WebMEV-associated bucket. converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it to be used in inputs.json . As above, this is a mapping of the input name to a \"dotted\" class implementation. docker/ The docker folder contains one or more Dockerfile-format files and the dependencies to create those Docker images. Each Dockerfile is named according to its target image. For instance, if one of the WDL files specifies that it depends on a Docker image named docker.io/myUser/foo , then the Dockerfile defining that image should be named Dockerfile.foo . Additional notes: Remarks about dependencies between main.wdl , inputs.json and operation_spec.json As much as we try to remove interdependencies between files (for ease of development), there are situations we can't resolve easily. One such case is the interdependencies between main.wdl , inputs.json , and the operation_spec.json files. As mentioned above, the inputs.json file supplied in the repository is effectively a template which is filled at runtime. The keys of that object correspond to inputs to the main WDL script main.wdl . For example, given a WDL script with the following input definition: workflow SomeWorkflow { ... Array[String] samples .... } then the inputs.json would require the key SomeWorkflow.samples . Generally, WDL constructs its inputs in the format of <Workflow name>.<input variable name> . Thus, inputs.json would appear, in part, like { ... \"SomeWorkflow.samples\": \"Array[String]\", ... } As mentioned above, the \"value\" (here, \"Array[String]\" ) does not matter; Broad's WOMTool will typically fill-in the expected type (as a string) to serve as a cue. Finally, WebMEV has to know which inputs of the Operation correspond to which inputs of the WDL script. Thus, in our operation_spec.json , the keys in our inputs object must be consistent with main.wdl and inputs.json : { ... \"inputs\": { ... \"SomeWorkflow.samples\": <OperationInput> ... } } Converting inputs As with all analysis execution modes, we have \"converter\" classes which translate user inputs into formats that are compatible with the job runner. For instance, using the example above, one of the inputs for a WDL could be an array of strings ( Array[String] in WDL-type syntax). Thus, a converter would be responsible for taking say, an ObservationSet , and turning that into a list of strings to provide the same names. For example, we may wish to convert an ObservationSet given as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } Then, the \"inputs\" data structure submitted to Cromwell (basically inputs.json after it has been filled-in) would, in part, look like: { ... \"SomeWorkflow.samples\": [\"sampleA\", \"sampleB\"], ... } Creation of Docker images As described above, each repository can contain an arbitrary (non-zero) number of WDL files, each of which can depend on one more Docker images for their runtime. There are some custom steps involved during the ingestion of new Cromwell-based workflow, which we explain below. When Docker images are specified in the runtime section of the WDL files, the line is formatted as: runtime { ... docker: \"<repo name>/<username>/<image name>:<tag>\" ... } e.g. runtime { ... docker: \"docker.io/myUser/foo:v1\" ... } Note that we only use the image name ( foo ) when we are ingesting and preparing a new workflow for use in WebMEV. The reason is that we wish to keep all Docker images \"in house\" within our own Dockerhub account; we do not want to depend on external Docker resources which may change without our knowledge or control. Therefore, the repository and username are not used. Further, the tag is also ignored as we ultimately replace it with the git commit hash. Since the git commit hash is not produced until after the commit, we obviously can't append the images with the proper tag prior to the commit. Thus, during the ingestion process we perform the following: Parse all WDL files in the github repo and extract out all the runtime Docker images. This will be a set of strings. For each Docker \"image string\" (e.g. someUser/foo:v1 ): Extract the image name, e.g. foo Search for a corresponding Dockerfile in the repo, e.g. docker/Dockerfile.foo Build the image, tagging with the github commit ID, e.g. abc123 Push the image to the WebMEV Dockerhub, e.g. docker.io/web-mev Edit and save the WDL file with the newly-built Docker image, e.g. docker.io/web-mev/foo:abc123 . Thus, regardless of whoever creates the original image, the repository should have all the files necessary to build a fresh image which we \"claim as our own\" by assigning our username/tag and pushing it to the WebMEV Dockerhub account. We note that this technically modifies the workflow relative to the github repository, so the WebMEV-internal version is not exactly the same. However, this difference is limited to the name of the Docker image. All other aspects of the analysis are able to be exactly recreated based on the repository. Note that repositories based on many Docker containers may take a significant time to ingest, as each image must be built and pushed to Dockerhub. Copying of static resources If the static_inputs.json file is present, we expect that this file will be used for static items that are not dependent on user input. We could also put such items as default in the operation_spec.json file, but we instead choose to extract them out to this file. Upon ingestion, the files will be copied to an Operation -specific bucket/folder. For instance, given the following static_inputs.json : { \"Workflow.genome_idx\":\"gs://some-bucket/grch38.idx\" } During ingestion, this Operation will be assigned a UUID. Then, we copy this index to a new location identified by that UUID. The updated/edited static_inputs.json file will be: { \"Workflow.genome_idx\":\"gs://mev-bucket/<UUID>/grch38.idx\" }","title":"Creating new analyses/operations"},{"location":"creating_analyses/#creating-a-new-analysis-operation-for-use-with-webmev","text":"WebMEV analyses (AKA Operation s) are designed to be transparent and portable. While certain files are required for integration with the WebMEV application, the analyses are designed so that they are self-contained and can be transparently reproduced elsewhere. Depending on the nature of the analysis, jobs are either executed locally (on the WebMEV server) or remotely on ephemeral hardware that is dynamically provisioned from the cloud computing provider. Thus, the \"run mode\" of the analyses affects which files are required for WebMEV integration. Below, we describe the architecture of WebMEV-compatible analyses and how one can go about creating new ones.","title":"Creating a new analysis (Operation) for use with WebMEV"},{"location":"creating_analyses/#local-docker-based-mode","text":"Typically, local Docker-based jobs are used for lightweight analyses that require a minimal amount of hardware. Examples include principal-component analyses, differential expression testing, and other script-based jobs with relatively modest footprints. Local Docker-based jobs are intended to be invoked like a standard commandline executable or script. Specifically, to run the job, we start the Docker container with a command similar to: docker run -d -v <docker volume>:<container workspace> --entrypoint=<CMD> <IMAGE> This runs the command specified by <CMD> in the environment provided by the Docker image. In this way, we isolate the software dependencies of the analysis from the host system and provide users a way to recreate their analysis at a later time, or to independently clone the analysis repository and run it on any Docker-capable system. To construct a WebMEV-compatible analysis for local-Docker execution mode, we require the following files be present in a repository: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it for the commandline invocation (e.g. as a comma-delimited list of strings) entrypoint.txt This is a text file that provides a command template to be filled-in with the appropriate concrete arguments. The templating syntax is Jinja2 (https://jinja.palletsprojects.com) docker/Dockerfile The docker folder contains (at minimum) a Dockerfile which provides the \"recipe\" for building the Docker image. Additional files to be included in the Docker build context (such as scripts or static data) can be placed in this folder. Outputs While there are no restrictions on the nature or content of the analysis itself, we have to capture the analysis outputs in a manner that WebMEV can interpret those outputs and present them to end-users. Thus, we require that the process create an outputs.json file in the container's \"workspace\". This file is accessible to WebMEV via the shared volume provided with the -v argument to the docker run command. More details below in the concrete example. Note that this is the only place where analysis code makes any reference to WebMEV. However, the creation of an outputs.json file does not influence the analysis code in any manner-- one could take an existing script, add a few lines to create the outputs.json and it would be ready for use as an analysis module in WebMEV.","title":"Local Docker-based mode"},{"location":"creating_analyses/#example","text":"For this example, we look at the requirements for a simple principal component analysis (PCA). The repository is available at https://github.com/web-mev/pca/ We first describe the overall structure and then talk specifically about each file. Overall structure The repository has: - operation_spec.json (required) - converters.json (required) - entrypoint.txt (required) - docker/ - Dockerfile (required) - run_pca.py - requirements.txt The analysis is designed so that it will execute a single python script ( docker/run_pca.py ) as follows: run_pca.py -i <path to input file> [-s <comma-delimited list of sample names>] The first arg ( -i ) provides a path to an input matrix (typically an expression/abundance matrix). The second (optional) argument ( -s ) allows us to specify sample names to use, formatted as a comma-delimited list of samples. By default (if no argument provided) all samples are used. In addition to running the PCA, this script will also create the outputs.json file. It's not required that you structure the code in any particular manner, but the analysis has to create the outputs.json file at some point before the container exits. Otherwise, the results will not be accessible for display with WebMEV. docker/ folder and Docker context In the docker/ folder we have the required Dockerfile , the script to run ( run_pca.py ), and a requirements.txt file which provides the packages needed to construct the proper Python installation. The Dockerfile looks like: FROM debian:stretch RUN apt-get update && \\ apt-get install -y python3-dev python3-pip # Install some Python3 libraries: RUN mkdir /opt/software ADD requirements.txt /opt/software/ ADD run_pca.py /opt/software/ RUN chmod +x /opt/software/run_pca.py RUN pip3 install -r /opt/software/requirements.txt ENTRYPOINT [\"/opt/software/run_pca.py\"] requirements.txt looks like: (truncated) cryptography==1.7.1 ... scikit-learn==0.22.2.post1 ... scipy==1.4.1 ... run_pca.py : For brevity, we omit the full run_pca.py script (available at https://github.com/web-mev/pca/blob/master/docker/run_pca.py), but note that the Dockerfile places this script in the /opt/software folder. Thus, we have to either append to the PATH in the container, or provide the full path to this script when we invoke it for execution. Below (see entrypoint.txt ) we use the latter. Finally, we note that this script creates an outputs.json file: ... outputs = { 'pca_coordinates': <path to output matrix of principal coordinates>, 'pc1_explained_variance':pca.explained_variance_ratio_[0], 'pc2_explained_variance': pca.explained_variance_ratio_[1] } json.dump(outputs, open(os.path.join(working_dir, 'outputs.json'), 'w')) As stated prior, it's not required that this script create that file, but that the file be created at some point before the container exits. This is the only place where scripts are required to \"know about WebMEV\". Everything else in the script operates divorced from any notion of WebMEV architecture. operation_spec.json The operation_spec.json file provides a description of the analysis and follows the format of our Operation data structure: { \"name\": \"\", \"description\": \"\", \"inputs\": <Mapping of keys to OperationInput objects>, \"outputs\": <Mapping of keys to OperationOutput objects>, \"mode\": \"\" } Importantly, the mode key must be set to \"local_docker\" which lets WebMEV know that this analysis/ Operation will be run as a Docker-based process on the server. Failure to provide a valid value for this key will trigger an error when the analysis is \"ingested\" and prepared by WebMEV. Concretely our PCA analysis: { \"name\": \"Principal component analysis (PCA)\", \"description\": \"Executes a 2-d PCA to examine the structure and variation of a dataset.\", \"inputs\": { \"input_matrix\": { \"description\": \"The input matrix. For example, a gene expression matrix for a cohort of samples.\", \"name\": \"Input matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"MTX\",\"I_MTX\", \"EXP_MTX\", \"RNASEQ_COUNT_MTX\"], \"many\": false } }, \"samples\": { \"description\": \"The samples to use in the PCA. By default, it will use all samples/observations.\", \"name\": \"Samples:\", \"required\": false, \"spec\": { \"attribute_type\": \"ObservationSet\" } } }, \"outputs\": { \"pca_coordinates\": { \"spec\": { \"attribute_type\": \"DataResource\", \"resource_type\": \"MTX\", \"many\": false } }, \"pc1_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } }, \"pc2_explained_variance\": { \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0 } } }, \"mode\": \"local_docker\" } In the inputs section, this Operation states that it has one required ( input_matrix ) and one optional input ( samples ). For input_matrix , we expect a single input file (a DataResource with many=false ) that has an appropriate resource type. As PCA requires a numeric matrix (in our convention, with samples/observations in columns and genes/features in rows) we restrict these input types to one of \"MTX\" , \"I_MTX\" , \"EXP_MTX\" , or \"RNASEQ_COUNT_MTX\" . The full list of all resource types is available at /api/resource-types/ The second, optional input ( samples ) allows us to subset the columns of the matrix to only include samples/observations of interest. The specification of this input states that we must provide it with an object of type ObservationSet . Recall, however, that our script is invoked by providing a comma-delimited list of sample names to the -s argument. Thus, we will use one of the \"converter\" classes to convert the ObservationSet instance into a comma-delimited string. This choice is left up to the developer of the analysis-- one could very well choose to provde the ObservationSet instance as an argument to their script and parse that accordingly. For outputs, we expect a single DataResource with type \"MTX\" and two bounded floats, which represent the explained variance of the PCA. converters.json The converters.json file tells WebMEV how to take a user-input and convert it to the appropriate format to invoke the script. To accomplish this, we provide a mapping of the input \"name\" to a class which implements the conversion. For us, this looks like: { \"input_matrix\":\"api.converters.data_resource.LocalDockerSingleDataResourceConverter\", \"samples\": \"api.converters.element_set.ObservationSetCsvConverter\" } The class implementations are provided using Python's \"dotted\" notation. A variety of commonly-used converters are provided with WebMEV, but developers are free to create their own implementations for their own WebMEV instances. The only requirement is that the class implements a common interface method named convert , which takes the user-supplied input and returns an appropriate output. As an example, we note that the LocalDockerSingleDataResourceConverter above takes the user-supplied input (a UUID which identifies a file/ Resource they own) and \"converts\" it to a path on the server. In this way, the run_pca.py script is not concerned with how WebMEV stores files, etc. WebMEV handles the file moving/copying and analyses can be written without dependencies on WebMEV-related architecture or how files are stored within WebMEV. The samples input uses the api.converters.element_set.ObservationSetCsvConverter which converts an ObservationSet such as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } into a CSV string: sampleA,sampleB . Clearly, this requires some knowledge of the available \"converter\" implementations. We expect that there are not so many that this burden is unreasonable. We decided that the flexibility provided by this inconvenience was more beneficial than restricting the types of inputs and how they can be formatted for invoking jobs. entrypoint.txt The entrypoint file has the command that will be run as the ENTRYPOINT of the Docker container. To accommodate optional inputs and permit additional flexibility, we use jinja2 template syntax. In our example, we have: /opt/software/run_pca.py -i {{input_matrix}} {% if samples %} -s {{samples}} {% endif %} (as referenced above, note that we provide the full path to the Python script. Alternatively, we could put the script somewhere on the PATH when building the Docker image) The variables in this template (between the double braces) must match the keys provided in the inputs section of the operation_spec.json document. Thus, if the samples input is omitted (which means all samples are used in the PCA calculation), the final command would look like: /opt/software/run_pca.py -i <path to matrix> If the samples input is provided, WebMEV handles converting the ObservationSet instance into a comma-delimited string to create: /opt/software/run_pca.py -i <path to matrix> -s A,B,C (e.g. for samples/observations named \"A\", \"B\", and \"C\")","title":"Example"},{"location":"creating_analyses/#a-suggested-workflow-for-creating-new-analyses","text":"First, without consideration for WebMEV, consider the expected inputs and outputs of your analysis. Generally, this will be some combination of files and simple parameters like strings or numbers. Now, write this hypothetical analysis as a formal Operation into the operation_spec.json file. Create a Dockerfile and corresponding Docker image with and an analysis script that is executable as a simple commandline program. Take care to include some code to create the outputs.json file at some point in the process. Take the \"prototype\" command you would use to execute the script and write it into entrypoint.txt using jinja2 template syntax. The input variable keys should correspond to those in your operation_spec.json . Create the converters.json file which will reformat the inputs into items that the entrypoint command will understand. Once all these files are in place, create a git repository and check the code into github. The analysis is ready for ingestion with WebMEV.","title":"A suggested workflow for creating new analyses"},{"location":"creating_analyses/#remote-cromwell-based-jobs","text":"For jobs that are run remotely with the help of the Cromwell job engine, we have slightly different required files. Cromwell-based jobs are executed using \"Workflow Definition Language\" (WDL) syntax files (https://openwdl.org/). When using this job engine, the primary purpose of WebMEV is to validate user inputs and reformat them to be compatible with the inputs required to run the workflow. For those who have not used Broad's Cromwell engine before, the three components of an analysis workflow include: WDL file(s): Specifies the commands that are run. You can think of this as you would a typical shell script. A JSON-format inputs file: This maps the expected workflow inputs (e.g. strings, numbers, or files) to specific values. For instance, if we expect a file, then the inputs JSON file will map the input variable to a file path. WebMEV is responsible for creating this file at runtime. One of more Docker containers: Cromwell orchestrates the startup/shutdown of cloud-based virtual machines but all commands are run within Docker runtimes on those machines. Thus, the WDL files will dictate which Docker images are used for each step in the analysis. There can be an arbitrary number of these. Thus, to create a Cromwell-based job that is compatible with WebMEV we require: operation_spec.json This file dictates the input and output parameters for the analysis. Of type Operation . This file is the same as with any WebMEV analysis. To specifically create an Operation for the Cromwell job runner, you must specify \"mode\": \"cromwell\" in the Operation object. main.wdl In general there can be any number of WDL-format files in the repository. However, the primary or \"entry\" WDL file must be named as main.wdl . inputs.json This is the JSON-format file which dictates the inputs to the workflow. It is a template that will be appropriately filled at runtime. Thus, the \"values\" of the mapping do not matter, but the keys must map to input variables in main.wdl . Typically, this file is easily created by Broad's WOMTool. See below for an example. static_inputs.json An optional file that gives values for variables like genome indexes and other relatively static inputs. Keys in this should be a subset of those in inputs.json . File paths contained here are used to copy files into a WebMEV-associated bucket. converters.json This file tells WebMEV how to take a user-supplied input (e.g. a list of samples/ Observation s) and format it to be used in inputs.json . As above, this is a mapping of the input name to a \"dotted\" class implementation. docker/ The docker folder contains one or more Dockerfile-format files and the dependencies to create those Docker images. Each Dockerfile is named according to its target image. For instance, if one of the WDL files specifies that it depends on a Docker image named docker.io/myUser/foo , then the Dockerfile defining that image should be named Dockerfile.foo .","title":"Remote, Cromwell-based jobs"},{"location":"creating_analyses/#additional-notes","text":"Remarks about dependencies between main.wdl , inputs.json and operation_spec.json As much as we try to remove interdependencies between files (for ease of development), there are situations we can't resolve easily. One such case is the interdependencies between main.wdl , inputs.json , and the operation_spec.json files. As mentioned above, the inputs.json file supplied in the repository is effectively a template which is filled at runtime. The keys of that object correspond to inputs to the main WDL script main.wdl . For example, given a WDL script with the following input definition: workflow SomeWorkflow { ... Array[String] samples .... } then the inputs.json would require the key SomeWorkflow.samples . Generally, WDL constructs its inputs in the format of <Workflow name>.<input variable name> . Thus, inputs.json would appear, in part, like { ... \"SomeWorkflow.samples\": \"Array[String]\", ... } As mentioned above, the \"value\" (here, \"Array[String]\" ) does not matter; Broad's WOMTool will typically fill-in the expected type (as a string) to serve as a cue. Finally, WebMEV has to know which inputs of the Operation correspond to which inputs of the WDL script. Thus, in our operation_spec.json , the keys in our inputs object must be consistent with main.wdl and inputs.json : { ... \"inputs\": { ... \"SomeWorkflow.samples\": <OperationInput> ... } } Converting inputs As with all analysis execution modes, we have \"converter\" classes which translate user inputs into formats that are compatible with the job runner. For instance, using the example above, one of the inputs for a WDL could be an array of strings ( Array[String] in WDL-type syntax). Thus, a converter would be responsible for taking say, an ObservationSet , and turning that into a list of strings to provide the same names. For example, we may wish to convert an ObservationSet given as: { \"multiple\": true, \"elements\": [ { \"id\":\"sampleA\", \"attributes\": {} }, { \"id\":\"sampleB\", \"attributes\": {} } ] } Then, the \"inputs\" data structure submitted to Cromwell (basically inputs.json after it has been filled-in) would, in part, look like: { ... \"SomeWorkflow.samples\": [\"sampleA\", \"sampleB\"], ... } Creation of Docker images As described above, each repository can contain an arbitrary (non-zero) number of WDL files, each of which can depend on one more Docker images for their runtime. There are some custom steps involved during the ingestion of new Cromwell-based workflow, which we explain below. When Docker images are specified in the runtime section of the WDL files, the line is formatted as: runtime { ... docker: \"<repo name>/<username>/<image name>:<tag>\" ... } e.g. runtime { ... docker: \"docker.io/myUser/foo:v1\" ... } Note that we only use the image name ( foo ) when we are ingesting and preparing a new workflow for use in WebMEV. The reason is that we wish to keep all Docker images \"in house\" within our own Dockerhub account; we do not want to depend on external Docker resources which may change without our knowledge or control. Therefore, the repository and username are not used. Further, the tag is also ignored as we ultimately replace it with the git commit hash. Since the git commit hash is not produced until after the commit, we obviously can't append the images with the proper tag prior to the commit. Thus, during the ingestion process we perform the following: Parse all WDL files in the github repo and extract out all the runtime Docker images. This will be a set of strings. For each Docker \"image string\" (e.g. someUser/foo:v1 ): Extract the image name, e.g. foo Search for a corresponding Dockerfile in the repo, e.g. docker/Dockerfile.foo Build the image, tagging with the github commit ID, e.g. abc123 Push the image to the WebMEV Dockerhub, e.g. docker.io/web-mev Edit and save the WDL file with the newly-built Docker image, e.g. docker.io/web-mev/foo:abc123 . Thus, regardless of whoever creates the original image, the repository should have all the files necessary to build a fresh image which we \"claim as our own\" by assigning our username/tag and pushing it to the WebMEV Dockerhub account. We note that this technically modifies the workflow relative to the github repository, so the WebMEV-internal version is not exactly the same. However, this difference is limited to the name of the Docker image. All other aspects of the analysis are able to be exactly recreated based on the repository. Note that repositories based on many Docker containers may take a significant time to ingest, as each image must be built and pushed to Dockerhub. Copying of static resources If the static_inputs.json file is present, we expect that this file will be used for static items that are not dependent on user input. We could also put such items as default in the operation_spec.json file, but we instead choose to extract them out to this file. Upon ingestion, the files will be copied to an Operation -specific bucket/folder. For instance, given the following static_inputs.json : { \"Workflow.genome_idx\":\"gs://some-bucket/grch38.idx\" } During ingestion, this Operation will be assigned a UUID. Then, we copy this index to a new location identified by that UUID. The updated/edited static_inputs.json file will be: { \"Workflow.genome_idx\":\"gs://mev-bucket/<UUID>/grch38.idx\" }","title":"Additional notes:"},{"location":"data_server/","text":"Using the WebMeV backend as a local data server The purpose of this document is to describe the process for creating a local WebMeV API that contains all the relevant operations and result files such that a frontend application can develop against actual output files and interact just as one might with a real, production server. The motivation for this is that backend analysis tools and frontend dev typically happens in parallel. When a new tool is created, the frontend needs access to the specific output/result files for developing the new results page and visualization. Unforunately, due to the complexities of some of the analyses and how they are run (e.g. locally run or remotely via Cromwell), it's not always feasible to run a fully-featured, local API server. For example, the local machine may not have sufficient resources to run a particular analysis. Further, it is faster to develop the frontend with a set of well-defined outputs to use. Hence, what we describe here will create an API server which has \"pre-baked\" results which can be served to a frontend application. Note that the purpose of the data server is NOT to run analyses, although it is technically possible provided you are running \"local\" (i.e. Docker-based analyses) on your local machine. Finally, also note that the data export described below can export a large amount of data, depending on how many users are actively using your WebMeV deployment. Thus, for a smaller storage footprint, it may be advisable to first generate a new cloud-based deployment with a minimal set of operations and results specific to your frontend development effort. Instructions The first step is extract the data from another deployment (e.g. production) which has operations and results you wish to import. For that, we have a helper script in etc/extract_data.sh . It takes five commandline args in order: Database ID: This is the string identifier for the database instance you are extracting data from. You can find this either from your terraform deployment, by browsing to the SQL area of the Google web console, or by executing the following gcloud command: gcloud sql instances list . We are looking for the string identifier in the first, NAME column. Database name: This is the name of the database created on the instance above. This is the postgres database you are creating as part of the provisioning scripts. GCP zone: This is the zone (e.g. us-east4-c) where the compute instance (the VM serving the API you are extracting data from) is located. GCP compute instance: This is the name of the VM hosting the API API storage bucket: The cloud-based API stores all user files in a storage bucket. We need to copy those files, so this variable is the name, without the gs:// prefix , of that bucket. Run the script: etc/extract_data.sh <DB ID> <DB name> <zone> <GCP VM name> <Bucket name> and it will perform all the data extraction and copying. It will place all the necessary files into a new bucket, which it will print to your console, gs://mev-data-export-<UUID> . Downloading the extracted data/results First, we need to download the data we just extracted. The Vagrant-based provisioning script expects that we are locating this data in a folder named example_data at the root of the project (i.e. as a sibling of your Vagrantfile). If not there already, make a directory: cd mev-backend mkdir example_data then, change into that directory and use gsutil to copy everything: cd example_data gsutil -m cp -r gs://<YOUR EXPORT BUCKET>/* . Note that the -m flag is optional, but allows faster, parallel downloads. When the download is complete, you should have three items in this example_data folder: db_export.gz user_resources/ operations/ Both user_resources/ and operations/ are folders with further subdirectories. Preparing your local development instance for this data To populate our development instance with this data, we need to tell the Vagrant provisioning script that we wish to populate our local instance with an existing database dump and some result files. To do this, we simply set RESTORE_FROM_BACKUP=yes in vagrant/env.txt . If you set this variable to anything but \"yes\" , then it will NOT work. Also note that setting this variable will effectively ignore the POPULATE_DB variable which is typically used for creating fake/dummy data in a dev environment. Then, as part of the provisioning process, we have two scripts (written as Django command hooks) which 1) edits the database and 2) moves the operation folders to the appropriate locations. When editing the database, the script updates all the resource paths to reference the local, filesystem based files instead of the bucket-based objects. Further, the database editing script assigns ownership of all files and executed operations to the superuser account created as part of the provisioning. Potential pitfalls One potential pitfall is that if you run the provisioning script multiple times, you need to repopulate the test data directly from the bucket. Otherwise, the referenced paths and database dump will have been changed and we cannot guarantee the integrity of the example_data/ folder.","title":"Mock data server"},{"location":"data_server/#using-the-webmev-backend-as-a-local-data-server","text":"The purpose of this document is to describe the process for creating a local WebMeV API that contains all the relevant operations and result files such that a frontend application can develop against actual output files and interact just as one might with a real, production server. The motivation for this is that backend analysis tools and frontend dev typically happens in parallel. When a new tool is created, the frontend needs access to the specific output/result files for developing the new results page and visualization. Unforunately, due to the complexities of some of the analyses and how they are run (e.g. locally run or remotely via Cromwell), it's not always feasible to run a fully-featured, local API server. For example, the local machine may not have sufficient resources to run a particular analysis. Further, it is faster to develop the frontend with a set of well-defined outputs to use. Hence, what we describe here will create an API server which has \"pre-baked\" results which can be served to a frontend application. Note that the purpose of the data server is NOT to run analyses, although it is technically possible provided you are running \"local\" (i.e. Docker-based analyses) on your local machine. Finally, also note that the data export described below can export a large amount of data, depending on how many users are actively using your WebMeV deployment. Thus, for a smaller storage footprint, it may be advisable to first generate a new cloud-based deployment with a minimal set of operations and results specific to your frontend development effort.","title":"Using the WebMeV backend as a local data server"},{"location":"data_server/#instructions","text":"The first step is extract the data from another deployment (e.g. production) which has operations and results you wish to import. For that, we have a helper script in etc/extract_data.sh . It takes five commandline args in order: Database ID: This is the string identifier for the database instance you are extracting data from. You can find this either from your terraform deployment, by browsing to the SQL area of the Google web console, or by executing the following gcloud command: gcloud sql instances list . We are looking for the string identifier in the first, NAME column. Database name: This is the name of the database created on the instance above. This is the postgres database you are creating as part of the provisioning scripts. GCP zone: This is the zone (e.g. us-east4-c) where the compute instance (the VM serving the API you are extracting data from) is located. GCP compute instance: This is the name of the VM hosting the API API storage bucket: The cloud-based API stores all user files in a storage bucket. We need to copy those files, so this variable is the name, without the gs:// prefix , of that bucket. Run the script: etc/extract_data.sh <DB ID> <DB name> <zone> <GCP VM name> <Bucket name> and it will perform all the data extraction and copying. It will place all the necessary files into a new bucket, which it will print to your console, gs://mev-data-export-<UUID> .","title":"Instructions"},{"location":"data_server/#downloading-the-extracted-dataresults","text":"First, we need to download the data we just extracted. The Vagrant-based provisioning script expects that we are locating this data in a folder named example_data at the root of the project (i.e. as a sibling of your Vagrantfile). If not there already, make a directory: cd mev-backend mkdir example_data then, change into that directory and use gsutil to copy everything: cd example_data gsutil -m cp -r gs://<YOUR EXPORT BUCKET>/* . Note that the -m flag is optional, but allows faster, parallel downloads. When the download is complete, you should have three items in this example_data folder: db_export.gz user_resources/ operations/ Both user_resources/ and operations/ are folders with further subdirectories.","title":"Downloading the extracted data/results"},{"location":"data_server/#preparing-your-local-development-instance-for-this-data","text":"To populate our development instance with this data, we need to tell the Vagrant provisioning script that we wish to populate our local instance with an existing database dump and some result files. To do this, we simply set RESTORE_FROM_BACKUP=yes in vagrant/env.txt . If you set this variable to anything but \"yes\" , then it will NOT work. Also note that setting this variable will effectively ignore the POPULATE_DB variable which is typically used for creating fake/dummy data in a dev environment. Then, as part of the provisioning process, we have two scripts (written as Django command hooks) which 1) edits the database and 2) moves the operation folders to the appropriate locations. When editing the database, the script updates all the resource paths to reference the local, filesystem based files instead of the bucket-based objects. Further, the database editing script assigns ownership of all files and executed operations to the superuser account created as part of the provisioning.","title":"Preparing your local development instance for this data"},{"location":"data_server/#potential-pitfalls","text":"One potential pitfall is that if you run the provisioning script multiple times, you need to repopulate the test data directly from the bucket. Otherwise, the referenced paths and database dump will have been changed and we cannot guarantee the integrity of the example_data/ folder.","title":"Potential pitfalls"},{"location":"elements/","text":"Elements, Observations, and Features We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from the BaseElement class, which captures their common structure and behavior. Specialization for each can be overridden in the child classes. In the context of a biological experimental, Observation s are synonymous with samples. Further, each Observation can have Feature s associated with it (e.g. gene expressions for 30,000 genes). One can think of Observation s and Feature s as comprising the columns and rows of a two-dimensional matrix. Note that in our convention, due to the typical format of expression matrices, we take each column to represent an Observation and each row to represent a Feature . We use Observation s and Feature s to hold metadata (as key-value pairs) about data that we manipulating in WebMEV. For instance, given a typical gene expression matrix we have information about only the names of the Observation s/samples and Feature s/genes. We can then specify attributes to annotate the Observation s and Feature s, allowing users to define experimental groups, or specify other information useful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Observations and Features"},{"location":"elements/#elements-observations-and-features","text":"We adopt the convention from statistical learning of referring to Observation s and Feature s of data. Both of these data structures derive from the BaseElement class, which captures their common structure and behavior. Specialization for each can be overridden in the child classes. In the context of a biological experimental, Observation s are synonymous with samples. Further, each Observation can have Feature s associated with it (e.g. gene expressions for 30,000 genes). One can think of Observation s and Feature s as comprising the columns and rows of a two-dimensional matrix. Note that in our convention, due to the typical format of expression matrices, we take each column to represent an Observation and each row to represent a Feature . We use Observation s and Feature s to hold metadata (as key-value pairs) about data that we manipulating in WebMEV. For instance, given a typical gene expression matrix we have information about only the names of the Observation s/samples and Feature s/genes. We can then specify attributes to annotate the Observation s and Feature s, allowing users to define experimental groups, or specify other information useful for visualization or filtering. These data structures have similar (if not exactly the same) behavior but we separate them for future compatability in case specialization of each class is needed. class api.data_structures.element. BaseElement ( id , attribute_dict={} ) A BaseElement is a base class from which we can derive both Observation and Features . For the purposes of clarity, we keep those entities separate. Yet, their behavior and structure are very much the same. This also allows us to add custom behavior to each at a later time if we require. An Element is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.observation. Observation ( id , attribute_dict={} ) An Observation is the generalization of a \"sample\" in the typical context of biological studies. One may think of samples and observations as interchangeable concepts. We call it an observation so that we are not limited by this convention, however. Observation instances act as metadata and can be used to filter and subset the data to which it is associated/attached. An Observation is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } } class api.data_structures.feature. Feature ( id , attribute_dict={} ) A Feature can also be referred to as a covariate or variable. These are measurements one can make about an Observation . For example, in the genomics context, a sample can have 30,000+ genes which we call \"features\" here. In the statistical learning context, these are feature vectors. Feature instances act as metadata and can be used to filter and subset the data to which it is associated/attached. For example, we can imagine filtering by genes/features which have a particular value, such as those genes where the attribute \"oncogene\" is set to \"true\" A Feature is structured as: { \"id\": <string identifier>, \"attributes\": { \"keyA\": <Attribute>, \"keyB\": <Attribute> } }","title":"Elements, Observations, and Features"},{"location":"example_workflow/","text":"Example workflow To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Workflow and analysis concepts"},{"location":"example_workflow/#example-workflow","text":"To demonstrate how the various components of MEV come together, an graphical depiction of a typical workflow is shown below. The steps will be discussed in detail and connected to the various entities of the MEV architecture. Associated with each DataResource (AKA a file, depicted as rectangles above) is an ObservationSet , a FeatureSet , neither, or both. ObservationSet and FeatureSet s are essentially indexes on the columns/samples and the rows/genes as explained in Resource metadata . Step-by-step Denote the samples/columns of the original matrix (an instance of ObservationSet ) as all_observations . Similarly, denote all the rows/genes (an instance of FeatureSet ) as all_features . The original count matrix is run through the \"standard\"/automatic analyses. These are depicted using the gears and each are instances of Operation s. An Operation is essentially a function-- it has some input(s) and produces some output(s). Each of those Operation instances creates some output data/files. The content/format of those is not important here. Depending on the Operation , outputs could be flat files stored server-side (and served to the client) or simply data structures served to the client. One of those Operation s (PCA) allows you to create a \"selection\", which amounts to selecting a subset of all the samples. This is shown at point \"A\" in the figure. Through the UI, the user selects the desired samples (e.g. by clicking on points or dragging over areas of the PCA plot) and implicitly creates a client-side ObservationSet , which we will call pca_based_filter . This pca_based_filter is necessarily a subset of all_observations . Note that the user does not know about the concept of ObservationSet instances. Rather, they are simply selecting samples and choosing to group and label them according to some criteria (e.g. being clustered in a PCA plot). Also note that the dotted line in the figure is meant to suggest that pca_based_filter was \"inspired by\" by the PCA Operation , but did not actually derive from it. That is, while the visuals of the PCA plot were used to create the filter, the actual data of the PCA (the projected points) is not part of pca_based_filter (which is an ObservationSet ). Users can, however, name the ObservationSet so that they can be reminded of how these \"selections\" were made. At point \"B\", we apply that pca_based_filter to filter the columns of the original count matrix (recall that the columns of that original file is all_observations ). Although the icon is not a \"gear\", the green circle denoting the application of the filter is still an Operation in the MEV context. Also, note that we can apply the pca_based_filter filter to any existing file that has an ObservationSet . Obviously, it only provides a useful filter if there is a non-empty intersection of those sets; otherwise the filter produces an empty result. That is technically a valid Operation , however. At this point, the only existing DataResource /file is the original count matrix which has an ObservationSet we called all_observations . and we certainly have a non-empty intersection of the sets pca_based_filter and all_observations , so the filter is \"useful\". In accordance with our notion of an Observation , the filtering takes inputs (an ObservationSet and a DataResource ) and produces output(s); here the output is another DataResource which we call matrixA . In the backend, this creates both a physical file and a Resource in the database. Recall that a Resource is just a way for MEV to track files, but is agnostic of the \"behavior\" of the files. We next run a differential expression analysis (DESeq2) on matrixA . This produces a table of differential expression results. Note that when we choose to run DESeq2, we will be given the option of choosing from all available count matrices. In our case, that is either the original count matrix or the matrixA . We choose matrixA in the diagram. At point \"C\", we create a \"row filter\" by selecting the significantly differentially expressed genes from the DESeq2 results table. Recall that in our nomenclature we call this a FeatureSet . This FeatureSet (name it dge_fs ) can then be applied to any of the existing files where it makes sense. Again, by that I mean that it can be applied as a filter to any existing table that has a FeatureSet . Currently those are: original count matrix (where we called it all_features ) matrixA DESEq2 table Since we have not yet applied any row filters, all three of those DataResource s/files have FeatureSet s equivalent to all_features . The three files are shown flowing into node \"D\", but only one can be chosen (shown with solid line- matrixA ) At point \"D\", we apply dge_fs to matrixA in a filtering Operation . This produces a new file which we call matrixB . If you're keeping score, matrixB is basically the original table with both a row and column filter applied. We then run analyses on matrixB , such as a new PCA and a GSEA analysis. Additional notes This way of operation ends up producing multiple files that copy portions of the original matrix. We could try and be slick and store those filter operations, but it's easier to just write new files. Allowing multiple DataResource s/files within a Workspace allows us to use multiple sources of data within an analysis. In the older iterations MEV, all the analyses have to \"flow\" from a single original file. This is more or less what we did in the figure above, but we are no longer constrained to operate in that way. One could imagine adding a VCF file to the Workspace which might allow one to perform an eQTL analysis, for example.","title":"Example workflow"},{"location":"general_architecture/","text":"General architecture of WebMEV WebMEV is a Django-based RESTful API web application with the following components: Components located within the dotted outline are always located on the same server. Their roles are: The nginx server accepts requests on port 80. If the request is for static content (resources such as CSS files located at /static/ ) then nginx will directly respond to the request. Note that there are minimal CSS and JS static files associated with Django Rest Framework's browsable API. It is expected that the API will be accessed with an appropriate frontend and thus there are minimal static files related to rendering user interfaces. The gunicorn application server handles non-static requests and is the interface between nginx and Django. These connect through a unix socket. For the database - We use a postgres database to store information about users, their files and workspaces, and metadata about the analysis operations and their executions. Depending on the deployment environment, the postgres server is implemented either on the host machine or using a cloud-based service . - For local, Vagrant/virtualbox-based deployments, postgres is directly installed on the host VM. - For cloud-based deployments, we connect to a cloud-based postgres instance. For GCP, the connection is established by use of Google's cloud SQL proxy (https://cloud.google.com/sql/docs/postgres/sql-proxy). This software allows the cloud-based VM to securely connect to the database instance and creates a socket on the VM. Django can then communicate with the database via this socket as if the database server were also located on the VM. The Cromwell server To run larger, more computationally demanding jobs we connect WebMEV to a remote Cromwell job scheduler which provides on-demand compute resources. When implemented as part of a cloud-based deployment, Cromwell resides on an independent VM. Job requests are sent from the WebMEV server to Cromwell's RESTful API. Upon receiving the necessary components of a job request (WDL scripts, JSON inputs), Cromwell orchestrates the provisioning of hardware to execute the job(s). After completion, Cromwell handles the destruction of these ephemeral resources and places any output files into storage buckets. WebMEV handles the querying of job status, relocation of result files, and any other WebMEV-specific business logic.","title":"Architecture"},{"location":"general_architecture/#general-architecture-of-webmev","text":"WebMEV is a Django-based RESTful API web application with the following components: Components located within the dotted outline are always located on the same server. Their roles are: The nginx server accepts requests on port 80. If the request is for static content (resources such as CSS files located at /static/ ) then nginx will directly respond to the request. Note that there are minimal CSS and JS static files associated with Django Rest Framework's browsable API. It is expected that the API will be accessed with an appropriate frontend and thus there are minimal static files related to rendering user interfaces. The gunicorn application server handles non-static requests and is the interface between nginx and Django. These connect through a unix socket. For the database - We use a postgres database to store information about users, their files and workspaces, and metadata about the analysis operations and their executions. Depending on the deployment environment, the postgres server is implemented either on the host machine or using a cloud-based service . - For local, Vagrant/virtualbox-based deployments, postgres is directly installed on the host VM. - For cloud-based deployments, we connect to a cloud-based postgres instance. For GCP, the connection is established by use of Google's cloud SQL proxy (https://cloud.google.com/sql/docs/postgres/sql-proxy). This software allows the cloud-based VM to securely connect to the database instance and creates a socket on the VM. Django can then communicate with the database via this socket as if the database server were also located on the VM. The Cromwell server To run larger, more computationally demanding jobs we connect WebMEV to a remote Cromwell job scheduler which provides on-demand compute resources. When implemented as part of a cloud-based deployment, Cromwell resides on an independent VM. Job requests are sent from the WebMEV server to Cromwell's RESTful API. Upon receiving the necessary components of a job request (WDL scripts, JSON inputs), Cromwell orchestrates the provisioning of hardware to execute the job(s). After completion, Cromwell handles the destruction of these ephemeral resources and places any output files into storage buckets. WebMEV handles the querying of job status, relocation of result files, and any other WebMEV-specific business logic.","title":"General architecture of WebMEV"},{"location":"install/","text":"Installation instructions The WebMEV backend consists of two virtual machines: the WebMEV web application itself and a remote job runner (Cromwell). The remote job runner is technically optional, but will allow you to easily integrate large, resource-intensive workflows such as alignments. We do not discuss custom deployments where the Cromwell server is omitted. Preliminaries WebMEV was built to be a cloud-native application and assumes you will typically be operating on one of the supported cloud platform providers (currently only GCP). However, use of a commercial cloud platform is not strictly necessary if you are only interested in using WebMEV's local Docker-based job runner which permits jobs with relatively small computational requirements. For such a use-case, the local Vagrant-based environment is sufficient. If you are using a commercial cloud provider, we assume you are familiar with basic cloud-management operations such as creating storage buckets and starting/stopping compute instances. When applicable, we will highlight provider-specific differences necessary for setup/configuration of WebMEV.","title":"Preliminaries"},{"location":"install/#installation-instructions","text":"The WebMEV backend consists of two virtual machines: the WebMEV web application itself and a remote job runner (Cromwell). The remote job runner is technically optional, but will allow you to easily integrate large, resource-intensive workflows such as alignments. We do not discuss custom deployments where the Cromwell server is omitted.","title":"Installation instructions"},{"location":"install/#preliminaries","text":"WebMEV was built to be a cloud-native application and assumes you will typically be operating on one of the supported cloud platform providers (currently only GCP). However, use of a commercial cloud platform is not strictly necessary if you are only interested in using WebMEV's local Docker-based job runner which permits jobs with relatively small computational requirements. For such a use-case, the local Vagrant-based environment is sufficient. If you are using a commercial cloud provider, we assume you are familiar with basic cloud-management operations such as creating storage buckets and starting/stopping compute instances. When applicable, we will highlight provider-specific differences necessary for setup/configuration of WebMEV.","title":"Preliminaries"},{"location":"management_commands/","text":"Django management commands in WebMeV There are several custom Django management commands available. For the most part, these are used by processes such as provisioning scripts. However, there are several that are actively used by WebMeV admins. We list these below and note what they do and how they are used. All of these are initiated using the standard Django python3 manage.py <command> calls. Help for each can be found by appending the -h flag. Commands for working public datasets WebMeV provides these commands for managing public datasets (e.g. TCGA) within the application. Rather than relying on cron jobs or other periodic mechanisms for data import/indexing, we provide these commands so admins can actively manage these datasets. pull_public_data This command calls the underlying implementation for a given public dataset. Provide the unique string identifier to start the data download/processing: usage: manage.py pull_public_data -d DATASET_ID This command only performs download and data munging- it does not modify any database tables or expose any new datasets. It only handles the preparation of the data that will ultimately be indexed in another step. Note that each public dataset has, in general, different formats and requirements. We expect that the implementation of this command for each public dataset will appropriately inform the admin of its actions and any output files. For instance, the TCGA dataset icreates two files- a metadata file and a count matrix. index_data This command will index one or more flat files into the specified dataset usage: manage.py index_data -d DATASET_ID path [path ...] Note that this assumes a \"core\" (in solr parlance) already exists for the dataset; typically these are created during machine provisioning. The files to index should be amenable to the XML schema defining that core. Files should also be one of the formats that indexing technology (solr) understands, such as CSV. Use of this command requires some knowledge of how the data will be exposed via the public dataset query interface. For instance, in the case of TCGA RNA-seq data, we create two files using the pull_public_data command above. However, this indexing step will only work with CSV-format metadata file. After all, we are using the indexer to provide a means to query/search datasets for appropriate data. Attempting to index the count file would likely break the process and realistically not make much sense. Other commands add_static_operations This is used to add \"static\" operations like file transfers (e.g. the Drobox+Cromwell flow) to the application. This is different than the \"dynamic\" analysis operations that are added after the WebMeV application is running. This is typically called by the provisioning script and does not need to be run manually. In principle, one could choose to amend this script to automatically ingest a set of analysis applications. build_docs This command creates this documentation. If the --push flag is added, then the documentation will be pushed to github to the gh-pages branch. By default, it is not pushed so you can inspect the pages locally. dump_db_for_test This is a thin wrapper around the Django dumpdata command, which will dump the contents of the database models under api to a JSON file. That data can then be used for the test suite. Note that the repository includes a api/tests/test_db.json file, so this command would typically be run by a developer to add new data to the test database. edit_db_data This is used if you would like to stand up a mock server that has some data prepopulated. The script modifies database records (e.g. changing the ownership of files) accordingly. The motivation for this command was to allow us to create a backend instance to be used by a frontend developer. In that case, we would like to provide \"real\" data they can use for visualizations. However, we can't simply export data from, for instance, our production server since the resources, etc. would be owned by user accounts only present on the production system. This command handles the modification of database records to hand over ownership to a single user. populate_db This command will add some dummy data to the database. The data added are a minimum set of database records that will allow for unit testing. As mentioned, the repository includes a test database file at api/tests/test_db.json . However, one could instead use this command to populate the database with new data, then subsequently use the dumb_db_for_test command above to create a new api/tests/test_db.json file.","title":"Management commands"},{"location":"management_commands/#django-management-commands-in-webmev","text":"There are several custom Django management commands available. For the most part, these are used by processes such as provisioning scripts. However, there are several that are actively used by WebMeV admins. We list these below and note what they do and how they are used. All of these are initiated using the standard Django python3 manage.py <command> calls. Help for each can be found by appending the -h flag.","title":"Django management commands in WebMeV"},{"location":"management_commands/#commands-for-working-public-datasets","text":"WebMeV provides these commands for managing public datasets (e.g. TCGA) within the application. Rather than relying on cron jobs or other periodic mechanisms for data import/indexing, we provide these commands so admins can actively manage these datasets.","title":"Commands for working public datasets"},{"location":"management_commands/#pull_public_data","text":"This command calls the underlying implementation for a given public dataset. Provide the unique string identifier to start the data download/processing: usage: manage.py pull_public_data -d DATASET_ID This command only performs download and data munging- it does not modify any database tables or expose any new datasets. It only handles the preparation of the data that will ultimately be indexed in another step. Note that each public dataset has, in general, different formats and requirements. We expect that the implementation of this command for each public dataset will appropriately inform the admin of its actions and any output files. For instance, the TCGA dataset icreates two files- a metadata file and a count matrix.","title":"pull_public_data"},{"location":"management_commands/#index_data","text":"This command will index one or more flat files into the specified dataset usage: manage.py index_data -d DATASET_ID path [path ...] Note that this assumes a \"core\" (in solr parlance) already exists for the dataset; typically these are created during machine provisioning. The files to index should be amenable to the XML schema defining that core. Files should also be one of the formats that indexing technology (solr) understands, such as CSV. Use of this command requires some knowledge of how the data will be exposed via the public dataset query interface. For instance, in the case of TCGA RNA-seq data, we create two files using the pull_public_data command above. However, this indexing step will only work with CSV-format metadata file. After all, we are using the indexer to provide a means to query/search datasets for appropriate data. Attempting to index the count file would likely break the process and realistically not make much sense.","title":"index_data"},{"location":"management_commands/#other-commands","text":"","title":"Other commands"},{"location":"management_commands/#add_static_operations","text":"This is used to add \"static\" operations like file transfers (e.g. the Drobox+Cromwell flow) to the application. This is different than the \"dynamic\" analysis operations that are added after the WebMeV application is running. This is typically called by the provisioning script and does not need to be run manually. In principle, one could choose to amend this script to automatically ingest a set of analysis applications.","title":"add_static_operations"},{"location":"management_commands/#build_docs","text":"This command creates this documentation. If the --push flag is added, then the documentation will be pushed to github to the gh-pages branch. By default, it is not pushed so you can inspect the pages locally.","title":"build_docs"},{"location":"management_commands/#dump_db_for_test","text":"This is a thin wrapper around the Django dumpdata command, which will dump the contents of the database models under api to a JSON file. That data can then be used for the test suite. Note that the repository includes a api/tests/test_db.json file, so this command would typically be run by a developer to add new data to the test database.","title":"dump_db_for_test"},{"location":"management_commands/#edit_db_data","text":"This is used if you would like to stand up a mock server that has some data prepopulated. The script modifies database records (e.g. changing the ownership of files) accordingly. The motivation for this command was to allow us to create a backend instance to be used by a frontend developer. In that case, we would like to provide \"real\" data they can use for visualizations. However, we can't simply export data from, for instance, our production server since the resources, etc. would be owned by user accounts only present on the production system. This command handles the modification of database records to hand over ownership to a single user.","title":"edit_db_data"},{"location":"management_commands/#populate_db","text":"This command will add some dummy data to the database. The data added are a minimum set of database records that will allow for unit testing. As mentioned, the repository includes a test database file at api/tests/test_db.json . However, one could instead use this command to populate the database with new data, then subsequently use the dumb_db_for_test command above to create a new api/tests/test_db.json file.","title":"populate_db"},{"location":"metadata/","text":"About Workspace metadata As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\" } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"multiple\": true, \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side; the API will not keep the additional information about groupings that the user has defined. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. The creation and visualization of custom ObservationSet s is merely a convenience provided by the front-end (if available). After all, in direct requests to the API for analyses that require OperationSet s, the requester can create those at will. Using the metadata for analyses After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } } Additional user-supplied metadata Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex p53_mutant_status S1 M 1 S2 F 0 S3 F 0 S4 F 1 S5 M 1 S6 M 0 This would then incorporate into the existing Observation instances so they now would look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } } ] } Backend endpoints To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/ which will track the union of all the Resource metadata in the Workspace . To reduce the amount of data returned, there will also be specific endpoints for Observation s and Feature s at /api/workspace/<UUID>/metadata/observations/ and /api/workspace/<UUID>/metadata/features/ . The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S2\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S3\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"F\", \"p53_mutant_status\": 1 } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } }, { \"id\": \"P1\", \"attributes\": {} }, { \"id\": \"P2\", \"attributes\": {} }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} }, { \"id\": \"P5\", \"attributes\": {} }, { \"id\": \"P6\", \"attributes\": {} }, ] }","title":"Workspace metadata"},{"location":"metadata/#about-workspace-metadata","text":"As described elsewhere, all analyses occur in the context of a user's Workspace ; the Workspace allows users to organize files and analyses logically. To operate on the potentially multiple files contained in a user's Workspace , we are obligated to track metadata that spans across the data resources and is maintained at the level of the user's Workspace . We are typically required to provide this metadata to analyses/ Operation s, including information about samples ( Observation s), genes ( Feature s), and possibly other annotation data. A Workspace can have multiple file/ Resource objects associated with it, each of which has its own unique metadata. Therefore, we conceive of \"workspace metadata\" which is composed from the union of the individual Resource 's metadata. Consider two Resource instances in a Workspace . The first ( Resource \"A\") is data generated by a user and has six samples, which we will denote as S1,...,S6; S1-S3 are wild-type and S4-S6 are mutant. The ObservationSet associated with Resource A could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\" } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } } ] } The other ( Resource B) is public-domain data and also has six samples, which we will denote as P1,...,P6. The ObservationSet associated with Resource B could look like: { \"multiple\": true, \"elements\": [ { \"id\": \"P1\", \"attributes\": {} }, ... { \"id\": \"P6\", \"attributes\": {} } ] } (Note that for simplicity/brevity these samples don't have any annotations/attributes for this example). Now, as far as the Workspace is concerned, there are 12 Observation instances by performing a union of the Observation s contained in the ObservationSet associated with each Resource . In the course of performing an analysis, the user might wish to create meaningful \"groups\" of these samples. Perhaps they merge the two count matrices underlying Resource s A and B, and perform a principal component analysis (PCA) on the output. They then note a clustering of the samples which they perceive as meaningful: (Via the dynamic user interface, we imagine the user selecting the five samples in the grey ellipse-- two of the public \"P\" samples, P3 and P4 cluster with the WT samples). They can then choose to create a new ObservationSet from those five samples: { \"multiple\": true, \"elements\": [ { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\" } }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} } ] } This information regarding user-defined groupings can be cached client-side; the API will not keep the additional information about groupings that the user has defined. However, the ultimate \"source\" of the metadata is provided by the Workspace , which maintains the ObservationSet s, FeatureSet s, and possibly other metadata. The creation and visualization of custom ObservationSet s is merely a convenience provided by the front-end (if available). After all, in direct requests to the API for analyses that require OperationSet s, the requester can create those at will.","title":"About Workspace metadata"},{"location":"metadata/#using-the-metadata-for-analyses","text":"After the user has created their own ObservationSet instances, they can use them for analyses such as a differential expression analysis. For instance, the inputs to such an analysis would be an expression matrix (perhaps the result of merging the \"S\" and \"P\" samples/ Observation s) and two ObservationSet instances. The payload to start such an analysis (sent to /api/operations/run/ ) would look something like: { \"operation_id\": <UUID for Operation>, \"workspace_id\": <UUID for Workspace>, \"inputs\": { \"count_matrix\": <UUID for merged Resource>, \"groupA\": <ObservationSet with S4,S5,S6,P3,P4>, \"groupB\": <ObservationSet with S1,S2,S3,P5,P6> } }","title":"Using the metadata for analyses"},{"location":"metadata/#additional-user-supplied-metadata","text":"Finally, the users might want to add additional annotations to their metadata. For instance, assuming we still are working with Resource instances A and B, we could upload an additional Resource with type \"ANN\" (for ann otation) and add it to this Workspace . For instance, maybe it looks like: sample sex p53_mutant_status S1 M 1 S2 F 0 S3 F 0 S4 F 1 S5 M 1 S6 M 0 This would then incorporate into the existing Observation instances so they now would look like: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, ... { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } } ] }","title":"Additional user-supplied metadata"},{"location":"metadata/#backend-endpoints","text":"To provide a \"single source of truth\", there will be a \"workspace metadata\" endpoint at /api/workspace/<UUID>/metadata/ which will track the union of all the Resource metadata in the Workspace . To reduce the amount of data returned, there will also be specific endpoints for Observation s and Feature s at /api/workspace/<UUID>/metadata/observations/ and /api/workspace/<UUID>/metadata/features/ . The front-end will maintain the various user selections (formerly \"sample sets\", now ObservationSet ) but the full set of available Observation instances will be kept on the backend. Using the example above, a request to /api/workspace/<UUID>/metadata/observations/ would return: { \"multiple\": true, \"elements\": [ { \"id\": \"S1\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S2\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S3\", \"attributes\": { \"genotype\": \"WT\", \"sex\": \"F\", \"p53_mutant_status\": 0 } }, { \"id\": \"S4\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"F\", \"p53_mutant_status\": 1 } }, { \"id\": \"S5\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 1 } }, { \"id\": \"S6\", \"attributes\": { \"genotype\": \"mutant\", \"sex\": \"M\", \"p53_mutant_status\": 0 } }, { \"id\": \"P1\", \"attributes\": {} }, { \"id\": \"P2\", \"attributes\": {} }, { \"id\": \"P3\", \"attributes\": {} }, { \"id\": \"P4\", \"attributes\": {} }, { \"id\": \"P5\", \"attributes\": {} }, { \"id\": \"P6\", \"attributes\": {} }, ] }","title":"Backend endpoints"},{"location":"mev_cluster_setup/","text":"The WebMEV API This section of the installation instructions describes: Creation of a local development stack using Vagrant Creation of a cloud-based stack As described in the general architecture section, WebMEV consists of two servers: the main Django-based application server and the remote job scheduler (Cromwell). The application server consists of a production web server (nginx), an application server (gunicorn), the Django-based application, and a database. The database can be implemented locally (when in the local, Vagrant-based build) or with a cloud-based instance (when deploying on a cloud platform). Regardless of the implementation of the database server, Django communicates with the database via a unix socket on the VM. Local development using Vagrant Install Git , VirtualBox , and Vagrant Clone the WebMEV backend repository: git clone https://github.com/web-mev/mev-backend.git Change into the repository directory: cd mev-backend Copy the template file for the environment variables you will need: cp vagrant/env.tmpl vagrant/env.txt . Note that the name/path of the file is important since the provisioning script makes reference to a file at this path. Fill in the proper environment variables in that copied file (e.g. vagrant/env.txt above). Note that for local dev like unit testing, many of the variables can be left as-is. However, if you would like to test the integration with dockerhub, gmail, then you can certainly fill-in those variables. Provision and start the VM: vagrant up SSH into the VM: vagrant ssh You are now ready to develop locally. Note that the code is shared between the host and VM at the /vagrant directory. To run unit tests cd /vagrant/mev python3 manage.py test (use the --failfast flag if you want to fail the test suite on the first failure encountered) To run the application locally By default, the provisioning script does not start the gunicorn application server. To do that, simply run: sudo supervisorctl start gunicorn (Note that you need to use elevated privileges since you are interacting with the supervisor daemon. Otherwise you will get a permission error.) Deploying to GCP using terraform The GCP deployment includes two virtual machines, a managed cloud-based database, and several network services. In particular, we employ a public-facing load balancer which obviates the need for configuring and managing SSL certificates on the individual VMs. Instead, the load balancer and Google-managed SSL certificates (with HTTPS forwarding rules) provide secure connections to WebMeV. In addition to having a GCP account, this deployment strategy assumes: You have a custom domain you wish to use You will only allow HTTPS connections (HTTP will be upgraded to HTTPS) You have already created a \"managed zone\" in GCP's Cloud DNS and your domain registrar is using the Cloud DNS nameservers. Note that Cloud DNS is NOT the same as Google Domains, if that happens to be your domain registrar. We do this since the terraform script dynamically creates DNS records which will point at the load balancer IP. These items are obviously not required , but you would need to modify the terraform scripts. To deploy Install Google Cloud SDK and Terraform Create a service account with the appropriate roles and obtain a JSON-format service account key file. You can do this via the gcloud commandline tool or using the web console . Keep the key file safe. To allow https, we need an SSL certificate. The gcloud commandline tool provides a convenient way to provision one: gcloud beta compute ssl-certificates create <CERTIFICATE NAME> --description=\"SSL certificate for WebMeV dev server\" --domains=\"mev-api.example.org\" Note: until the full stack is up and the load balancer is active, this will remain in a \"provisioning\" state; specify only the backend domain in the above command. cd deploy/terraform/live terraform init cp config.tfvars.template terraform.tfvars Edit terraform.tfvars to assign required configuration values. You can refer to the vars.tf file for explanations of these variables and what they do. These are obviously the same environment variables used in the local, Vagrant-based deployment, so further information may be available in the comments of the vagrant/env.tmpl file at the root of this repository. Special notes: - The ssl_cert variable refers to the SSL certificate you created above. It is a resource string like: projects/<GCP PROJECT>/global/sslCertificates/<CERTIFICATE NAME> - The credentials_file variable is the JSON-format key file you obtained for your service account. Assuming you saved this file in your deploy/terraform/live/ directory, then you can simply write the file name. - The service_account_email variable is the email-like name of the service account you created. For example, abc123@<GCP PROJECT>.iam.gserviceaccount.com Create and set your terraform workspace. Note that we make use of terraform workspaces which allow independent deployments using the same scripts. The name of the workspace is typically used when naming the cloud-based resources created (e.g. backend-<WORKSPACE NAME> ) to make them more obvious when interacting with the GCP console or gcloud tool. By default, the workspace name is, appropriately enough, default . You can create a new workspace with: terraform workspace new <WORKSPACE NAME> and you can switch between workspaces with: terraform workspace select <WORKSPACE NAME> Once you have set a workspace, you can build it with terraform apply This will prompt for any configuration variables that were not included in terraform.tfvars . Often, variables like these are intentionally left out to force the user to take a moment and think about what they are doing. One example is the git commit ID. This will tell terraform which git commit should be deployed. We intentionally prompt for this so that the developer is forced to examine the exact version of the application they wish to deploy. - To delete your resources: terraform destroy Creating a service account with the GCP console Navigate to \"IAM & Admin\" --> \"Service Accounts\": Click the \"create service account\" button at the top: Give the account a name: Add \"roles\" to this service account. To work with the deployment, you will need: Editor ( roles/editor ) Service Networking Admin ( roles/servicenetworking.networksAdmin ) After the account is created, you will be directed to the page with all the service accounts for your your GCP project. Click on the one you just created: Go to the \"keys\" tab and create a new JSON key: Save this key in your deploy/terraform/live/ folder. Then the terraform.tfvars file can reference this file easily. Creating a service account with the gcloud tool Create a service account: gcloud iam service-accounts create <NAME> --display-name=\"<SOME NAME>\" e.g. gcloud iam service-accounts create abc123 --display-name=\"A basic name\" The identifier \"abc123\" below will create a service account with a \"full\" email-like name, e.g. abc123@myproject.iam.gserviceaccount.com . The name needs to conform to some basic requirements (minimum length, characters) and gcloud will warn you if you it doesn't comply. Add the appropriate policies to this service account. This will enable the account to perform certain actions like creation of VMs, etc. Specifically, you will need to add the roles/editor and roles/servicenetworking.networksAdmin to your service account. The latter will enable your application server to connect to the cloud-based database. Project \"editor\" permissions alone will not be adequate. gcloud projects add-iam-policy-binding myproject \\ --member='serviceAccount:abc123@myproject.iam.gserviceaccount.com' \\ --role='roles/editor' gcloud projects add-iam-policy-binding myproject \\ --member='serviceAccount:abc123@myproject.iam.gserviceaccount.com' \\ --role='roles/servicenetworking.networksAdmin' Then create a JSON key file which terraform will use: gcloud iam service-accounts keys create my_gcp_key.json \\ --iam-account=abc123@myproject.iam.gserviceaccount.com","title":"Installation and setup"},{"location":"mev_cluster_setup/#the-webmev-api","text":"This section of the installation instructions describes: Creation of a local development stack using Vagrant Creation of a cloud-based stack As described in the general architecture section, WebMEV consists of two servers: the main Django-based application server and the remote job scheduler (Cromwell). The application server consists of a production web server (nginx), an application server (gunicorn), the Django-based application, and a database. The database can be implemented locally (when in the local, Vagrant-based build) or with a cloud-based instance (when deploying on a cloud platform). Regardless of the implementation of the database server, Django communicates with the database via a unix socket on the VM.","title":"The WebMEV API"},{"location":"mev_cluster_setup/#local-development-using-vagrant","text":"Install Git , VirtualBox , and Vagrant Clone the WebMEV backend repository: git clone https://github.com/web-mev/mev-backend.git Change into the repository directory: cd mev-backend Copy the template file for the environment variables you will need: cp vagrant/env.tmpl vagrant/env.txt . Note that the name/path of the file is important since the provisioning script makes reference to a file at this path. Fill in the proper environment variables in that copied file (e.g. vagrant/env.txt above). Note that for local dev like unit testing, many of the variables can be left as-is. However, if you would like to test the integration with dockerhub, gmail, then you can certainly fill-in those variables. Provision and start the VM: vagrant up SSH into the VM: vagrant ssh You are now ready to develop locally. Note that the code is shared between the host and VM at the /vagrant directory. To run unit tests cd /vagrant/mev python3 manage.py test (use the --failfast flag if you want to fail the test suite on the first failure encountered) To run the application locally By default, the provisioning script does not start the gunicorn application server. To do that, simply run: sudo supervisorctl start gunicorn (Note that you need to use elevated privileges since you are interacting with the supervisor daemon. Otherwise you will get a permission error.)","title":"Local development using Vagrant"},{"location":"mev_cluster_setup/#deploying-to-gcp-using-terraform","text":"The GCP deployment includes two virtual machines, a managed cloud-based database, and several network services. In particular, we employ a public-facing load balancer which obviates the need for configuring and managing SSL certificates on the individual VMs. Instead, the load balancer and Google-managed SSL certificates (with HTTPS forwarding rules) provide secure connections to WebMeV. In addition to having a GCP account, this deployment strategy assumes: You have a custom domain you wish to use You will only allow HTTPS connections (HTTP will be upgraded to HTTPS) You have already created a \"managed zone\" in GCP's Cloud DNS and your domain registrar is using the Cloud DNS nameservers. Note that Cloud DNS is NOT the same as Google Domains, if that happens to be your domain registrar. We do this since the terraform script dynamically creates DNS records which will point at the load balancer IP. These items are obviously not required , but you would need to modify the terraform scripts. To deploy Install Google Cloud SDK and Terraform Create a service account with the appropriate roles and obtain a JSON-format service account key file. You can do this via the gcloud commandline tool or using the web console . Keep the key file safe. To allow https, we need an SSL certificate. The gcloud commandline tool provides a convenient way to provision one: gcloud beta compute ssl-certificates create <CERTIFICATE NAME> --description=\"SSL certificate for WebMeV dev server\" --domains=\"mev-api.example.org\" Note: until the full stack is up and the load balancer is active, this will remain in a \"provisioning\" state; specify only the backend domain in the above command. cd deploy/terraform/live terraform init cp config.tfvars.template terraform.tfvars Edit terraform.tfvars to assign required configuration values. You can refer to the vars.tf file for explanations of these variables and what they do. These are obviously the same environment variables used in the local, Vagrant-based deployment, so further information may be available in the comments of the vagrant/env.tmpl file at the root of this repository. Special notes: - The ssl_cert variable refers to the SSL certificate you created above. It is a resource string like: projects/<GCP PROJECT>/global/sslCertificates/<CERTIFICATE NAME> - The credentials_file variable is the JSON-format key file you obtained for your service account. Assuming you saved this file in your deploy/terraform/live/ directory, then you can simply write the file name. - The service_account_email variable is the email-like name of the service account you created. For example, abc123@<GCP PROJECT>.iam.gserviceaccount.com Create and set your terraform workspace. Note that we make use of terraform workspaces which allow independent deployments using the same scripts. The name of the workspace is typically used when naming the cloud-based resources created (e.g. backend-<WORKSPACE NAME> ) to make them more obvious when interacting with the GCP console or gcloud tool. By default, the workspace name is, appropriately enough, default . You can create a new workspace with: terraform workspace new <WORKSPACE NAME> and you can switch between workspaces with: terraform workspace select <WORKSPACE NAME> Once you have set a workspace, you can build it with terraform apply This will prompt for any configuration variables that were not included in terraform.tfvars . Often, variables like these are intentionally left out to force the user to take a moment and think about what they are doing. One example is the git commit ID. This will tell terraform which git commit should be deployed. We intentionally prompt for this so that the developer is forced to examine the exact version of the application they wish to deploy. - To delete your resources: terraform destroy Creating a service account with the GCP console Navigate to \"IAM & Admin\" --> \"Service Accounts\": Click the \"create service account\" button at the top: Give the account a name: Add \"roles\" to this service account. To work with the deployment, you will need: Editor ( roles/editor ) Service Networking Admin ( roles/servicenetworking.networksAdmin ) After the account is created, you will be directed to the page with all the service accounts for your your GCP project. Click on the one you just created: Go to the \"keys\" tab and create a new JSON key: Save this key in your deploy/terraform/live/ folder. Then the terraform.tfvars file can reference this file easily. Creating a service account with the gcloud tool Create a service account: gcloud iam service-accounts create <NAME> --display-name=\"<SOME NAME>\" e.g. gcloud iam service-accounts create abc123 --display-name=\"A basic name\" The identifier \"abc123\" below will create a service account with a \"full\" email-like name, e.g. abc123@myproject.iam.gserviceaccount.com . The name needs to conform to some basic requirements (minimum length, characters) and gcloud will warn you if you it doesn't comply. Add the appropriate policies to this service account. This will enable the account to perform certain actions like creation of VMs, etc. Specifically, you will need to add the roles/editor and roles/servicenetworking.networksAdmin to your service account. The latter will enable your application server to connect to the cloud-based database. Project \"editor\" permissions alone will not be adequate. gcloud projects add-iam-policy-binding myproject \\ --member='serviceAccount:abc123@myproject.iam.gserviceaccount.com' \\ --role='roles/editor' gcloud projects add-iam-policy-binding myproject \\ --member='serviceAccount:abc123@myproject.iam.gserviceaccount.com' \\ --role='roles/servicenetworking.networksAdmin' Then create a JSON key file which terraform will use: gcloud iam service-accounts keys create my_gcp_key.json \\ --iam-account=abc123@myproject.iam.gserviceaccount.com","title":"Deploying to GCP using terraform"},{"location":"observation_and_feature_sets/","text":"ObservationSet and FeatureSet We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class api.data_structures.observation_set. ObservationSet ( init_elements , multiple=True ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Observation>, <Observation>, ... ] } class api.data_structures.feature_set. FeatureSet ( init_elements , multiple=True ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSets"},{"location":"observation_and_feature_sets/#observationset-and-featureset","text":"We can compose unique sets of Observation and Feature instances into ObservationSet and FeatureSet instances, respectively. These data structures can be used as a way to subset/filter samples/observations or to provide groups of samples to analyses (such as when comparing two groups for a differential expression analysis). class api.data_structures.observation_set. ObservationSet ( init_elements , multiple=True ) An ObservationSet is a collection of unique Observation instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the ObservationSet is the set of samples that were assayed. We depend on the native python set data structure and appropriately hashable/comparable Observation instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Observation s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Observation>, <Observation>, ... ] } class api.data_structures.feature_set. FeatureSet ( init_elements , multiple=True ) A FeatureSet is a collection of unique Feature instances and is typically used as a metadata data structure attached to some \"real\" data. For instance, given a matrix of gene expressions, the FeatureSet is the set of genes. We depend on the native python set data structure and appropriately hashable/comparable Feature instances. This essentially copies most of the functionality of the native set class, simply passing through the operations, but includes some additional members specific to our application. Notably, we disallow (i.e. raise exceptions) if there are attempts to create duplicate Feature s, in contrast to native sets which silently ignore duplicate elements. A serialized representation would look like: { \"multiple\": <bool>, \"elements\": [ <Feature>, <Feature>, ... ] }","title":"ObservationSet and FeatureSet"},{"location":"operation_resources/","text":"Operation resources For certain Operation s, we require data such as gene lists, gene-alias lookups, genome indices, and similar files which are not provided or owned by WebMEV users. Instead, these special Resource s are associated with the Operation they are used with. Suggestively, we call these OperationResource instances. Depending on the run-mode of the Operation , these OperationResource s are handled in different ways. Regardless of the run-mode of the Operation (i.e. local, remote), we can handle these operation-specific resources in two ways. One method relies on the image container to hold the file. The other method requires WebMEV to \"register\" the OperationResource just as it does with user-associated Resource s . The first method to distribute these user-independent files is to simply build them directly into the container. For small files, this solution is straightforward and does not require any special handling by WebMEV itself. For reproducing analyses, the versioned container image can always be pulled and run, ensuring the same files are used. Note that this assumes the files are \"static\" within that container and not dynamically queried/generated when the container is executed for an analysis operation. For instance, if the gene annotations, etc. are queried from BioMart on-the-fly during an analysis execution, then we can't necessarily guarantee 100% fidelity as the underlying data may change. On the other hand, if the files are created upon building/pushing the image, then those files can be safely distributed with the image and will remain unchanged within that image. Instead of building the files directly into the container image, one may also choose to create the files up front and then have WebMEV \"register\" them when the Operation is ingested. We do this by including an addition file ( operation_resources.json ) with the repository. This file gives the \"name\" of the file, its resource type (e.g. integer matrix, BED file, etc.), and the path to the file. For example, if we have an alignment process that depends on a pre-computed index, our operation_spec.json definition file would have an OperationResource input as follows: { \"name\": \"BWA alignment\", ... \"inputs\": { ... \"genome_index\": { \"description\": \"The genome to align to.\", \"name\": \"Genome choice:\", \"required\": true, \"spec\": { \"attribute_type\": \"OperationResource\", \"resource_type\": \"*\" } } ... } ... } (the resource_type wildcard means \"any\" file type, which is fine for our purposes here). Then, in the same repository, we would have an operation_resources.json that looks like: { \"genome_index\": [ { \"name\": \"Human\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/human_index.tar\" }, { \"name\": \"Mouse\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/mouse_index.tar\" } ] } Upon ingestion of this Operation , WebMEV will check for the presence of those files and, if present, will create database objects for these files. When the user wishes to run this Operation , those resources will be presented as the available options for that genome_index input. In accordance with our goal of reproducible research, the ingestion process copies the files to an operation-specific directory. Thus, given the Operation 's UUID, we will copy the file(s) to a storage location identified by that UUID. This way, we can prevent that original path from being overwritten or deleted over time. Note that the path provided may either be \"local\" (e.g. \"path\": \"some_file.tsv\" ) or remote (e.g. \"path\": \"gs://my-bucket/human_index.tar ). In the former case (where the path is relative or does not specify a \"special\" prefix like \"gs://\") WebMEV expects the file to be among the cloned repository files. Otherwise, we confirm the existence of the file using the appropriate libraries capable of interfacing with the remote storage systems. If the files are not found, then the ingestion of the Operation fails. The \"repository-tracked\" files are limited to relatively small files, however. Larger files, such as genome indexes which can be many GB, must be done using the remote storage option, which does require a bit more care to ensure everything is in-sync prior to ingestion. After ingestion, everything is properly versioned by reference to the Operation s UUID. As a final note, in the spirit of reproducible analyses, we advise that all scripts, etc. used to create the resource files are included in the repository or are otherwise adequately described. This is not something that can be enforced by WebMEV, but we aim to adhere with this guideline.","title":"Operation-associated resources"},{"location":"operation_resources/#operation-resources","text":"For certain Operation s, we require data such as gene lists, gene-alias lookups, genome indices, and similar files which are not provided or owned by WebMEV users. Instead, these special Resource s are associated with the Operation they are used with. Suggestively, we call these OperationResource instances. Depending on the run-mode of the Operation , these OperationResource s are handled in different ways. Regardless of the run-mode of the Operation (i.e. local, remote), we can handle these operation-specific resources in two ways. One method relies on the image container to hold the file. The other method requires WebMEV to \"register\" the OperationResource just as it does with user-associated Resource s . The first method to distribute these user-independent files is to simply build them directly into the container. For small files, this solution is straightforward and does not require any special handling by WebMEV itself. For reproducing analyses, the versioned container image can always be pulled and run, ensuring the same files are used. Note that this assumes the files are \"static\" within that container and not dynamically queried/generated when the container is executed for an analysis operation. For instance, if the gene annotations, etc. are queried from BioMart on-the-fly during an analysis execution, then we can't necessarily guarantee 100% fidelity as the underlying data may change. On the other hand, if the files are created upon building/pushing the image, then those files can be safely distributed with the image and will remain unchanged within that image. Instead of building the files directly into the container image, one may also choose to create the files up front and then have WebMEV \"register\" them when the Operation is ingested. We do this by including an addition file ( operation_resources.json ) with the repository. This file gives the \"name\" of the file, its resource type (e.g. integer matrix, BED file, etc.), and the path to the file. For example, if we have an alignment process that depends on a pre-computed index, our operation_spec.json definition file would have an OperationResource input as follows: { \"name\": \"BWA alignment\", ... \"inputs\": { ... \"genome_index\": { \"description\": \"The genome to align to.\", \"name\": \"Genome choice:\", \"required\": true, \"spec\": { \"attribute_type\": \"OperationResource\", \"resource_type\": \"*\" } } ... } ... } (the resource_type wildcard means \"any\" file type, which is fine for our purposes here). Then, in the same repository, we would have an operation_resources.json that looks like: { \"genome_index\": [ { \"name\": \"Human\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/human_index.tar\" }, { \"name\": \"Mouse\", \"resource_type\": \"*\", \"path\": \"gs://my-bucket/mouse_index.tar\" } ] } Upon ingestion of this Operation , WebMEV will check for the presence of those files and, if present, will create database objects for these files. When the user wishes to run this Operation , those resources will be presented as the available options for that genome_index input. In accordance with our goal of reproducible research, the ingestion process copies the files to an operation-specific directory. Thus, given the Operation 's UUID, we will copy the file(s) to a storage location identified by that UUID. This way, we can prevent that original path from being overwritten or deleted over time. Note that the path provided may either be \"local\" (e.g. \"path\": \"some_file.tsv\" ) or remote (e.g. \"path\": \"gs://my-bucket/human_index.tar ). In the former case (where the path is relative or does not specify a \"special\" prefix like \"gs://\") WebMEV expects the file to be among the cloned repository files. Otherwise, we confirm the existence of the file using the appropriate libraries capable of interfacing with the remote storage systems. If the files are not found, then the ingestion of the Operation fails. The \"repository-tracked\" files are limited to relatively small files, however. Larger files, such as genome indexes which can be many GB, must be done using the remote storage option, which does require a bit more care to ensure everything is in-sync prior to ingestion. After ingestion, everything is properly versioned by reference to the Operation s UUID. As a final note, in the spirit of reproducible analyses, we advise that all scripts, etc. used to create the resource files are included in the repository or are otherwise adequately described. This is not something that can be enforced by WebMEV, but we aim to adhere with this guideline.","title":"Operation resources"},{"location":"operations/","text":"Operations and ExecutedOperations An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository_url\": <string url>, \"repo_name\": <string>. \"git_hash\": <string>, \"workspace_operation\": <bool> } where: mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) workspace_operation : This bool tells us whether the operation is run within the context of a user's workspace. Some operations, such as file uploads, can be run outside of a workspace and hence don't require some of the additional fields that a workspace-associated operation would use. repository_url , repo_name : identifies the github repo (and name) used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). The spec allows us to validate a user's input for compliance with the expected type; for instance, when an analysis is requested, the spec ensures that we get sensible inputs for that field. ExecutedOperation s Once a user makes a request with the proper, validated inputs, we create an ExecutedOperation which tracks the execution of the job. An ExecutedOperation tracks the following: The Workspace , assuming the underlying Operation has workspace_operation=True . This gives access to the user who initiated the execution. a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings A concrete example For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields. Executing an Operation The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0]. Operation modes Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be built and pushed to Dockerhub during ingestion. The image will be tagged with the github commit hash so that the file(s) used to build the image can be uniquely identified and re-traced over time.","title":"Operation concepts"},{"location":"operations/#operations-and-executedoperations","text":"An Operation is any manipulation of some data that produces some output; it defines the type of analysis that is run, its inputs and outputs, and other relevant information. An Operation can be as simple as selecting a subset of the columns/rows of a matrix or running a large-scale processing job that spans many machines and significant time. An ExecutedOperation represents an actual execution of an Operation . While the Operation identifies the process used, the ExecutedOperation contains information about the actual execution, such as the job status, the exact inputs and outputs, the runtime, and other relevant information. Clearly, the ExecutedOperation maintains a foreign-key relation to the Operation . The various ExecutedOperation s performed in MEV will all create some output so that there will be no ambiguity regarding how data was manipulated through the course of an analysis workflow. Essentially, we do not perform in-place operations on data. For example, if a user chooses a subset of samples/columns in their expression matrix, we create a new DataResource (and corresponding Resource database record). While this has the potential to create multiple files with similar data, this makes auditing a workflow history much simpler. Typically, the size of files where users are interacting with the data are relatively small (on the order of MB) so excessive storage is not a major concern. Note that client-side manipulations of the data, such as filtering out samples are distinct from this concept of an Operation / ExecutedOperation . That is, users can select various filters on their data to change the visualizations without executing any Operation s. However , once they choose to use the subset data for use in an analysis, they will be required to implicitly execute an Operation on the backend. As a concrete example, consider analyzing a large cohort of expression data from TCGA. A user initially imports the expression matrix and perhaps uses PCA or some other clustering method in an exploratory analysis. Each of those initial analyses were ExecutedOperation s. Based on those initial visualizations, the user may select a subset of those samples to investigate for potential subtypes; note that those client-side sample selections have not triggered any actual analyses. However, once they choose to run those samples through a differential expression analysis, we require that they perform filter/subset Operation . As new DataResource s are created, the metadata tracks which ExecutedOperation created them, addressed by the UUID assigned to each ExecutedOperation . By tracing the foreign-key relation, we can determine the exact Operation that was run so that the steps of the analysis are transparent and reproducible. Operation s can be lightweight jobs such as a basic filter or a simple R script, or involve complex, multi-step pipelines involving WDL and Cromwell. Depending on the computational resources needed, the Operation can be run locally or remotely. As jobs complete, their outputs will populate in the user's workspace and further analysis can be performed. All jobs, whether local or remote, will be placed in a queue and executed ascynchronously. Progress/status of remote jobs can be monitored by querying the Cromwell server. Also note that ALL Operation s (even basic table filtering) are executed in Docker containers so that the software and environments can be carefully tracked and controlled. This ensures a consistent \"plugin\" style architecture so that new Operation s can be integrated consistently. Operation s should maintain the following data: unique identifier (UUID) name description of the analysis Inputs to the analysis. These are the acceptable types and potentially some parameters. For instance, a DESeq2 analysis should take an IntegerMatrix , two ObservationSet instances (defining the contrast groups), and a string \"name\" for the contrast. See below for a concrete example. Outputs of the analysis. This would be similar to the inputs in that it describes the \"types\" of outputs. Again, using the DESEq2 example, the outputs could be a \"feature table\" ( FT , FeatureTable ) giving the table of differentially expressed genes (e.g. fold-change, p-values) and a normalized expression matrix of type \"expression matrix\" ( EXP_MTX ). github repo/Docker info (commit hashes) so that the backend analysis code may be traced whether the Operation is a \"local\" one or requires use of the Cromwell engine. The front-end users don't need to know that, but internally MEV needs to know how to run the analysis. Note that some of these will be specified by whoever creates the analysis. However, some information (like the UUID identifier, git hash, etc.) will be populated when the analysis is \"ingested\". It should not be the job of the analysis developer to maintain those pieces of information and we can create them on the fly during ingestion. Therefore, an Operation has the following structure: { \"id\": <UUID>, \"name\": <string>, \"description\": <string>, \"inputs\": Object<OperationInput>, \"outputs\": Object<OperationOutput>, \"mode\": <string>, \"repository_url\": <string url>, \"repo_name\": <string>. \"git_hash\": <string>, \"workspace_operation\": <bool> } where: mode : identifies how the analysis is run. Will be one of an enum of strings (e.g. local_docker , cromwell ) workspace_operation : This bool tells us whether the operation is run within the context of a user's workspace. Some operations, such as file uploads, can be run outside of a workspace and hence don't require some of the additional fields that a workspace-associated operation would use. repository_url , repo_name : identifies the github repo (and name) used to pull the data. For the ingestion, admins will give that url which will initiate a clone process. git_hash : This is the commit hash which uniquely identifies the code state. This way the analysis code can be exactly traced back. Both inputs and outputs address nested objects. That is, they are mappings of string identifiers to OperationInput or OperationOutput instances: { 'abc': <OperationInput/OperationOutput>, 'def': <OperationInput/OperationOutput> } An OperationInput looks like: { \"description\": <string>, \"name\": <string>, \"required\": <bool>, \"spec\": <InputSpec> } (and similarly for OperationOutput , which has fewer keys). As an example of an OperationInputs , consider a p-value for thresholding: { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 } } The spec key addresses a child class of InputSpec whose behavior is specific to each \"type\" (above, a BoundedFloat ). The spec allows us to validate a user's input for compliance with the expected type; for instance, when an analysis is requested, the spec ensures that we get sensible inputs for that field.","title":"Operations and ExecutedOperations"},{"location":"operations/#executedoperations","text":"Once a user makes a request with the proper, validated inputs, we create an ExecutedOperation which tracks the execution of the job. An ExecutedOperation tracks the following: The Workspace , assuming the underlying Operation has workspace_operation=True . This gives access to the user who initiated the execution. a foreign-key to the Operation \"type\" unique identifier (UUID) for the execution a job identifier. We need the Cromwell job UUID to track the progress as we query the Cromwell server. For Docker-based jobs, we can set the tag on the container and then query its status (e.g. if it's still \"up\", then the job is still going) The inputs to the analysis (a JSON document) The outputs (another JSON document) once complete Job execution status (e.g. \"running\", \"complete\", \"failed\", etc.) Start time Completion time Any errors or warnings","title":"ExecutedOperations"},{"location":"operations/#a-concrete-example","text":"For this, consider a differential expression analysis (e.g. like DESeq2). In this simplified analysis, we will take a count matrix, a p-value (for filtering significance based on some hypothesis test), and an output file name. For outputs, we have a single file which has the results of the differential expression testing on each gene. Since each row concerns a gene (and the columns give information about that gene), the output file is a \"feature table\" in our nomenclature. Thus, the file which defines this analysis would look like: { \"name\":\"DESeq2 differential gene expression\", \"description\": \"Find genes which are differentially expressed and filter...\" \"inputs\": { \"count_matrix\": { \"description\": \"The count matrix of expressions\", \"name\": \"Count matrix:\", \"required\": true, \"spec\": { \"attribute_type\": \"DataResource\", \"resource_types\": [\"RNASEQ_COUNT_MTX\", \"I_MTX\"], \"many\": false } }, \"p_val\": { \"description\": \"The filtering threshold for the p-value\", \"name\": \"P-value threshold:\", \"required\": false, \"spec\": { \"attribute_type\": \"BoundedFloat\", \"min\": 0, \"max\": 1.0, \"default\": 0.05 }, } \"output_filename\": { \"description\": \"The name of the contrast for your own reference.\", \"name\": \"Contrast name:\", \"required\": false, \"spec\": { \"attribute_type\": \"String\", \"default\": \"deseq2_results\" } } }, \"outputs\": { \"dge_table\": { \"spec\":{ \"attribute_type\": \"DataResource\", \"resource_type\": \"FT\" } } } } This specification will be placed into a file. In the repo, there will be a Dockerfile and possibly other files (e.g. scripts). Upon ingestion, MEV will read this inputs file, get the commit hash, assign a UUID, build the container, push the container, etc. As mentioned before, we note that the JSON above does not contain all of the required fields to create an Operation instance; it is missing id , git_hash , and repository_url . Note that when the API endpoint /api/operations/ is requested, the returned object will match that above, but will also contain those required additional fields.","title":"A concrete example"},{"location":"operations/#executing-an-operation","text":"The Operation objects above are typically used to populate the user interface such that the proper input fields can be displayed (e.g. a file chooser for an input that specifies it requires a DataResource ). To actually initiate the Operation , thus creating an ExecutedOperation , the front-end (or API request) will need to POST a payload with the proper parameters/inputs. The backend will check those inputs against the specification. As an example, a valid payload for the above would be: { \"operation_id\": <UUID>, \"workspace_id\": <UUID>, \"inputs\": { \"count_matrix\": <UUID of Resource> \"p_val\": 0.01 } } (note that the \"output_filename\" field was not required so we do not need it here) The operation_id allows us to locate the Operation that we wish to run and the workspace_id allows us to associate the eventual ExecutedOperation with a Workspace . Finally, the inputs key is an object of key-value pairs. Depending on the \"type\" of the input, the values can be effectively arbitrary. Walking through the backend logic In the backend, we locate the proper Operation by its UUID. In our example, we see that this Operation expects two required inputs: \"count_matrix\" and \"p_val\" . Below, we walk though how these are validated. For the count_matrix input, we see the spec field says it accepts \"DataResource\" s (files) with resource types of [\"RNASEQ_COUNT_MTX\", \"I_MTX\"] . It also says \"many\": false , so we only will accept a single file. The payload example above provided a single UUID (so it is validated for \"many\": false ). Then, we will take that UUID and query our database to see if it corresponds to a Resource instance that has a resource_type member that is either \"RNASEQ_COUNT_MTX\" or \"I_MTX\" . If that is indeed the case, then the \"count_matrix\" field is successfully validated. For the \"p_val\" field, we receive a value of 0.01. The spec states that this input should be of type BoundedFloat with a min of 0.0 and a max of 1.0. The backend validates that 0.01 is indeed in the range [0.0,1.0].","title":"Executing an Operation"},{"location":"operations/#operation-modes","text":"Depending on how the Operation should be run, we perform different actions upon ingestion. For local docker-based runs, we obviously require that the image be located on the same machine as the MEV instance. To ensure everything is \"aligned\", we require additional files depending on the run mode. These are verified during the ingestion process. For instance, in the local docker run mode, we need a Dockerfile to build from. The image will be built and pushed to Dockerhub during ingestion. The image will be tagged with the github commit hash so that the file(s) used to build the image can be uniquely identified and re-traced over time.","title":"Operation modes"},{"location":"resource_metadata/","text":"Resource metadata Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). Since it represents a database table, it does not do perform validation, etc. on the actual files that have the data. However, the DataResource is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Thus, instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent.","title":"Resource metadata"},{"location":"resource_metadata/#resource-metadata","text":"Metadata can be associated with type of DataResource . Note that a DataResource is related, but distinct from a Resource . The latter is for tracking the various file-based resources in the database; it knows about the file location, size, and the type of the resource (as a string field). Since it represents a database table, it does not do perform validation, etc. on the actual files that have the data. However, the DataResource is a base class from which the many specialized \"types\" of resources derive. For instance, an IntegerMatrix derives from a DataResource . Thus, instead of being a database record, a DataResource captures the expected format and behavior of the resource. For instance, the children classes of DataResource contain validators and parsers. Associated with each DataResource is some metadata. The specification may expand to incorporate additional fields, but at minimum, it should contain: An ObservationSet . For a FastQ file representing a single sample (most common case), the ObservationSet would have a single item (of type Observation ) containing information about that particular sample. For a count matrix of size (p, N), the ObservationSet would have N items (again, of type Observation ) giving information about the samples in the columns. A FeatureSet . This is a collection of covariates corresponding to a single Observation . A Feature is something that is measured (e.g. read counts for a gene). For a count matrix of size (p, N), the FeatureSet would have p items (of type Feature ) and correspond to the p genes measured for a single sample. For a sequence-based file like a FastQ, this would simply be null; perhaps there are alternative intepretations of this concept, but the point is that the field can be null. A table of differentially expressed genes would have a FeatureSet , but not an ObservationSet ; in this case the Feature s are the genes and we are given information like log-fold change and p-value. A parent operation. As an analysis workflow can be represented as a directed, acyclic graph (DAG), we would like to track the flow of data and operations on the data. Tracking the \"parent\" of a DataResource allows us to determine which operation generated the data and hence reconstruct the full DAG. The original input files would have a null parent.","title":"Resource metadata"},{"location":"resource_types/","text":"Resource types A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. Resource types allow us to specify the format of input/output files of Operation s. Therefore, we can predictably present options for those inputs and allow Resource s to flow from one analysis to another. The string identifiers map to concrete classes that implement validation methods for the Resource . For example, the string I_MTX indicates that the Resource is an integer matrix. When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into several broad categories: Table-based formats Sequence-based formats JSON General. Not a true type, but rather denotes that a better, more specific type cannot be specified. Table-based formats Table-based formats are any matrix-like format, such as a typical CSV file. In addition to being a common file format for expression matrices and similar experimental data, this covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. Specific types are shown below in the auto-generated documentation, but we touch on some of the more general descriptions immediately below. During validation, the primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. We enforce that specific sub-types of this general table-based format adhere to our expectations. For instance, an expression matrix requires a first row which contains samples/observation names. Furthermore, the first column should correspond to gene identifiers ( Feature s more generally). While we cannot exhaustively validate every file, we make certain reasonable assumptions. For example, if the first row is all numbers, we assume that a header is missing. Certainly one could name their samples with numeric identifiers, but we enforce that they need to be strings. Failure to conform to these expectations will result in the file failing to validate. Users should be informed of the failure with a helpful message for resolution. Also note that while the user may submit files in a format such as CSV, we internally convert to a common format (e.g. TSV) so that downstream tools can avoid having to include multiple file-parsing schemes. Since table-based formats naturally lend themselves to arrays of atomic items (i.e. each row as a \"record\"), the contents of table-based formats can be requested in a paginated manner via the API. Sequence-based formats Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. These types of files cannot reasonably be validated up front, so any Operation s which use these should plan on gracefully handling problems with their format. JSON For data that is not easily represented in a table-based format, we retain JSON as a general format. We use Python's internal json library to enforce the format of these files. Any failure to parse the file results in a validation error. Note that the contents of array-based JSON files can be paginated, but general JSON objected-based resources cannot. An example of the former is: [ { \"keyA\": 1, \"some_value\": \"abc\" }, ... { \"keyA\": 8, \"some_value\": \"xyz\" } ] These can be paginated so that each internal \"object\" (e.g. {\"keyA\": 1,\"some_value\":\"abc\"} ) is a record. General Generally this format should be avoided as it allows un-validated/unrestricted data formats to be passed around. However, for certain types (such as a tarball of many files), we sometimes have no other reasonable option. Table-based resource types class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. RnaSeqCountMatrix ( ) A very-explicit class (mainly for making things user-friendly) where we provide specialized behavior/messages specific to count matrices generated from RNA-seq data. The same as an integer matrix, but named to be suggestive for WebMEV users. class resource_types.table_types. ElementTable ( ) An ElementTable captures common behavior of tables which annotate Observation s (AnnotationTable) or Feature s (FeatureTable) It's effectively an abstract class. See the derived classes which implement the specific behavior for Observation s or Feature s. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to add Attribute s to the corresponding Observation s. Note that the backend doesn't manage this. Instead, the front-end will be responsible for taking the AnnotationTable and creating/modifying Observation s. class resource_types.table_types. FeatureTable ( ) A FeatureTable is a type of table that has aggregate information about the features, but does not have any \"observations\" in the columns. An example would be the results of a differential expression analysis. Each row corresponds to a gene (feature) and the columns are information about that gene (such as p-value). Another example could be a table of metadata about genes (e.g. pathways or perhaps a mapping to a different gene identifier). The first column will give the feature/gene identifiers and the remaining columns will have information about that gene class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here. Sequence-based formats class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Resource types"},{"location":"resource_types/#resource-types","text":"A Resource represents some generic notion of data and its resource_type field/member is a string identifier that identifies the specific format of the data. Resource types allow us to specify the format of input/output files of Operation s. Therefore, we can predictably present options for those inputs and allow Resource s to flow from one analysis to another. The string identifiers map to concrete classes that implement validation methods for the Resource . For example, the string I_MTX indicates that the Resource is an integer matrix. When a new Resource is added (via upload or directly by an admin via the API), the validation method is called. Similarly, if a user tries to change the resource_type , it will trigger the validation process. Current resource_types fall into several broad categories: Table-based formats Sequence-based formats JSON General. Not a true type, but rather denotes that a better, more specific type cannot be specified. Table-based formats Table-based formats are any matrix-like format, such as a typical CSV file. In addition to being a common file format for expression matrices and similar experimental data, this covers a wide variety of standard formats encountered in computational biology, including GTF annotation files and BED files. Specific types are shown below in the auto-generated documentation, but we touch on some of the more general descriptions immediately below. During validation, the primitive data types contained in each column are determined using Python's Pandas library, which refers to these as \"dtypes\"; for example, a column identified as int64 certainly qualifies as an integer type. If the column contains any non-integers (but all numbers), Pandas automatically converts it to a float type (e.g. float64 ) which allows us to easily validate the content of each column. We enforce that specific sub-types of this general table-based format adhere to our expectations. For instance, an expression matrix requires a first row which contains samples/observation names. Furthermore, the first column should correspond to gene identifiers ( Feature s more generally). While we cannot exhaustively validate every file, we make certain reasonable assumptions. For example, if the first row is all numbers, we assume that a header is missing. Certainly one could name their samples with numeric identifiers, but we enforce that they need to be strings. Failure to conform to these expectations will result in the file failing to validate. Users should be informed of the failure with a helpful message for resolution. Also note that while the user may submit files in a format such as CSV, we internally convert to a common format (e.g. TSV) so that downstream tools can avoid having to include multiple file-parsing schemes. Since table-based formats naturally lend themselves to arrays of atomic items (i.e. each row as a \"record\"), the contents of table-based formats can be requested in a paginated manner via the API. Sequence-based formats Sequence-based formats are formats like FastQ, Fasta, or SAM/BAM. These types of files cannot reasonably be validated up front, so any Operation s which use these should plan on gracefully handling problems with their format. JSON For data that is not easily represented in a table-based format, we retain JSON as a general format. We use Python's internal json library to enforce the format of these files. Any failure to parse the file results in a validation error. Note that the contents of array-based JSON files can be paginated, but general JSON objected-based resources cannot. An example of the former is: [ { \"keyA\": 1, \"some_value\": \"abc\" }, ... { \"keyA\": 8, \"some_value\": \"xyz\" } ] These can be paginated so that each internal \"object\" (e.g. {\"keyA\": 1,\"some_value\":\"abc\"} ) is a record. General Generally this format should be avoided as it allows un-validated/unrestricted data formats to be passed around. However, for certain types (such as a tarball of many files), we sometimes have no other reasonable option.","title":"Resource types"},{"location":"resource_types/#table-based-resource-types","text":"class resource_types.table_types. TableResource ( ) The TableResource is the most generic form of a delimited file. Any type of data that can be represented as rows and columns. This or any of the more specific subclasses can be contained in files saved in CSV, TSV, or Excel (xls/xlsx) format. If in Excel format, the data of interest must reside in the first sheet of the workbook. Special tab-delimited files like BED or VCF files are recognized by their canonical extension (e.g. \".bed\" or \".vcf\"). Note that unless you create a \"specialized\" implementation (e.g. like for a BED file), then we assume you have features as rows and observables as columns. class resource_types.table_types. Matrix ( ) A Matrix is a delimited table-based file that has only numeric types. These types can be mixed, like floats and integers class resource_types.table_types. IntegerMatrix ( ) An IntegerMatrix further specializes the Matrix to admit only integers. class resource_types.table_types. RnaSeqCountMatrix ( ) A very-explicit class (mainly for making things user-friendly) where we provide specialized behavior/messages specific to count matrices generated from RNA-seq data. The same as an integer matrix, but named to be suggestive for WebMEV users. class resource_types.table_types. ElementTable ( ) An ElementTable captures common behavior of tables which annotate Observation s (AnnotationTable) or Feature s (FeatureTable) It's effectively an abstract class. See the derived classes which implement the specific behavior for Observation s or Feature s. class resource_types.table_types. AnnotationTable ( ) An AnnotationTable is a special type of table that will be responsible for annotating Observations/samples (e.g. adding sample names and associated attributes like experimental group or other covariates) The first column will give the sample names and the remaining columns will each individually represent different covariates associated with that sample. For example, if we received the following table: sample genotype treatment A WT Y B WT N Then this table can be used to add Attribute s to the corresponding Observation s. Note that the backend doesn't manage this. Instead, the front-end will be responsible for taking the AnnotationTable and creating/modifying Observation s. class resource_types.table_types. FeatureTable ( ) A FeatureTable is a type of table that has aggregate information about the features, but does not have any \"observations\" in the columns. An example would be the results of a differential expression analysis. Each row corresponds to a gene (feature) and the columns are information about that gene (such as p-value). Another example could be a table of metadata about genes (e.g. pathways or perhaps a mapping to a different gene identifier). The first column will give the feature/gene identifiers and the remaining columns will have information about that gene class resource_types.table_types. BEDFile ( ) A file format that corresponds to the BED format. This is the minimal BED format, which has: chromosome start position end position Additional columns are ignored. By default, BED files do NOT contain headers and we enforce that here.","title":"Table-based resource types"},{"location":"resource_types/#sequence-based-formats","text":"class resource_types.sequence_types. SequenceResource ( ) This class is used to represent sequence-based files such as Fasta, Fastq, SAM/BAM We cannot (reasonably) locally validate the contents of these files quickly or exhaustively, so minimal validation is performed remotely class resource_types.sequence_types. FastAResource ( ) This type is for validating Fasta files, compressed or not. Fasta files are recognized using the following formats: fasta fasta.gz fa fa.gz class resource_types.sequence_types. FastQResource ( ) This resource type is for Fastq files, compressed or not. Fastq files are recognized using the following formats: fastq fastq.gz fq fq.gz class resource_types.sequence_types. AlignedSequenceResource ( ) This resource type is for SAM/BAM files. We accept both SAM and BAM files named using their canonical extensions: \".bam\" for BAM files \".sam\" for SAM files","title":"Sequence-based formats"},{"location":"resources/","text":"Resources Resource s represent data in some file-based format. They come in two types-- those owned by specific users ( Resource ) and those that are user-independent and associated with analysis operations ( OperationResource ). Examples of the latter include files for reference genomes, aligner indexes, or other analysis-specific files that a user does not need to maintain or directly interact with. Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders (e.g. Dropbox). They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. OperationResource s do not have owners, but instead maintain a foreign-key relationship with their associated Operation . Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats in addition to more generic text-based formats such as CSV, TSV. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. Note that we can only validate certain types of files, such as CSV and TSV. Validation of sequence-based files such as FastQ and BAM is not feasible and thus we skip validation. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Succesfully validated files can sometimes be changed to a convenient internal representation. For instance, we accept expression matrices in multiple formats (e.g. CSV, TSV, XLSX). However, to avoid each analysis Operation from having to parse many potential file formats, we internally convert it to a consistent format, such as TSV. Thus, all the downstream tools expect that the validated resource passed as an input is saved in a TSV/tab-delimited format. Resources and metadata Depending on the type of Resource , we are able to infer and extract metadata from the file based on the format. For example, given a validated Resource that represents an RNA-seq count matrix, we assume that the column headers represent samples ( Observation s) and the rows represent genes ( Feature s). These metadata allow us to create subsets of the Observation s and Feature s for creating experimental contrasts and other typical analysis tasks. More on Observation s and Feature s is described elsewhere. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. When a user chooses to \"add\" a Resource to a Workspace , we append the Workspace to the set of Workspace instances associated with that Resource . That is, each Resource tracks which Workspace s it is associated with. This is accomplished via a many-to-many mapping in the database. Users can remove a Resource from a Workspace , but only if it has NOT been used for any portions of the analysis . We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Note that removing a Resource from a Workspace does not delete a file- it only modifies the workspaces field on the Resource database instance. Deletion of Resources Resource s can only be deleted from the file manager on the \"home\" screen (i.e. not in the Workspace view) in the UI. If a Resource is associated/attached to one or more Workspace s, then you cannot delete the Resource . A Resource can only be deleted if: It is associated with zero Workspace s It is not used in any Operation Technically, we only need the first case. If a Resource has been used in an Operation , we don't allow the user to remove it from the Workspace . Thus, a file being associated with zero Workspace s means that it has not been used in any Operation s Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process, such as validation. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.abstract_resource. AbstractResource ( *args , **kwargs ) This is the base class which holds common fields for both the user-owned Resource model and the user-independent OperationResource model. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). class api.models.operation_resource. OperationResource ( *args , **kwargs ) An OperationResource is a specialization of a Resource which is not owned by anyone specific, but is rather associated with a single Operation . Used for things like genome indexes, etc. where the user is not responsible for supplying or maintaining the resource. Note that it maintains a reference to the Operation input field it corresponds to. This front-end components to easily map the OperationResource to the proper input field for user selection. The is_active and is_public fields default to True .","title":"General info"},{"location":"resources/#resources","text":"Resource s represent data in some file-based format. They come in two types-- those owned by specific users ( Resource ) and those that are user-independent and associated with analysis operations ( OperationResource ). Examples of the latter include files for reference genomes, aligner indexes, or other analysis-specific files that a user does not need to maintain or directly interact with. Much of the information regarding Resource instances is provided in the auto-generated docstring below, but here we highlight some key elements of the Resource model. Namely, the kinds of operations users and admins can take to create, delete, or otherwise manipulated Resource s via the API. Resource creation Regular MEV users can only create Resource instances by uploading files, either via a direct method (upload from local machine) or by using one our cloud-based uploaders (e.g. Dropbox). They can't do this via the API. Admins can \"override\" and create Resource instances manually via the API. Regardless of who created the Resource , the validation process is started asynchronously. We cannot assume that the files are properly validated, even if the request was initiated by an admin. Upon creation of the Resource , it is immediately set to \"inactive\" ( is_active = False ) while we validate the particular type. Resource instances have a single owner, which is the owner who uploaded the file, or directly specified by the admin in the API request. OperationResource s do not have owners, but instead maintain a foreign-key relationship with their associated Operation . Resource \"type\" A Resource is required to have a \"type\" (e.g. an integer matrix) which we call a resource_type . These types are restricted to a set of common file formats in addition to more generic text-based formats such as CSV, TSV. Upon creation, resource_type is set to None which indicates that the Resource has not been validated. The type of the Resource can be specified when the file is uploaded or at any other time (i.e. users can change the type if they desire). Each request to change type initiates an asynchronous validation process. Note that we can only validate certain types of files, such as CSV and TSV. Validation of sequence-based files such as FastQ and BAM is not feasible and thus we skip validation. If the validation of the resource_type fails, we revert back to the previous successfully validated type. If the type was previously None (as with a new upload), we simply revert back to None and inform the user the validation failed. Succesfully validated files can sometimes be changed to a convenient internal representation. For instance, we accept expression matrices in multiple formats (e.g. CSV, TSV, XLSX). However, to avoid each analysis Operation from having to parse many potential file formats, we internally convert it to a consistent format, such as TSV. Thus, all the downstream tools expect that the validated resource passed as an input is saved in a TSV/tab-delimited format. Resources and metadata Depending on the type of Resource , we are able to infer and extract metadata from the file based on the format. For example, given a validated Resource that represents an RNA-seq count matrix, we assume that the column headers represent samples ( Observation s) and the rows represent genes ( Feature s). These metadata allow us to create subsets of the Observation s and Feature s for creating experimental contrasts and other typical analysis tasks. More on Observation s and Feature s is described elsewhere. Resources and Workspaces Resource instances are initially \"unattached\" meaning they are associated with their owner, but have not been associated with any user workspaces. When a user chooses to \"add\" a Resource to a Workspace , we append the Workspace to the set of Workspace instances associated with that Resource . That is, each Resource tracks which Workspace s it is associated with. This is accomplished via a many-to-many mapping in the database. Users can remove a Resource from a Workspace , but only if it has NOT been used for any portions of the analysis . We want to retain the completeness of the analysis, so deleting files that are part of the analysis \"tree\" would create gaps. Note that removing a Resource from a Workspace does not delete a file- it only modifies the workspaces field on the Resource database instance. Deletion of Resources Resource s can only be deleted from the file manager on the \"home\" screen (i.e. not in the Workspace view) in the UI. If a Resource is associated/attached to one or more Workspace s, then you cannot delete the Resource . A Resource can only be deleted if: It is associated with zero Workspace s It is not used in any Operation Technically, we only need the first case. If a Resource has been used in an Operation , we don't allow the user to remove it from the Workspace . Thus, a file being associated with zero Workspace s means that it has not been used in any Operation s Notes related to backend implementation In general, the is_active = False flag disallows any updating of the Resource attributes via the API. All post/patch/put requests will return a 400 status. This prevents multiple requests from interfering with an ongoing background process, such as validation. Users cannot change the path member. The actual storage of the files should not matter to the users so they are unable to change the path member. class api.models.abstract_resource. AbstractResource ( *args , **kwargs ) This is the base class which holds common fields for both the user-owned Resource model and the user-independent OperationResource model. class api.models.resource. Resource ( *args , **kwargs ) A Resource is an abstraction of data. It represents some piece of data we are analyzing or manipulating in the course of an analysis workflow. Resource s are most often represented by flat files, but their physical storage is not important. They could be stored locally or in cloud storage accessible to MEV. Various \"types\" of Resource s implement specific constraints on the data that are important for tracking inputs and outputs of analyses. For example, if an analysis module needs to operate on a matrix of integers, we can enforce that the only Resource s available as inputs are those identified (and verified) as IntegerMatrix \"types\". Note that we store all types of Resource s in the database as a single table and maintain the notion of \"type\" by a string-field identifier. Creating specific database tables for each type of Resource would be unnecessary. By connecting the string stored in the database with a concrete implementation class we can check the type of the Resource . Resource s are not active ( is_active flag in the database) until their \"type\" has been verified. API users will submit the intended type with the request and the backend will check that. Violations are reported and the Resource remains inactive ( is_active=False ). class api.models.operation_resource. OperationResource ( *args , **kwargs ) An OperationResource is a specialization of a Resource which is not owned by anyone specific, but is rather associated with a single Operation . Used for things like genome indexes, etc. where the user is not responsible for supplying or maintaining the resource. Note that it maintains a reference to the Operation input field it corresponds to. This front-end components to easily map the OperationResource to the proper input field for user selection. The is_active and is_public fields default to True .","title":"Resources"},{"location":"setup_configuration/","text":"Configuration options and parameters As described in the installation section, the WebMeV API depends on environment variables to configure the application. For local development with Vagrant, this requires copying vagrant/env.tmpl to vagrant/env.txt and filling in the relevant variables. Each environment variable is commented with detailed explanations and you should consult that file for their interpretation. For a cloud-based deployment with terraform, you similarly need to copy the config.tfvars.template file to terraform.tfvars and enter the config parameters. For the most part, these variables are similar to the local deployment, but we make some detailed notes/comments below. Comments about configuration parameters: project_id : This is the \"string\" name of the Google project, not the numerical ID. credentials_file : This is the name of the JSON-format file containing your service account credentials. See the section on setup to learn about where and how to obtain this file. This is assumed to reside in the deploy/terraform/live folder so your \"main\" terraform.tfvars file can reference it easily. enable_remote_job_runners : The value \"yes\" enables the remote job runners like Cromwell. If any other value, then we will not allow remote jobs to be executed, which limits the types of analyses that can be run. cromwell_bucket : The name (no gs:// prefix!) of the storage bucket used by Cromwell. Ideally, this is in the same region/zone you are deploying in. cromwell_db_* : Database params, so make these appropriately secure. service_account_email : The full, email-like name for the service account you created for installation. See instructions there. Something like \"abc123@myproject.iam.gserviceaccount.com\". managed_dns_zone : As part of the deployment, we have terraform add DNS records which will point at the IP address of the public-facing load balancer. Hence, you must already have Google Cloud DNS as your nameserver for your domain. As mentioned in setup, deviations from this expectation are not covered and will require customization of your terraform main.tf script(s). ssl_cert : As described in the setup section, you will need to provision a SSL certificate to serve HTTPS. This certificate will be added to your load balancer. It is given as a \"resource string\", so it will be something like: projects/<GCP PROJECT>/global/sslCertificates/<CERTIFICATE NAME> . domain : The domain where your API will be accessed. We add a DNS record for this domain which points at the IP address of the load balancer. frontend_domain : This is not strictly necessary, but will allow a frontend application located on another domain to interact with the API. Otherwise, Django will reject the request due to same-origin CORS policies. other_cors_origins : This will allow additional frontend applications to connect. For instance, if you would like your local development frontend (accessible at localhost:4200) to connect to this deployment, you can add \"http://localhost:4200\". This is a comma-delimited string, so you can have multiple values. Be sure to include the http protocol, as shown. mev_storage_bucket : The name (no gs:// prefix!) of the storage bucket used by WebMeV. This is where user's files are stored. Ideally, this is in the same region/zone you are deploying in. storage_location : This is either \"remote\" or \"local\". Using \"remote\" makes use of the Google storage bucket named above. The \"local\" setting stores files on the server, which can be inappropriate for working with larger files since it requires larger hard disk space. For cloud deployments, just set this to \"remote\". from_email : Since we are using the Gmail API, we can use a Google group-associated email send messages. Thus, if you are using the Gmail account of \"user@gmail.com\" who is associated with the \"mygroup@gmail.com\" address, then you can set this to \"Some group mygroup@gmail.com \" so that emails are sent on behalf of that group. This allows emails to appear to be sent from non-personal Gmail accounts. Specify as a format like \"Some name <some@email.com>\" gmail_* : These are the authentication parameters obtained by performing an authentication flow with Gmail . sentry_url : Only necessary to use if you are using a Sentry issue-tracker. If not using Sentry, then you can just leave this as an empty string. dockerhub_username , dockerhub_passwd : These parameters are used to set your username and password for interacting with your Dockerhub account. This allows us to dynamically create Docker images and push them to the Dockerhub registry so that they are accessible and users can replicate their analyses. dockerhub_org : An organization-level account for Dockerhub. Your individual user must be associated with this organization. If you do not have one or do not wish to use a Dockerhub organization, simply set this to your Dockerhub username . Email backends ( EMAIL_BACKEND_CHOICE ) Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE Django setting. The value for that comes from the EMAIL_BACKEND_CHOICE environment variable for local dev and from the email_backend setting in terraform.tfvars . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends. Currently, the only real/outgoing mail backend is Gmail. Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. As mentioned, this is the only setting that allows actual email to be sent! To use this email backend, you will need to perform an OAuth2 authentication flow to obtain the proper credentials: Steps: Choose a Gmail account (or create a new account) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMeV local VM, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib (https://pypi.org/project/google-auth-oauthlib/) library is installed. The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using that final JSON file, you can fill in the four variables contained in either your vagrant/env.txt for local dev or your deploy/terraform/live/terraform.tfvars for a cloud deployment. Be careful with these credentials as they give full access to the Gmail account in question!! About storage backends Storage of user files can be either local (on the WebMEV server) or in some remote filesystem (e.g. in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice and provide a common interface. Implementations for storage backends can be found in the api/storage_backends folder. However, for ease, we limit the options for these backends depending on other configuration variables. For instance, if the admin indicates that they want to run remote jobs (e.g. with Cromwell), we only allow remote storage ( storage_location = \"remote\" in terraform.tfvars ), as the job runner will require access to bucket storage. We anticipate that the job runner will be handling/processing large files and we want to keep all files in remote bucket-based storage instead of consuming disk space on the host machine. Thus, one may only use storage_location = \"local\" if they are only planning to use local Docker-based jobs. Note that each storage backend may require additional environment variables to be set. For instance, if using the bucket-based backend, we naturally require the name of the bucket we will be using (e.g. mev_bucket_name for GCP). Checking for these additional required environment variables is accomplished by attempting an initial import of the storage backend class during startup of the Django applicatoin. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (ideally) prevents errors in runtime due to incomplete/invalid configuration.","title":"Configuration"},{"location":"setup_configuration/#configuration-options-and-parameters","text":"As described in the installation section, the WebMeV API depends on environment variables to configure the application. For local development with Vagrant, this requires copying vagrant/env.tmpl to vagrant/env.txt and filling in the relevant variables. Each environment variable is commented with detailed explanations and you should consult that file for their interpretation. For a cloud-based deployment with terraform, you similarly need to copy the config.tfvars.template file to terraform.tfvars and enter the config parameters. For the most part, these variables are similar to the local deployment, but we make some detailed notes/comments below. Comments about configuration parameters: project_id : This is the \"string\" name of the Google project, not the numerical ID. credentials_file : This is the name of the JSON-format file containing your service account credentials. See the section on setup to learn about where and how to obtain this file. This is assumed to reside in the deploy/terraform/live folder so your \"main\" terraform.tfvars file can reference it easily. enable_remote_job_runners : The value \"yes\" enables the remote job runners like Cromwell. If any other value, then we will not allow remote jobs to be executed, which limits the types of analyses that can be run. cromwell_bucket : The name (no gs:// prefix!) of the storage bucket used by Cromwell. Ideally, this is in the same region/zone you are deploying in. cromwell_db_* : Database params, so make these appropriately secure. service_account_email : The full, email-like name for the service account you created for installation. See instructions there. Something like \"abc123@myproject.iam.gserviceaccount.com\". managed_dns_zone : As part of the deployment, we have terraform add DNS records which will point at the IP address of the public-facing load balancer. Hence, you must already have Google Cloud DNS as your nameserver for your domain. As mentioned in setup, deviations from this expectation are not covered and will require customization of your terraform main.tf script(s). ssl_cert : As described in the setup section, you will need to provision a SSL certificate to serve HTTPS. This certificate will be added to your load balancer. It is given as a \"resource string\", so it will be something like: projects/<GCP PROJECT>/global/sslCertificates/<CERTIFICATE NAME> . domain : The domain where your API will be accessed. We add a DNS record for this domain which points at the IP address of the load balancer. frontend_domain : This is not strictly necessary, but will allow a frontend application located on another domain to interact with the API. Otherwise, Django will reject the request due to same-origin CORS policies. other_cors_origins : This will allow additional frontend applications to connect. For instance, if you would like your local development frontend (accessible at localhost:4200) to connect to this deployment, you can add \"http://localhost:4200\". This is a comma-delimited string, so you can have multiple values. Be sure to include the http protocol, as shown. mev_storage_bucket : The name (no gs:// prefix!) of the storage bucket used by WebMeV. This is where user's files are stored. Ideally, this is in the same region/zone you are deploying in. storage_location : This is either \"remote\" or \"local\". Using \"remote\" makes use of the Google storage bucket named above. The \"local\" setting stores files on the server, which can be inappropriate for working with larger files since it requires larger hard disk space. For cloud deployments, just set this to \"remote\". from_email : Since we are using the Gmail API, we can use a Google group-associated email send messages. Thus, if you are using the Gmail account of \"user@gmail.com\" who is associated with the \"mygroup@gmail.com\" address, then you can set this to \"Some group mygroup@gmail.com \" so that emails are sent on behalf of that group. This allows emails to appear to be sent from non-personal Gmail accounts. Specify as a format like \"Some name <some@email.com>\" gmail_* : These are the authentication parameters obtained by performing an authentication flow with Gmail . sentry_url : Only necessary to use if you are using a Sentry issue-tracker. If not using Sentry, then you can just leave this as an empty string. dockerhub_username , dockerhub_passwd : These parameters are used to set your username and password for interacting with your Dockerhub account. This allows us to dynamically create Docker images and push them to the Dockerhub registry so that they are accessible and users can replicate their analyses. dockerhub_org : An organization-level account for Dockerhub. Your individual user must be associated with this organization. If you do not have one or do not wish to use a Dockerhub organization, simply set this to your Dockerhub username .","title":"Configuration options and parameters"},{"location":"setup_configuration/#email-backends-email_backend_choice","text":"Various operations like user registration and password changes require us to send users emails with encoded tokens to verify their email address and permit changes to their account. Accordingly, WebMEV needs the ability to send emails. We allow customization of this email backend through the EMAIL_BACKEND_CHOICE Django setting. The value for that comes from the EMAIL_BACKEND_CHOICE environment variable for local dev and from the email_backend setting in terraform.tfvars . Certain cloud providers, such as GCP, place restrictions on outgoing emails to prevent abuse. Per their documentation ( https://cloud.google.com/compute/docs/tutorials/sending-mail ), GCP recommends to use a third-party service such as SendGrid. If you wish to use these, you will have to implement your own email backend per the interface described at the Django project: https://docs.djangoproject.com/en/3.0/topics/email/#defining-a-custom-email-backend By default, we provide the following email backends. Currently, the only real/outgoing mail backend is Gmail. Console ( EMAIL_BACKEND_CHOICE=CONSOLE ) Note: This backend is for development purposes only-- no emails are actually sent! If EMAIL_BACKEND_CHOICE is not set, WebMEV defaults to using Django's \"console\" backend, which simply prints the emails to stdout. This is fine for development purposes where the tokens can be copy/pasted for live-testing, but is obviously not suitable for a production environment. Gmail ( EMAIL_BACKEND_CHOICE=GMAIL ) Alternatively, one can use the Gmail API to send emails from their personal or institution google account. As mentioned, this is the only setting that allows actual email to be sent! To use this email backend, you will need to perform an OAuth2 authentication flow to obtain the proper credentials: Steps: Choose a Gmail account (or create a new account) from which you wish to send email notifications. On your own machine (or wherever you can login to your Google account), go to the Google developers console ( https://console.developers.google.com or https://console.cloud.google.com ) and head to \"APIs & Services\" and \"Dashboard\". Click on \"Enable APIs and Services\", search for \"Gmail\", and enable the Gmail API. Go to the \"Credentials\" section under \"APIs and Services\". Just as above, we will create a set of OAuth credentials. Click on the \"Create credentials\" button and choose \"OAuth Client ID\". Choose \"Other\" from the options and give these credentials a name. Once the credentials are created, download the JSON-format file when it prompts. Using that credential file, run the helpers/exchange_gmail_credentials.py script like: python3 helpers/exchange_gmail_credentials.py -i <original_creds_path> -o <final_creds_path> (Note that this script can be run from within the WebMeV local VM, as it contains the appropriate Python packages. If you are not using the application container, you can run the script as long as the google-auth-oauthlib (https://pypi.org/project/google-auth-oauthlib/) library is installed. The script will ask you to copy a link into your browser, which you can do on any machine where you can authenticate with Google. That URL will ask you to choose/log-in to the Gmail account you will be using to send emails. Finally, if successfully authenticated, it will provide you with a \"code\" which you will copy into your terminal. Once complete, the script will write a new JSON-format file at the location specified with the -o argument. Using that final JSON file, you can fill in the four variables contained in either your vagrant/env.txt for local dev or your deploy/terraform/live/terraform.tfvars for a cloud deployment. Be careful with these credentials as they give full access to the Gmail account in question!!","title":"Email backends (EMAIL_BACKEND_CHOICE) "},{"location":"setup_configuration/#about-storage-backends","text":"Storage of user files can be either local (on the WebMEV server) or in some remote filesystem (e.g. in a Google storage bucket). To abstract this, we have storage \"backends\" that control the behavior for each storage choice and provide a common interface. Implementations for storage backends can be found in the api/storage_backends folder. However, for ease, we limit the options for these backends depending on other configuration variables. For instance, if the admin indicates that they want to run remote jobs (e.g. with Cromwell), we only allow remote storage ( storage_location = \"remote\" in terraform.tfvars ), as the job runner will require access to bucket storage. We anticipate that the job runner will be handling/processing large files and we want to keep all files in remote bucket-based storage instead of consuming disk space on the host machine. Thus, one may only use storage_location = \"local\" if they are only planning to use local Docker-based jobs. Note that each storage backend may require additional environment variables to be set. For instance, if using the bucket-based backend, we naturally require the name of the bucket we will be using (e.g. mev_bucket_name for GCP). Checking for these additional required environment variables is accomplished by attempting an initial import of the storage backend class during startup of the Django applicatoin. By convention, any configuration parameters required should at the \"top-level\" of the Python module/file. This way, when we attempt the import while starting the application, any missing configuration variables will raise an exception. This (ideally) prevents errors in runtime due to incomplete/invalid configuration.","title":"About storage backends"},{"location":"workspaces/","text":"Workspaces class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"},{"location":"workspaces/#workspaces","text":"class api.models.workspace. Workspace ( *args , **kwargs ) A Workspace is a way to logically group the files and and analyses that are part of a user's work. Users can have multiple Workspace s to separate distinct analyses. Data, files, and analyses are grouped under a Workspace such that all information related to the analyses, including analysis history, is captured the Workspace .","title":"Workspaces"}]}